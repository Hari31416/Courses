
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This is a collection of all the notebooks from various courses I have taken.">
      
      
        <meta name="author" content="Harikesh Kushwaha">
      
      
        <link rel="canonical" href="https://hari31416.github.io/Courses/DLS/C5/Assignments/Building_a_Recurrent_Neural_Network_Step_by_Step/Building_a_Recurrent_Neural_Network_Step_by_Step/">
      
      
        <link rel="prev" href="../../../../C4/Assignments/Transfer_learning_with_MobileNet/Transfer_learning_with_MobileNet_v1/">
      
      
        <link rel="next" href="../../Dinosaurus_Island_Character_level_language_model/Dinosaurus_Island_Character_level_language_model/">
      
      <link rel="icon" href="../../../../../assets/ml.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.15">
    
    
      
        <title>Building a Recurrent Neural Network Step by Step - Courses</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../../../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../../../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#building-your-recurrent-neural-network-step-by-step" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Courses" class="md-header__button md-logo" aria-label="Courses" data-md-component="logo">
      
  <img src="../../../../../assets/ml.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Courses
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Building a Recurrent Neural Network Step by Step
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/Hari31416/Courses" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Courses
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Courses" class="md-nav__button md-logo" aria-label="Courses" data-md-component="logo">
      
  <img src="../../../../../assets/ml.png" alt="logo">

    </a>
    Courses
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Hari31416/Courses" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Courses
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Deep Learning Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step/" class="md-nav__link">
        Building your Deep Neural Network Step by Step
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/Assignments/Deep_Neural_Network_-_Application/Deep_Neural_Network_-_Application/" class="md-nav__link">
        Deep Neural Network   Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/Assignments/Logistic_Regression_with_a_Neural_Network_mindset/Logistic_Regression_with_a_Neural_Network_mindset/" class="md-nav__link">
        Logistic Regression with a Neural Network mindset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/Assignments/Planar_data_classification_with_one_hidden_layer/Planar_data_classification_with_one_hidden_layer/" class="md-nav__link">
        Planar data classification with one hidden layer
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C2/Assignments/Gradient_Checking/Gradient_Checking/" class="md-nav__link">
        Gradient Checking
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C2/Assignments/Initialization/Initialization/" class="md-nav__link">
        Initialization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C2/Assignments/Optimization_Methods/Optimization_methods/" class="md-nav__link">
        Optimization methods
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C2/Assignments/Regularization/Regularization/" class="md-nav__link">
        Regularization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C2/Assignments/Tensorflow_introduction/Tensorflow_introduction/" class="md-nav__link">
        Tensorflow introduction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
          C4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          C4
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C4/Assignments/Car_detection_with_YOLO/Autonomous_driving_application_Car_detection/" class="md-nav__link">
        Autonomous driving application Car detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C4/Assignments/Convolution_model_Application/Convolution_model_Application/" class="md-nav__link">
        Convolution model Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C4/Assignments/Image_segmentation_Unet/Image_segmentation_Unet_v2/" class="md-nav__link">
        Image segmentation Unet v2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C4/Assignments/Residual_Networks/Residual_Networks/" class="md-nav__link">
        Residual Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C4/Assignments/Transfer_learning_with_MobileNet/Transfer_learning_with_MobileNet_v1/" class="md-nav__link">
        Transfer learning with MobileNet v1
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
          C5
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2_4">
          <span class="md-nav__icon md-icon"></span>
          C5
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Building a Recurrent Neural Network Step by Step
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Building a Recurrent Neural Network Step by Step
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pre-requisites" class="md-nav__link">
    Pre-requisites
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#be-careful-when-modifying-the-starter-code" class="md-nav__link">
    Be careful when modifying the starter code!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#important-note-on-submission-to-the-autograder" class="md-nav__link">
    Important Note on Submission to the AutoGrader
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-of-content" class="md-nav__link">
    Table of Content
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#packages" class="md-nav__link">
    Packages
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-forward-propagation-for-the-basic-recurrent-neural-network" class="md-nav__link">
    1 - Forward Propagation for the Basic Recurrent Neural Network
  </a>
  
    <nav class="md-nav" aria-label="1 - Forward Propagation for the Basic Recurrent Neural Network">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dimensions-of-input-x" class="md-nav__link">
    Dimensions of input \(x\)
  </a>
  
    <nav class="md-nav" aria-label="Dimensions of input \(x\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-with-n_x-number-of-units" class="md-nav__link">
    Input with \(n_x\) number of units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#time-steps-of-size-t_x" class="md-nav__link">
    Time steps of size \(T_{x}\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batches-of-size-m" class="md-nav__link">
    Batches of size \(m\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-tensor-of-shape-n_xmt_x" class="md-nav__link">
    3D Tensor of shape \((n_{x},m,T_{x})\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#taking-a-2d-slice-for-each-time-step-xlangle-t-rangle" class="md-nav__link">
    Taking a 2D slice for each time step: \(x^{\langle t \rangle}\)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#definition-of-hidden-state-a" class="md-nav__link">
    Definition of hidden state \(a\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensions-of-hidden-state-a" class="md-nav__link">
    Dimensions of hidden state \(a\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensions-of-prediction-haty" class="md-nav__link">
    Dimensions of prediction \(\hat{y}\)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#steps" class="md-nav__link">
    Steps:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11-rnn-cell" class="md-nav__link">
    1.1 - RNN Cell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-1-rnn_cell_forward" class="md-nav__link">
    Exercise 1 - rnn_cell_forward
  </a>
  
    <nav class="md-nav" aria-label="Exercise 1 - rnn_cell_forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#additional-hints" class="md-nav__link">
    Additional Hints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-rnn-forward-pass" class="md-nav__link">
    1.2 - RNN Forward Pass
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-rnn_forward" class="md-nav__link">
    Exercise 2 - rnn_forward
  </a>
  
    <nav class="md-nav" aria-label="Exercise 2 - rnn_forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#additional-hints_1" class="md-nav__link">
    Additional Hints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#congratulations" class="md-nav__link">
    Congratulations!
  </a>
  
    <nav class="md-nav" aria-label="Congratulations!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#situations-when-this-rnn-will-perform-better" class="md-nav__link">
    Situations when this RNN will perform better:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-long-short-term-memory-lstm-network" class="md-nav__link">
    2 - Long Short-Term Memory (LSTM) Network
  </a>
  
    <nav class="md-nav" aria-label="2 - Long Short-Term Memory (LSTM) Network">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overview-of-gates-and-states" class="md-nav__link">
    Overview of gates and states
  </a>
  
    <nav class="md-nav" aria-label="Overview of gates and states">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#forget-gate-mathbfgamma_f" class="md-nav__link">
    Forget gate \(\mathbf{\Gamma}_{f}\)
  </a>
  
    <nav class="md-nav" aria-label="Forget gate \(\mathbf{\Gamma}_{f}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-the-equation" class="md-nav__link">
    Explanation of the equation:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-in-the-code" class="md-nav__link">
    Variable names in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#candidate-value-tildemathbfclangle-t-rangle" class="md-nav__link">
    Candidate value \(\tilde{\mathbf{c}}^{\langle t \rangle}\)
  </a>
  
    <nav class="md-nav" aria-label="Candidate value \(\tilde{\mathbf{c}}^{\langle t \rangle}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation_1" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-the-equation_1" class="md-nav__link">
    Explanation of the equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-in-the-code_1" class="md-nav__link">
    Variable names in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update-gate-mathbfgamma_i" class="md-nav__link">
    Update gate \(\mathbf{\Gamma}_{i}\)
  </a>
  
    <nav class="md-nav" aria-label="Update gate \(\mathbf{\Gamma}_{i}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation_2" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-the-equation_2" class="md-nav__link">
    Explanation of the equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-in-code-please-note-that-theyre-different-than-the-equations" class="md-nav__link">
    Variable names in code (Please note that they're different than the equations)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cell-state-mathbfclangle-t-rangle" class="md-nav__link">
    Cell state \(\mathbf{c}^{\langle t \rangle}\)
  </a>
  
    <nav class="md-nav" aria-label="Cell state \(\mathbf{c}^{\langle t \rangle}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation_3" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-equation" class="md-nav__link">
    Explanation of equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-and-shapes-in-the-code" class="md-nav__link">
    Variable names and shapes in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#output-gate-mathbfgamma_o" class="md-nav__link">
    Output gate \(\mathbf{\Gamma}_{o}\)
  </a>
  
    <nav class="md-nav" aria-label="Output gate \(\mathbf{\Gamma}_{o}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation_4" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-the-equation_3" class="md-nav__link">
    Explanation of the equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-in-the-code_2" class="md-nav__link">
    Variable names in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hidden-state-mathbfalangle-t-rangle" class="md-nav__link">
    Hidden state \(\mathbf{a}^{\langle t \rangle}\)
  </a>
  
    <nav class="md-nav" aria-label="Hidden state \(\mathbf{a}^{\langle t \rangle}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equation_5" class="md-nav__link">
    Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explanation-of-equation_1" class="md-nav__link">
    Explanation of equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variable-names-and-shapes-in-the-code_1" class="md-nav__link">
    Variable names  and shapes in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prediction-mathbfylangle-t-rangle_pred" class="md-nav__link">
    Prediction \(\mathbf{y}^{\langle t \rangle}_{pred}\)
  </a>
  
    <nav class="md-nav" aria-label="Prediction \(\mathbf{y}^{\langle t \rangle}_{pred}\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variable-names-and-shapes-in-the-code_2" class="md-nav__link">
    Variable names and shapes in the code
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#21-lstm-cell" class="md-nav__link">
    2.1 - LSTM Cell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-lstm_cell_forward" class="md-nav__link">
    Exercise 3 - lstm_cell_forward
  </a>
  
    <nav class="md-nav" aria-label="Exercise 3 - lstm_cell_forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#additional-hints_2" class="md-nav__link">
    Additional Hints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-forward-pass-for-lstm" class="md-nav__link">
    2.2 - Forward Pass for LSTM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-4-lstm_forward" class="md-nav__link">
    Exercise 4 - lstm_forward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#congratulations_1" class="md-nav__link">
    Congratulations!
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-backpropagation-in-recurrent-neural-networks-optional-ungraded" class="md-nav__link">
    3 - Backpropagation in Recurrent Neural Networks (OPTIONAL / UNGRADED)
  </a>
  
    <nav class="md-nav" aria-label="3 - Backpropagation in Recurrent Neural Networks (OPTIONAL / UNGRADED)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-basic-rnn-backward-pass" class="md-nav__link">
    3.1 - Basic RNN  Backward Pass
  </a>
  
    <nav class="md-nav" aria-label="3.1 - Basic RNN Backward Pass">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equations" class="md-nav__link">
    Equations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-5-rnn_cell_backward" class="md-nav__link">
    Exercise 5 - rnn_cell_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-6-rnn_backward" class="md-nav__link">
    Exercise 6 - rnn_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-lstm-backward-pass" class="md-nav__link">
    3.2 - LSTM Backward Pass
  </a>
  
    <nav class="md-nav" aria-label="3.2 - LSTM Backward Pass">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-one-step-backward" class="md-nav__link">
    1. One Step Backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-gate-derivatives" class="md-nav__link">
    2. Gate Derivatives
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-parameter-derivatives" class="md-nav__link">
    3. Parameter Derivatives
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-7-lstm_cell_backward" class="md-nav__link">
    Exercise 7 - lstm_cell_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-backward-pass-through-the-lstm-rnn" class="md-nav__link">
    3.3 Backward Pass through the LSTM RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-8-lstm_backward" class="md-nav__link">
    Exercise 8 - lstm_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#congratulations-on-completing-this-assignment" class="md-nav__link">
    Congratulations on completing this assignment!
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Dinosaurus_Island_Character_level_language_model/Dinosaurus_Island_Character_level_language_model/" class="md-nav__link">
        Dinosaurus Island Character level language model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Emojify/Emoji_v3a/" class="md-nav__link">
        Emoji v3a
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Improvise_a_Jazz_Solo_with_an_LSTM_Network/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4/" class="md-nav__link">
        Improvise a Jazz Solo with an LSTM Network v4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Neural_machine_translation_with_attention/Neural_machine_translation_with_attention_v4a/" class="md-nav__link">
        Neural machine translation with attention v4a
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Operations_on_Word_Vectors_Debiasing/Operations_on_word_vectors_v2a/" class="md-nav__link">
        Operations on word vectors v2a
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Machine Learning Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/C1_W1_Lab03_Model_Representation_Soln/" class="md-nav__link">
        C1 W1 Lab03 Model Representation Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/C1_W1_Lab04_Cost_function_Soln/" class="md-nav__link">
        C1 W1 Lab04 Cost function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/C1_W1_Lab05_Gradient_Descent_Soln/" class="md-nav__link">
        C1 W1 Lab05 Gradient Descent Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab01_Python_Jupyter_Soln/" class="md-nav__link">
        C1 W1 Lab01 Python Jupyter Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab02_Course_Preview_Soln/" class="md-nav__link">
        C1 W1 Lab02 Course Preview Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab03_Model_Representation_Soln/" class="md-nav__link">
        C1 W1 Lab03 Model Representation Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab04_Cost_function_Soln/" class="md-nav__link">
        C1 W1 Lab04 Cost function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab05_Gradient_Descent_Soln/" class="md-nav__link">
        C1 W1 Lab05 Gradient Descent Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Notes/Linear_Regression/" class="md-nav__link">
        Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W1/Notes/Overview/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab01_Python_Numpy_Vectorization_Soln/" class="md-nav__link">
        C1 W2 Lab01 Python Numpy Vectorization Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab02_Multiple_Variable_Soln/" class="md-nav__link">
        C1 W2 Lab02 Multiple Variable Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln/" class="md-nav__link">
        C1 W2 Lab03 Feature Scaling and Learning Rate Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab04_FeatEng_PolyReg_Soln/" class="md-nav__link">
        C1 W2 Lab04 FeatEng PolyReg Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab05_Sklearn_GD_Soln/" class="md-nav__link">
        C1 W2 Lab05 Sklearn GD Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/C1_W2_Lab06_Sklearn_Normal_Soln/" class="md-nav__link">
        C1 W2 Lab06 Sklearn Normal Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W2/Assignments/Linear_Regression/C1_W2_Linear_Regression/" class="md-nav__link">
        C1 W2 Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab01_Classification_Soln/" class="md-nav__link">
        C1 W3 Lab01 Classification Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab02_Sigmoid_function_Soln/" class="md-nav__link">
        C1 W3 Lab02 Sigmoid function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab03_Decision_Boundary_Soln/" class="md-nav__link">
        C1 W3 Lab03 Decision Boundary Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab04_LogisticLoss_Soln/" class="md-nav__link">
        C1 W3 Lab04 LogisticLoss Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab05_Cost_Function_Soln/" class="md-nav__link">
        C1 W3 Lab05 Cost Function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab07_Scikit_Learn_Soln/" class="md-nav__link">
        C1 W3 Lab07 Scikit Learn Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab08_Overfitting_Soln/" class="md-nav__link">
        C1 W3 Lab08 Overfitting Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C1/W3/Assignments/C1_W3_Lab09_Regularization_Soln/" class="md-nav__link">
        C1 W3 Lab09 Regularization Soln
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W1/Assignment/C2_W1_Assignment/" class="md-nav__link">
        C2 W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W1/Lab/C2_W1_Lab01_Neurons_and_Layers/" class="md-nav__link">
        C2 W1 Lab01 Neurons and Layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W1/Lab/C2_W1_Lab02_CoffeeRoasting_TF/" class="md-nav__link">
        C2 W1 Lab02 CoffeeRoasting TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W1/Lab/C2_W1_Lab03_CoffeeRoasting_Numpy/" class="md-nav__link">
        C2 W1 Lab03 CoffeeRoasting Numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W2/Assignment/C2_W2_Assignment/" class="md-nav__link">
        C2 W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W2/Lab/C2_W2_Multiclass_TF/" class="md-nav__link">
        C2 W2 Multiclass TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W2/Lab/C2_W2_Relu/" class="md-nav__link">
        C2 W2 Relu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W2/Lab/C2_W2_SoftMax/" class="md-nav__link">
        C2 W2 SoftMax
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W3/Assignment/C2_W3_Assignment/" class="md-nav__link">
        C2 W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C2/W4/Assignment/C2_W4_Decision_Tree_with_Markdown/" class="md-nav__link">
        C2 W4 Decision Tree with Markdown
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W1/Assignment/A1/C3_W1_KMeans_Assignment/" class="md-nav__link">
        C3 W1 KMeans Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W1/Assignment/A2/C3_W1_Anomaly_Detection/" class="md-nav__link">
        C3 W1 Anomaly Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W2/Assignment/A1/C3_W2_Collaborative_RecSys_Assignment/" class="md-nav__link">
        C3 W2 Collaborative RecSys Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W2/Assignment/A2/C3_W2_RecSysNN_Assignment/" class="md-nav__link">
        C3 W2 RecSysNN Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W3/Assignment/C3_W3_A1_Assignment/" class="md-nav__link">
        C3 W3 A1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../MLS/C3/W3/Lab/State-action%20value%20function%20example/" class="md-nav__link">
        State action value function example
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          TensorFlow Advanced Techniques Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow Advanced Techniques Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_1_functional-practice/" class="md-nav__link">
        C1 W1 Lab 1 functional practice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_2_multi-output/" class="md-nav__link">
        C1 W1 Lab 2 multi output
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_3_siamese-network/" class="md-nav__link">
        C1 W1 Lab 3 siamese network
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Multiple_Output_Models_using_the_Keras_Functional_API/C1W1_Assignment/" class="md-nav__link">
        C1W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Creating_a_Custom_Loss_Function/C1W2_Assignment/" class="md-nav__link">
        C1W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Labs/C1_W2_Lab_1_huber-loss/" class="md-nav__link">
        C1 W2 Lab 1 huber loss
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Labs/C1_W2_Lab_2_huber-object-loss/" class="md-nav__link">
        C1 W2 Lab 2 huber object loss
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Implement_a_Quadratic_Layer/C1W3_Assignment/" class="md-nav__link">
        C1W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_1_lambda-layer/" class="md-nav__link">
        C1 W3 Lab 1 lambda layer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_2_custom-dense-layer/" class="md-nav__link">
        C1 W3 Lab 2 custom dense layer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_3_custom-layer-activation/" class="md-nav__link">
        C1 W3 Lab 3 custom layer activation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Create_a_VGG_network/C1W4_Assignment/" class="md-nav__link">
        C1W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Labs/C1_W4_Lab_1_basic-model/" class="md-nav__link">
        C1 W4 Lab 1 basic model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Labs/C1_W4_Lab_2_resnet-example/" class="md-nav__link">
        C1 W4 Lab 2 resnet example
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Assignment/C2W1_Assignment/" class="md-nav__link">
        C2W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Labs/C2_W1_Lab_1_basic-tensors/" class="md-nav__link">
        C2 W1 Lab 1 basic tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Labs/C2_W1_Lab_2_gradient-tape-basics/" class="md-nav__link">
        C2 W1 Lab 2 gradient tape basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Assignment/C2W2_Assignment/" class="md-nav__link">
        C2W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Labs/C2_W2_Lab_1_training-basics/" class="md-nav__link">
        C2 W2 Lab 1 training basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Labs/C2_W2_Lab_2_training-categorical/" class="md-nav__link">
        C2 W2 Lab 2 training categorical
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Assignment/C2W3_Assignment/" class="md-nav__link">
        C2W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Labs/C2_W3_Lab_1_autograph-basics/" class="md-nav__link">
        C2 W3 Lab 1 autograph basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Labs/C2_W3_Lab_2-graphs-for-complex-code/" class="md-nav__link">
        C2 W3 Lab 2 graphs for complex code
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Assignment/C2W4_Assignment/" class="md-nav__link">
        C2W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_1_basic-mirrored-strategy/" class="md-nav__link">
        C2 W4 Lab 1 basic mirrored strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_2_multi-GPU-mirrored-strategy/" class="md-nav__link">
        C2 W4 Lab 2 multi GPU mirrored strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_3_using-TPU-strategy/" class="md-nav__link">
        C2 W4 Lab 3 using TPU strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_4_one-device-strategy/" class="md-nav__link">
        C2 W4 Lab 4 one device strategy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Assignment/C3W1_Assignment/" class="md-nav__link">
        C3W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Labs/C3_W1_Lab_2_Transfer_Learning_CIFAR_10/" class="md-nav__link">
        C3 W1 Lab 2 Transfer Learning CIFAR 10
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Labs/C3_W1_Lab_3_Object_Localization/" class="md-nav__link">
        C3 W1 Lab 3 Object Localization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Assignment/C3W2_Assignment/" class="md-nav__link">
        C3W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Labs/C3_W2_Lab_1_Simple_Object_Detection/" class="md-nav__link">
        C3 W2 Lab 1 Simple Object Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Labs/Object_Detection_Inference_on_TF_2_and_TF_Hub/" class="md-nav__link">
        Object Detection Inference on TF 2 and TF Hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Assignment/C3W3_Assignment/" class="md-nav__link">
        C3W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_1_VGG16_FCN8_CamVid/" class="md-nav__link">
        C3 W3 Lab 1 VGG16 FCN8 CamVid
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_2_OxfordPets_UNet/" class="md-nav__link">
        C3 W3 Lab 2 OxfordPets UNet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_3_Mask_RCNN_ImageSegmentation/" class="md-nav__link">
        C3 W3 Lab 3 Mask RCNN ImageSegmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Assignment/C3_W4_Assignment/" class="md-nav__link">
        C3 W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_1_FashionMNIST_CAM/" class="md-nav__link">
        C3 W4 Lab 1 FashionMNIST CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_2_CatsDogs_CAM/" class="md-nav__link">
        C3 W4 Lab 2 CatsDogs CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_3_Saliency/" class="md-nav__link">
        C3 W4 Lab 3 Saliency
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          C4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          C4
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Assignment/C4W1_Assignment/" class="md-nav__link">
        C4W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Labs/C4_W1_Lab_1_Neural_Style_Transfer/" class="md-nav__link">
        C4 W1 Lab 1 Neural Style Transfer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Labs/C4_W1_Lab_2_Fast_NST/" class="md-nav__link">
        C4 W1 Lab 2 Fast NST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Assignment/C4W2_Assignment/" class="md-nav__link">
        C4W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_1_FirstAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 1 FirstAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_2_MNIST_Autoencoder/" class="md-nav__link">
        C4 W2 Lab 2 MNIST Autoencoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_3_MNIST_DeepAutoencoder/" class="md-nav__link">
        C4 W2 Lab 3 MNIST DeepAutoencoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_4_FashionMNIST_CNNAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 4 FashionMNIST CNNAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_5_FashionMNIST_NoisyCNNAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 5 FashionMNIST NoisyCNNAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W3/Assignment/C4W3_Assignment/" class="md-nav__link">
        C4W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W3/Labs/C4_W3_Lab_1_VAE_MNIST/" class="md-nav__link">
        C4 W3 Lab 1 VAE MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Assignment/C4W4_Assignment/" class="md-nav__link">
        C4W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_1_First_GAN/" class="md-nav__link">
        C4 W4 Lab 1 First GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_2_First_DCGAN/" class="md-nav__link">
        C4 W4 Lab 2 First DCGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_3_CelebA_GAN_Experiments/" class="md-nav__link">
        C4 W4 Lab 3 CelebA GAN Experiments
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Generative Adversarial Networks Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Generative Adversarial Networks Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W1/Assignments/C1W1_Your_First_GAN/" class="md-nav__link">
        C1W1 Your First GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W1/Labs/Intro_to_PyTorch/" class="md-nav__link">
        Intro to PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W2/Assignments/C1_W2_Assignment/" class="md-nav__link">
        C1 W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W2/Labs/C1W2_Video_Generation_%28Optional%29/" class="md-nav__link">
        C1W2 Video Generation (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W3/Assignments/C1W3_WGAN_GP/" class="md-nav__link">
        C1W3 WGAN GP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W3/Labs/SNGAN/" class="md-nav__link">
        SNGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W4/Assignments/C1W4A_Build_a_Conditional_GAN/" class="md-nav__link">
        C1W4A Build a Conditional GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W4/Assignments/C1W4B_Controllable_Generation/" class="md-nav__link">
        C1W4B Controllable Generation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W1/Assignments/C2W1_Assignment/" class="md-nav__link">
        C2W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W1/Labs/PPL/" class="md-nav__link">
        PPL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W2/Assignments/C2W2_Assignment/" class="md-nav__link">
        C2W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W2/Labs/C2W2_VAE/" class="md-nav__link">
        C2W2 VAE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Assignments/C2W3_Assignment/" class="md-nav__link">
        C2W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Labs/BigGAN/" class="md-nav__link">
        BigGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Labs/StyleGAN2/" class="md-nav__link">
        StyleGAN2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W1/Assignments/C3W1_Assignment/" class="md-nav__link">
        C3W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W1/Labs/C3W1_Generative_Teaching_Networks_%28Optional%29/" class="md-nav__link">
        C3W1 Generative Teaching Networks (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Assignments/C3W2A_Assignment/" class="md-nav__link">
        C3W2A Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Assignments/C3W2B_Assignment/" class="md-nav__link">
        C3W2B Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Labs/C3W2_GauGAN_%28Optional%29/" class="md-nav__link">
        C3W2 GauGAN (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Labs/C3W2_Pix2PixHD_%28Optional%29/" class="md-nav__link">
        C3W2 Pix2PixHD (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W3/Assignments/C3W3_Assignment/" class="md-nav__link">
        C3W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W3/Labs/C3W3_MUNIT_%28Optional%29/" class="md-nav__link">
        C3W3 MUNIT (Optional)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Deep Learning AI
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning AI
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          Diffusion
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          Diffusion
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab1/L1_Sampling/" class="md-nav__link">
        L1 Sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab2/L2_Training/" class="md-nav__link">
        L2 Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab3/L3_Context/" class="md-nav__link">
        L3 Context
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab4/L4_FastSampling/" class="md-nav__link">
        L4 FastSampling
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          Prompt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          Prompt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l2-guidelines/" class="md-nav__link">
        L2 guidelines
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l4-summarizing/" class="md-nav__link">
        L4 summarizing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l5-inferring/" class="md-nav__link">
        L5 inferring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l6-transforming/" class="md-nav__link">
        L6 transforming
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l7-expanding/" class="md-nav__link">
        L7 expanding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l8-chatbot/" class="md-nav__link">
        L8 chatbot
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../../license/" class="md-nav__link">
        License
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script>
(function() {
  function addWidgetsRenderer() {
    var requireJsScript = document.createElement('script');
    requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';

    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var jupyterWidgetsScript = document.createElement('script');
    var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    jupyterWidgetsScript.src = widgetRendererSrc;

    document.body.appendChild(requireJsScript);
    document.body.appendChild(jupyterWidgetsScript);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table align="left" class="tfo-notebook-buttons">
<td style="border: none;">
<a target='blank' href='https://colab.research.google.com/github/Hari31416/Data-Sciences/blob/main/Notes/DLS/C5/Assignments/Building_a_Recurrent_Neural_Network_Step_by_Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb?hl=en'><img src='https://colab.research.google.com/img/colab_favicon_256px.png' width='28' height='28'/>Run on Google Colab</a></td>
<td style="border: none;">
<a target='blank' href='https://github.com/Hari31416/Data-Sciences/blob/main/Notes/DLS/C5/Assignments/Building_a_Recurrent_Neural_Network_Step_by_Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb'><img src='https://cdn.icon-icons.com/icons2/2368/PNG/512/github_logo_icon_143772.png' width='28' height='28'/>View on Github</a></td>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="building-your-recurrent-neural-network-step-by-step">Building your Recurrent Neural Network - Step by Step</h1>
<p>Welcome to Course 5's first assignment, where you'll be implementing key components of a Recurrent Neural Network, or RNN, in NumPy! </p>
<p>By the end of this assignment, you'll be able to:</p>
<ul>
<li>Define notation for building sequence models</li>
<li>Describe the architecture of a basic RNN</li>
<li>Identify the main components of an LSTM</li>
<li>Implement backpropagation through time for a basic RNN and an LSTM</li>
<li>Give examples of several types of RNN </li>
</ul>
<p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have "memory." They can read inputs <span class="arithmatex">\(x^{\langle t \rangle}\)</span> (such as words) one at a time, and remember some contextual information through the hidden layer activations that get passed from one time step to the next. This allows a unidirectional (one-way) RNN to take information from the past to process later inputs. A bidirectional (two-way) RNN can take context from both the past and the future, much like Marty McFly. </p>
<p><strong>Notation</strong>:
- Superscript <span class="arithmatex">\([l]\)</span> denotes an object associated with the <span class="arithmatex">\(l^{th}\)</span> layer. </p>
<ul>
<li>
<p>Superscript <span class="arithmatex">\((i)\)</span> denotes an object associated with the <span class="arithmatex">\(i^{th}\)</span> example. </p>
</li>
<li>
<p>Superscript <span class="arithmatex">\(\langle t \rangle\)</span> denotes an object at the <span class="arithmatex">\(t^{th}\)</span> time 
step. </p>
</li>
<li>
<p>Subscript <span class="arithmatex">\(i\)</span> denotes the <span class="arithmatex">\(i^{th}\)</span> entry of a vector.</p>
</li>
</ul>
<p><strong>Example</strong>:<br />
- <span class="arithmatex">\(a^{(2)[3]&lt;4&gt;}_5\)</span> denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step &lt;4&gt;, and 5th entry in the vector.</p>
<h4 id="pre-requisites">Pre-requisites</h4>
<ul>
<li>You should already be familiar with <code>numpy</code></li>
<li>To refresh your knowledge of numpy, you can review course 1 of the specialization "Neural Networks and Deep Learning":<ul>
<li>Specifically, review the week 2's practice assignment <a href="https://www.coursera.org/learn/neural-networks-deep-learning/programming/isoAV/python-basics-with-numpy">"Python Basics with Numpy (optional assignment)"</a></li>
</ul>
</li>
</ul>
<h4 id="be-careful-when-modifying-the-starter-code">Be careful when modifying the starter code!</h4>
<ul>
<li>When working on graded functions, please remember to only modify the code that is between:
<div class="highlight"><pre><span></span><code><span class="c1">#### START CODE HERE</span>
</code></pre></div>
and:
<div class="highlight"><pre><span></span><code><span class="c1">#### END CODE HERE</span>
</code></pre></div></li>
<li>In particular, avoid modifying the first line of graded routines. These start with:
<div class="highlight"><pre><span></span><code><span class="c1"># GRADED FUNCTION: routine_name</span>
</code></pre></div>
The automatic grader (autograder) needs these to locate the function - so even a change in spacing will cause issues with the autograder, returning 'failed' if any of these are modified or missing. Now, let's get started!</li>
</ul>
<h2 id="important-note-on-submission-to-the-autograder">Important Note on Submission to the AutoGrader</h2>
<p>Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:</p>
<ol>
<li>You have not added any <em>extra</em> <code>print</code> statement(s) in the assignment.</li>
<li>You have not added any <em>extra</em> code cell(s) in the assignment.</li>
<li>You have not changed any of the function parameters.</li>
<li>You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.</li>
<li>You are not changing the assignment code where it is not required, like creating <em>extra</em> variables.</li>
</ol>
<p>If you do any of the following, you will get something like, <code>Grader not found</code> (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these <a href="https://www.coursera.org/learn/nlp-sequence-models/supplement/qHIve/h-ow-to-refresh-your-workspace">instructions</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="table-of-content">Table of Content</h2>
<ul>
<li><a href="#0">Packages</a></li>
<li><a href="#1">1 - Forward Propagation for the Basic Recurrent Neural Network</a><ul>
<li><a href="#1-1">1.1 - RNN Cell</a><ul>
<li><a href="#ex-1">Exercise 1 - rnn_cell_forward</a></li>
</ul>
</li>
<li><a href="#1-2">1.2 - RNN Forward Pass</a><ul>
<li><a href="#ex-2">Exercise 2 - rnn_forward</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2 - Long Short-Term Memory (LSTM) Network</a><ul>
<li><a href="#2-1">2.1 - LSTM Cell</a><ul>
<li><a href="#ex-3">Exercise 3 - lstm_cell_forward</a></li>
</ul>
</li>
<li><a href="#2-2">2.2 - Forward Pass for LSTM</a><ul>
<li><a href="#ex-4">Exercise 4 - lstm_forward</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">3 - Backpropagation in Recurrent Neural Networks (OPTIONAL / UNGRADED)</a><ul>
<li><a href="#3-1">3.1 - Basic RNN Backward Pass</a><ul>
<li><a href="#ex-5">Exercise 5 - rnn_cell_backward</a></li>
<li><a href="#ex-6">Exercise 6 - rnn_backward</a></li>
</ul>
</li>
<li><a href="#3-2">3.2 - LSTM Backward Pass</a><ul>
<li><a href="#ex-7">Exercise 7 - lstm_cell_backward</a></li>
</ul>
</li>
<li><a href="#3-3">3.3 Backward Pass through the LSTM RNN</a><ul>
<li><a href="#ex-8">Exercise 8 - lstm_backward</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='0'></a></p>
<h2 id="packages">Packages</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">rnn_utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='1'></a></p>
<h2 id="1-forward-propagation-for-the-basic-recurrent-neural-network">1 - Forward Propagation for the Basic Recurrent Neural Network</h2>
<p>Later this week, you'll get a chance to generate music using an RNN! The basic RNN that you'll implement has the following structure: </p>
<p>In this example, <span class="arithmatex">\(T_x = T_y\)</span>. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/RNN.png" style="width:500;height:300px;">
<caption><center><font color='purple'><b>Figure 1</b>: Basic RNN model </center></caption></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="dimensions-of-input-x">Dimensions of input <span class="arithmatex">\(x\)</span></h3>
<h4 id="input-with-n_x-number-of-units">Input with <span class="arithmatex">\(n_x\)</span> number of units</h4>
<ul>
<li>For a single time step of a single input example, <span class="arithmatex">\(x^{(i) \langle t \rangle }\)</span> is a one-dimensional input vector</li>
<li>Using language as an example, a language with a 5000-word vocabulary could be one-hot encoded into a vector that has 5000 units.  So <span class="arithmatex">\(x^{(i)\langle t \rangle}\)</span> would have the shape (5000,)  </li>
<li>The notation <span class="arithmatex">\(n_x\)</span> is used here to denote the number of units in a single time step of a single training example</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="time-steps-of-size-t_x">Time steps of size <span class="arithmatex">\(T_{x}\)</span></h4>
<ul>
<li>A recurrent neural network has multiple time steps, which you'll index with <span class="arithmatex">\(t\)</span>.</li>
<li>In the lessons, you saw a single training example <span class="arithmatex">\(x^{(i)}\)</span> consisting of multiple time steps <span class="arithmatex">\(T_x\)</span>. In this notebook, <span class="arithmatex">\(T_{x}\)</span> will denote the number of timesteps in the longest sequence.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="batches-of-size-m">Batches of size <span class="arithmatex">\(m\)</span></h4>
<ul>
<li>Let's say we have mini-batches, each with 20 training examples  </li>
<li>To benefit from vectorization, you'll stack 20 columns of <span class="arithmatex">\(x^{(i)}\)</span> examples</li>
<li>For example, this tensor has the shape (5000,20,10) </li>
<li>You'll use <span class="arithmatex">\(m\)</span> to denote the number of training examples  </li>
<li>So, the shape of a mini-batch is <span class="arithmatex">\((n_x,m,T_x)\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3d-tensor-of-shape-n_xmt_x">3D Tensor of shape <span class="arithmatex">\((n_{x},m,T_{x})\)</span></h4>
<ul>
<li>The 3-dimensional tensor <span class="arithmatex">\(x\)</span> of shape <span class="arithmatex">\((n_x,m,T_x)\)</span> represents the input <span class="arithmatex">\(x\)</span> that is fed into the RNN</li>
</ul>
<h4 id="taking-a-2d-slice-for-each-time-step-xlangle-t-rangle">Taking a 2D slice for each time step: <span class="arithmatex">\(x^{\langle t \rangle}\)</span></h4>
<ul>
<li>At each time step, you'll use a mini-batch of training examples (not just a single example)</li>
<li>So, for each time step <span class="arithmatex">\(t\)</span>, you'll use a 2D slice of shape <span class="arithmatex">\((n_x,m)\)</span></li>
<li>This 2D slice is referred to as <span class="arithmatex">\(x^{\langle t \rangle}\)</span>.  The variable name in the code is <code>xt</code>.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="definition-of-hidden-state-a">Definition of hidden state <span class="arithmatex">\(a\)</span></h3>
<ul>
<li>The activation <span class="arithmatex">\(a^{\langle t \rangle}\)</span> that is passed to the RNN from one time step to another is called a "hidden state."</li>
</ul>
<h3 id="dimensions-of-hidden-state-a">Dimensions of hidden state <span class="arithmatex">\(a\)</span></h3>
<ul>
<li>Similar to the input tensor <span class="arithmatex">\(x\)</span>, the hidden state for a single training example is a vector of length <span class="arithmatex">\(n_{a}\)</span></li>
<li>If you include a mini-batch of <span class="arithmatex">\(m\)</span> training examples, the shape of a mini-batch is <span class="arithmatex">\((n_{a},m)\)</span></li>
<li>When you include the time step dimension, the shape of the hidden state is <span class="arithmatex">\((n_{a}, m, T_x)\)</span></li>
<li>You'll loop through the time steps with index <span class="arithmatex">\(t\)</span>, and work with a 2D slice of the 3D tensor  </li>
<li>This 2D slice is referred to as <span class="arithmatex">\(a^{\langle t \rangle}\)</span></li>
<li>In the code, the variable names used are either <code>a_prev</code> or <code>a_next</code>, depending on the function being implemented</li>
<li>The shape of this 2D slice is <span class="arithmatex">\((n_{a}, m)\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="dimensions-of-prediction-haty">Dimensions of prediction <span class="arithmatex">\(\hat{y}\)</span></h3>
<ul>
<li>Similar to the inputs and hidden states, <span class="arithmatex">\(\hat{y}\)</span> is a 3D tensor of shape <span class="arithmatex">\((n_{y}, m, T_{y})\)</span><ul>
<li><span class="arithmatex">\(n_{y}\)</span>: number of units in the vector representing the prediction</li>
<li><span class="arithmatex">\(m\)</span>: number of examples in a mini-batch</li>
<li><span class="arithmatex">\(T_{y}\)</span>: number of time steps in the prediction</li>
</ul>
</li>
<li>For a single time step <span class="arithmatex">\(t\)</span>, a 2D slice <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> has shape <span class="arithmatex">\((n_{y}, m)\)</span></li>
<li>In the code, the variable names are:<ul>
<li><code>y_pred</code>: <span class="arithmatex">\(\hat{y}\)</span> </li>
<li><code>yt_pred</code>: <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's how you can implement an RNN: </p>
<h3 id="steps">Steps:</h3>
<ol>
<li>Implement the calculations needed for one time step of the RNN.</li>
<li>Implement a loop over <span class="arithmatex">\(T_x\)</span> time steps in order to process all the inputs, one at a time. </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='1-1'></a></p>
<h3 id="11-rnn-cell">1.1 - RNN Cell</h3>
<p>You can think of the recurrent neural network as the repeated use of a single cell. First, you'll implement the computations for a single time step. The following figure describes the operations for a single time step of an RNN cell: </p>
<p><img src="images/rnn_step_forward_figure2_v3a.png" style="width:700px;height:300px;">
<caption><center><font color='purple'><b>Figure 2</b>: Basic RNN cell. Takes as input <span class="arithmatex">\(x^{\langle t \rangle}\)</span> (current input) and <span class="arithmatex">\(a^{\langle t - 1\rangle}\)</span> (previous hidden state containing information from the past), and outputs <span class="arithmatex">\(a^{\langle t \rangle}\)</span> which is given to the next RNN cell and also used to predict <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> 
</center></caption></p>
<p><strong><code>RNN cell</code> versus <code>RNN_cell_forward</code></strong>:
* Note that an RNN cell outputs the hidden state <span class="arithmatex">\(a^{\langle t \rangle}\)</span>.<br />
    * <code>RNN cell</code> is shown in the figure as the inner box with solid lines<br />
* The function that you'll implement, <code>rnn_cell_forward</code>, also calculates the prediction <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span>
    * <code>RNN_cell_forward</code> is shown in the figure as the outer box with dashed lines</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='ex-1'></a></p>
<h3 id="exercise-1-rnn_cell_forward">Exercise 1 - rnn_cell_forward</h3>
<p>Implement the RNN cell described in Figure 2.</p>
<p><strong>Instructions</strong>:
1. Compute the hidden state with tanh activation: <span class="arithmatex">\(a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)\)</span>
2. Using your new hidden state <span class="arithmatex">\(a^{\langle t \rangle}\)</span>, compute the prediction <span class="arithmatex">\(\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)\)</span>. (The function <code>softmax</code> is provided)
3. Store <span class="arithmatex">\((a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)\)</span> in a <code>cache</code>
4. Return <span class="arithmatex">\(a^{\langle t \rangle}\)</span> , <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> and <code>cache</code></p>
<h4 id="additional-hints">Additional Hints</h4>
<ul>
<li>A little more information on <a href="https://numpy.org/devdocs/reference/generated/numpy.tanh.html">numpy.tanh</a></li>
<li>In this assignment, there's an existing <code>softmax</code> function for you to use.  It's located in the file 'rnn_utils.py' and has already been imported.</li>
<li>For matrix multiplication, use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: rnn_cell_forward</span>

<span class="k">def</span> <span class="nf">rnn_cell_forward</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a single forward step of the RNN-cell as described in Figure (2)</span>

<span class="sd">    Arguments:</span>
<span class="sd">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span>
<span class="sd">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span>
<span class="sd">    parameters -- python dictionary containing:</span>
<span class="sd">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span>
<span class="sd">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span>
<span class="sd">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span>
<span class="sd">                        ba --  Bias, numpy array of shape (n_a, 1)</span>
<span class="sd">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span>
<span class="sd">    Returns:</span>
<span class="sd">    a_next -- next hidden state, of shape (n_a, m)</span>
<span class="sd">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span>
<span class="sd">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve parameters from &quot;parameters&quot;</span>
    <span class="n">Wax</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wax&quot;</span><span class="p">]</span>
    <span class="n">Waa</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Waa&quot;</span><span class="p">]</span>
    <span class="n">Wya</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wya&quot;</span><span class="p">]</span>
    <span class="n">ba</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;ba&quot;</span><span class="p">]</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;by&quot;</span><span class="p">]</span>

    <span class="c1">### START CODE HERE ### (≈2 lines)</span>
    <span class="c1"># compute next activation state using the formula given above</span>
    <span class="n">a_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Waa</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wax</span><span class="p">,</span> <span class="n">xt</span><span class="p">)</span><span class="o">+</span><span class="n">ba</span><span class="p">)</span>
    <span class="c1"># compute output of the current cell using the formula given above</span>
    <span class="n">yt_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wya</span><span class="p">,</span> <span class="n">a_next</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># store values you need for backward propagation in cache</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_next</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a_next</span><span class="p">,</span> <span class="n">yt_pred</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xt_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">a_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Waa&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wax&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wya&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;ba&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">a_next_tmp</span><span class="p">,</span> <span class="n">yt_pred_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span> <span class="o">=</span> <span class="n">rnn_cell_forward</span><span class="p">(</span><span class="n">xt_tmp</span><span class="p">,</span> <span class="n">a_prev_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a_next[4] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a_next_tmp</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a_next.shape = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a_next_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;yt_pred[1] =</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">yt_pred_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;yt_pred.shape = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">yt_pred_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">rnn_cell_forward_tests</span><span class="p">(</span><span class="n">rnn_cell_forward</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>a_next[4] = 
 [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978
 -0.18887155  0.99815551  0.6531151   0.82872037]
a_next.shape = 
 (5, 10)
yt_pred[1] =
 [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212
 0.36920224 0.9966312  0.9982559  0.17746526]
yt_pred.shape = 
 (2, 10)
<span class="ansi-green-intense-fg">All tests passed
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>: 
<div class="highlight"><pre><span></span><code><span class="n">a_next</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> 
 <span class="p">[</span> <span class="mf">0.59584544</span>  <span class="mf">0.18141802</span>  <span class="mf">0.61311866</span>  <span class="mf">0.99808218</span>  <span class="mf">0.85016201</span>  <span class="mf">0.99980978</span>
 <span class="o">-</span><span class="mf">0.18887155</span>  <span class="mf">0.99815551</span>  <span class="mf">0.6531151</span>   <span class="mf">0.82872037</span><span class="p">]</span>
<span class="n">a_next</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> 
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">yt_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span>
 <span class="p">[</span> <span class="mf">0.9888161</span>   <span class="mf">0.01682021</span>  <span class="mf">0.21140899</span>  <span class="mf">0.36817467</span>  <span class="mf">0.98988387</span>  <span class="mf">0.88945212</span>
  <span class="mf">0.36920224</span>  <span class="mf">0.9966312</span>   <span class="mf">0.9982559</span>   <span class="mf">0.17746526</span><span class="p">]</span>
<span class="n">yt_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> 
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='1-2'></a></p>
<h3 id="12-rnn-forward-pass">1.2 - RNN Forward Pass</h3>
<ul>
<li>A recurrent neural network (RNN) is a repetition of the RNN cell that you've just built. <ul>
<li>If your input sequence of data is 10 time steps long, then you will re-use the RNN cell 10 times </li>
</ul>
</li>
<li>Each cell takes two inputs at each time step:<ul>
<li><span class="arithmatex">\(a^{\langle t-1 \rangle}\)</span>: The hidden state from the previous cell</li>
<li><span class="arithmatex">\(x^{\langle t \rangle}\)</span>: The current time step's input data</li>
</ul>
</li>
<li>It has two outputs at each time step:<ul>
<li>A hidden state (<span class="arithmatex">\(a^{\langle t \rangle}\)</span>)</li>
<li>A prediction (<span class="arithmatex">\(y^{\langle t \rangle}\)</span>)</li>
</ul>
</li>
<li>The weights and biases <span class="arithmatex">\((W_{aa}, b_{a}, W_{ax}, b_{x})\)</span> are re-used each time step <ul>
<li>They are maintained between calls to <code>rnn_cell_forward</code> in the 'parameters' dictionary</li>
</ul>
</li>
</ul>
<p><img src="images/rnn_forward_sequence_figure3_v3a.png" style="width:800px;height:180px;">
<caption><center><font color='purple'><b>Figure 3</b>: Basic RNN. The input sequence <span class="arithmatex">\(x = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, ..., x^{\langle T_x \rangle})\)</span>  is carried over <span class="arithmatex">\(T_x\)</span> time steps. The network outputs <span class="arithmatex">\(y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, ..., y^{\langle T_x \rangle})\)</span>. </center></caption></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='ex-2'></a></p>
<h3 id="exercise-2-rnn_forward">Exercise 2 - rnn_forward</h3>
<p>Implement the forward propagation of the RNN described in Figure 3.</p>
<p><strong>Instructions</strong>:
* Create a 3D array of zeros, <span class="arithmatex">\(a\)</span> of shape <span class="arithmatex">\((n_{a}, m, T_{x})\)</span> that will store all the hidden states computed by the RNN
* Create a 3D array of zeros, <span class="arithmatex">\(\hat{y}\)</span>, of shape <span class="arithmatex">\((n_{y}, m, T_{x})\)</span> that will store the predictions<br />
    - Note that in this case, <span class="arithmatex">\(T_{y} = T_{x}\)</span> (the prediction and input have the same number of time steps)
* Initialize the 2D hidden state <code>a_next</code> by setting it equal to the initial hidden state, <span class="arithmatex">\(a_{0}\)</span>
* At each time step <span class="arithmatex">\(t\)</span>:
    - Get <span class="arithmatex">\(x^{\langle t \rangle}\)</span>, which is a 2D slice of <span class="arithmatex">\(x\)</span> for a single time step <span class="arithmatex">\(t\)</span>
        - <span class="arithmatex">\(x^{\langle t \rangle}\)</span> has shape <span class="arithmatex">\((n_{x}, m)\)</span>
        - <span class="arithmatex">\(x\)</span> has shape <span class="arithmatex">\((n_{x}, m, T_{x})\)</span>
    - Update the 2D hidden state <span class="arithmatex">\(a^{\langle t \rangle}\)</span> (variable name <code>a_next</code>), the prediction <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> and the cache by running <code>rnn_cell_forward</code>
        - <span class="arithmatex">\(a^{\langle t \rangle}\)</span> has shape <span class="arithmatex">\((n_{a}, m)\)</span>
    - Store the 2D hidden state in the 3D tensor <span class="arithmatex">\(a\)</span>, at the <span class="arithmatex">\(t^{th}\)</span> position
        - <span class="arithmatex">\(a\)</span> has shape <span class="arithmatex">\((n_{a}, m, T_{x})\)</span>
    - Store the 2D <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> prediction (variable name <code>yt_pred</code>) in the 3D tensor <span class="arithmatex">\(\hat{y}_{pred}\)</span> at the <span class="arithmatex">\(t^{th}\)</span> position
        - <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> has shape <span class="arithmatex">\((n_{y}, m)\)</span>
        - <span class="arithmatex">\(\hat{y}\)</span> has shape <span class="arithmatex">\((n_{y}, m, T_x)\)</span>
    - Append the cache to the list of caches
* Return the 3D tensor <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(\hat{y}\)</span>, as well as the list of caches</p>
<h4 id="additional-hints_1">Additional Hints</h4>
<ul>
<li>Some helpful documentation on <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html">np.zeros</a></li>
<li>If you have a 3 dimensional numpy array and are indexing by its third dimension, you can use array slicing like this: <code>var_name[:,:,i]</code></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: rnn_forward</span>

<span class="k">def</span> <span class="nf">rnn_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span>
<span class="sd">    a0 -- Initial hidden state, of shape (n_a, m)</span>
<span class="sd">    parameters -- python dictionary containing:</span>
<span class="sd">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span>
<span class="sd">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span>
<span class="sd">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span>
<span class="sd">                        ba --  Bias numpy array of shape (n_a, 1)</span>
<span class="sd">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span>

<span class="sd">    Returns:</span>
<span class="sd">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span>
<span class="sd">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span>
<span class="sd">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Initialize &quot;caches&quot; which will contain the list of all caches</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Retrieve dimensions from shapes of x and parameters[&quot;Wya&quot;]</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_y</span><span class="p">,</span> <span class="n">n_a</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wya&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>

    <span class="c1"># initialize &quot;a&quot; and &quot;y_pred&quot; with zeros (≈2 lines)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span><span class="p">))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span><span class="p">))</span>

    <span class="c1"># Initialize a_next (≈1 line)</span>
    <span class="n">a_next</span> <span class="o">=</span> <span class="n">a0</span>

    <span class="c1"># loop over all time-steps</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T_x</span><span class="p">):</span>
        <span class="c1"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span>
        <span class="n">a_next</span><span class="p">,</span> <span class="n">yt_pred</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">rnn_cell_forward</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">],</span> <span class="n">a_next</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span>
        <span class="n">a</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_next</span>
        <span class="c1"># Save the value of the prediction in y (≈1 line)</span>
        <span class="n">y_pred</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">yt_pred</span>
        <span class="c1"># Append &quot;cache&quot; to &quot;caches&quot; (≈1 line)</span>
        <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># store values needed for backward propagation in cache</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">(</span><span class="n">caches</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">caches</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">a0_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Waa&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wax&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wya&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;ba&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">a_tmp</span><span class="p">,</span> <span class="n">y_pred_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">,</span> <span class="n">a0_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[4][1] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a_tmp</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a.shape = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_pred[1][3] =</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_pred_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_pred.shape = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_pred_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;caches[1][1][3] =</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">caches_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;len(caches) = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches_tmp</span><span class="p">))</span>

<span class="c1">#UNIT TEST    </span>
<span class="n">rnn_forward_test</span><span class="p">(</span><span class="n">rnn_forward</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>a[4][1] = 
 [-0.99999375  0.77911235 -0.99861469 -0.99833267]
a.shape = 
 (5, 10, 4)
y_pred[1][3] =
 [0.79560373 0.86224861 0.11118257 0.81515947]
y_pred.shape = 
 (2, 10, 4)
caches[1][1][3] =
 [-1.1425182  -0.34934272 -0.20889423  0.58662319]
len(caches) = 
 2
<span class="ansi-green-intense-fg">All tests passed
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">a</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> 
 <span class="p">[</span><span class="o">-</span><span class="mf">0.99999375</span>  <span class="mf">0.77911235</span> <span class="o">-</span><span class="mf">0.99861469</span> <span class="o">-</span><span class="mf">0.99833267</span><span class="p">]</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> 
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span>
 <span class="p">[</span> <span class="mf">0.79560373</span>  <span class="mf">0.86224861</span>  <span class="mf">0.11118257</span>  <span class="mf">0.81515947</span><span class="p">]</span>
<span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> 
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">1.1425182</span>  <span class="o">-</span><span class="mf">0.34934272</span> <span class="o">-</span><span class="mf">0.20889423</span>  <span class="mf">0.58662319</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span> <span class="o">=</span> 
 <span class="mi">2</span>
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="congratulations">Congratulations!</h3>
<p>You've successfully built the forward propagation of a recurrent neural network from scratch. Nice work! </p>
<h4 id="situations-when-this-rnn-will-perform-better">Situations when this RNN will perform better:</h4>
<ul>
<li>This will work well enough for some applications, but it suffers from vanishing gradients. </li>
<li>The RNN works best when each output <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> can be estimated using "local" context.  </li>
<li>"Local" context refers to information that is close to the prediction's time step <span class="arithmatex">\(t\)</span>.</li>
<li>More formally, local context refers to inputs <span class="arithmatex">\(x^{\langle t' \rangle}\)</span> and predictions <span class="arithmatex">\(\hat{y}^{\langle t \rangle}\)</span> where <span class="arithmatex">\(t'\)</span> is close to <span class="arithmatex">\(t\)</span>.</li>
</ul>
<p><font color='blue'><b>What you should remember:</b>
* The recurrent neural network, or RNN, is essentially the repeated use of a single cell.
* A basic RNN reads inputs one at a time, and remembers information through the hidden layer activations (hidden states) that are passed from one time step to the next.
    * The time step dimension determines how many times to re-use the RNN cell
* Each cell takes two inputs at each time step:
    * The hidden state from the previous cell
    * The current time step's input data
* Each cell has two outputs at each time step:
    * A hidden state 
    * A prediction
</font></p>
<p>In the next section, you'll build a more complex model, the LSTM, which is better at addressing vanishing gradients. The LSTM is better able to remember a piece of information and save it for many time steps.  </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='2'></a></p>
<h2 id="2-long-short-term-memory-lstm-network">2 - Long Short-Term Memory (LSTM) Network</h2>
<p>The following figure shows the operations of an LSTM cell:</p>
<p><img src="images/LSTM_figure4_v3a.png" style="width:500;height:400px;">
<caption><center><font color='purple'><b>Figure 4</b>: LSTM cell. This tracks and updates a "cell state," or memory variable <span class="arithmatex">\(c^{\langle t \rangle}\)</span> at every time step, which can be different from <span class="arithmatex">\(a^{\langle t \rangle}\)</span>.<br />
Note, the <span class="arithmatex">\(softmax^{}\)</span> includes a dense layer and softmax.</center></caption></p>
<p>Similar to the RNN example above, you'll begin by implementing the LSTM cell for a single time step. Then, you'll iteratively call it from inside a "for loop" to have it process an input with <span class="arithmatex">\(T_x\)</span> time steps. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="overview-of-gates-and-states">Overview of gates and states</h3>
<h4 id="forget-gate-mathbfgamma_f">Forget gate <span class="arithmatex">\(\mathbf{\Gamma}_{f}\)</span></h4>
<ul>
<li>Let's assume you are reading words in a piece of text, and plan to use an LSTM to keep track of grammatical structures, such as whether the subject is singular ("puppy") or plural ("puppies"). </li>
<li>If the subject changes its state (from a singular word to a plural word), the memory of the previous state becomes outdated, so you'll "forget" that outdated state.</li>
<li>The "forget gate" is a tensor containing values between 0 and 1.<ul>
<li>If a unit in the forget gate has a value close to 0, the LSTM will "forget" the stored state in the corresponding unit of the previous cell state.</li>
<li>If a unit in the forget gate has a value close to 1, the LSTM will mostly remember the corresponding value in the stored state.</li>
</ul>
</li>
</ul>
<h5 id="equation">Equation</h5>
<div class="arithmatex">\[\mathbf{\Gamma}_f^{\langle t \rangle} = \sigma(\mathbf{W}_f[\mathbf{a}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_f)\tag{1} \]</div>
<h5 id="explanation-of-the-equation">Explanation of the equation:</h5>
<ul>
<li><span class="arithmatex">\(\mathbf{W_{f}}\)</span> contains weights that govern the forget gate's behavior. </li>
<li>The previous time step's hidden state <span class="arithmatex">\([a^{\langle t-1 \rangle}\)</span> and current time step's input <span class="arithmatex">\(x^{\langle t \rangle}]\)</span> are concatenated together and multiplied by <span class="arithmatex">\(\mathbf{W_{f}}\)</span>. </li>
<li>A sigmoid function is used to make each of the gate tensor's values <span class="arithmatex">\(\mathbf{\Gamma}_f^{\langle t \rangle}\)</span> range from 0 to 1.</li>
<li>The forget gate  <span class="arithmatex">\(\mathbf{\Gamma}_f^{\langle t \rangle}\)</span> has the same dimensions as the previous cell state <span class="arithmatex">\(c^{\langle t-1 \rangle}\)</span>. </li>
<li>This means that the two can be multiplied together, element-wise.</li>
<li>Multiplying the tensors <span class="arithmatex">\(\mathbf{\Gamma}_f^{\langle t \rangle} * \mathbf{c}^{\langle t-1 \rangle}\)</span> is like applying a mask over the previous cell state.</li>
<li>If a single value in <span class="arithmatex">\(\mathbf{\Gamma}_f^{\langle t \rangle}\)</span> is 0 or close to 0, then the product is close to 0.<ul>
<li>This keeps the information stored in the corresponding unit in <span class="arithmatex">\(\mathbf{c}^{\langle t-1 \rangle}\)</span> from being remembered for the next time step.</li>
</ul>
</li>
<li>Similarly, if one value is close to 1, the product is close to the original value in the previous cell state.<ul>
<li>The LSTM will keep the information from the corresponding unit of <span class="arithmatex">\(\mathbf{c}^{\langle t-1 \rangle}\)</span>, to be used in the next time step.</li>
</ul>
</li>
</ul>
<h5 id="variable-names-in-the-code">Variable names in the code</h5>
<p>The variable names in the code are similar to the equations, with slight differences.<br />
* <code>Wf</code>: forget gate weight <span class="arithmatex">\(\mathbf{W}_{f}\)</span>
* <code>bf</code>: forget gate bias <span class="arithmatex">\(\mathbf{b}_{f}\)</span>
* <code>ft</code>: forget gate <span class="arithmatex">\(\Gamma_f^{\langle t \rangle}\)</span></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="candidate-value-tildemathbfclangle-t-rangle">Candidate value <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span></h4>
<ul>
<li>The candidate value is a tensor containing information from the current time step that <strong>may</strong> be stored in the current cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span>.</li>
<li>The parts of the candidate value that get passed on depend on the update gate.</li>
<li>The candidate value is a tensor containing values that range from -1 to 1.</li>
<li>The tilde "~" is used to differentiate the candidate <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span> from the cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span>.</li>
</ul>
<h5 id="equation_1">Equation</h5>
<div class="arithmatex">\[\mathbf{\tilde{c}}^{\langle t \rangle} = \tanh\left( \mathbf{W}_{c} [\mathbf{a}^{\langle t - 1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_{c} \right) \tag{3}\]</div>
<h5 id="explanation-of-the-equation_1">Explanation of the equation</h5>
<ul>
<li>The <em>tanh</em> function produces values between -1 and 1.</li>
</ul>
<h5 id="variable-names-in-the-code_1">Variable names in the code</h5>
<ul>
<li><code>cct</code>: candidate value <span class="arithmatex">\(\mathbf{\tilde{c}}^{\langle t \rangle}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="update-gate-mathbfgamma_i">Update gate <span class="arithmatex">\(\mathbf{\Gamma}_{i}\)</span></h4>
<ul>
<li>You use the update gate to decide what aspects of the candidate <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span> to add to the cell state <span class="arithmatex">\(c^{\langle t \rangle}\)</span>.</li>
<li>The update gate decides what parts of a "candidate" tensor <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span> are passed onto the cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span>.</li>
<li>The update gate is a tensor containing values between 0 and 1.<ul>
<li>When a unit in the update gate is close to 1, it allows the value of the candidate <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span> to be passed onto the hidden state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span></li>
<li>When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.</li>
</ul>
</li>
<li>Notice that the subscript "i" is used and not "u", to follow the convention used in the literature.</li>
</ul>
<h5 id="equation_2">Equation</h5>
<div class="arithmatex">\[\mathbf{\Gamma}_i^{\langle t \rangle} = \sigma(\mathbf{W}_i[a^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_i)\tag{2} \]</div>
<h5 id="explanation-of-the-equation_2">Explanation of the equation</h5>
<ul>
<li>Similar to the forget gate, here <span class="arithmatex">\(\mathbf{\Gamma}_i^{\langle t \rangle}\)</span>, the sigmoid produces values between 0 and 1.</li>
<li>The update gate is multiplied element-wise with the candidate, and this product (<span class="arithmatex">\(\mathbf{\Gamma}_{i}^{\langle t \rangle} * \tilde{c}^{\langle t \rangle}\)</span>) is used in determining the cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span>.</li>
</ul>
<h5 id="variable-names-in-code-please-note-that-theyre-different-than-the-equations">Variable names in code (Please note that they're different than the equations)</h5>
<p>In the code, you'll use the variable names found in the academic literature.  These variables don't use "u" to denote "update".
* <code>Wi</code> is the update gate weight <span class="arithmatex">\(\mathbf{W}_i\)</span> (not "Wu") 
* <code>bi</code> is the update gate bias <span class="arithmatex">\(\mathbf{b}_i\)</span> (not "bu")
* <code>it</code> is the update gate <span class="arithmatex">\(\mathbf{\Gamma}_i^{\langle t \rangle}\)</span> (not "ut")</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="cell-state-mathbfclangle-t-rangle">Cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span></h4>
<ul>
<li>The cell state is the "memory" that gets passed onto future time steps.</li>
<li>The new cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span> is a combination of the previous cell state and the candidate value.</li>
</ul>
<h5 id="equation_3">Equation</h5>
<div class="arithmatex">\[ \mathbf{c}^{\langle t \rangle} = \mathbf{\Gamma}_f^{\langle t \rangle}* \mathbf{c}^{\langle t-1 \rangle} + \mathbf{\Gamma}_{i}^{\langle t \rangle} *\mathbf{\tilde{c}}^{\langle t \rangle} \tag{4} \]</div>
<h5 id="explanation-of-equation">Explanation of equation</h5>
<ul>
<li>The previous cell state <span class="arithmatex">\(\mathbf{c}^{\langle t-1 \rangle}\)</span> is adjusted (weighted) by the forget gate <span class="arithmatex">\(\mathbf{\Gamma}_{f}^{\langle t \rangle}\)</span></li>
<li>and the candidate value <span class="arithmatex">\(\tilde{\mathbf{c}}^{\langle t \rangle}\)</span>, adjusted (weighted) by the update gate <span class="arithmatex">\(\mathbf{\Gamma}_{i}^{\langle t \rangle}\)</span></li>
</ul>
<h5 id="variable-names-and-shapes-in-the-code">Variable names and shapes in the code</h5>
<ul>
<li><code>c</code>: cell state, including all time steps, <span class="arithmatex">\(\mathbf{c}\)</span> shape <span class="arithmatex">\((n_{a}, m, T_x)\)</span></li>
<li><code>c_next</code>: new (next) cell state, <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span> shape <span class="arithmatex">\((n_{a}, m)\)</span></li>
<li><code>c_prev</code>: previous cell state, <span class="arithmatex">\(\mathbf{c}^{\langle t-1 \rangle}\)</span>, shape <span class="arithmatex">\((n_{a}, m)\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="output-gate-mathbfgamma_o">Output gate <span class="arithmatex">\(\mathbf{\Gamma}_{o}\)</span></h4>
<ul>
<li>The output gate decides what gets sent as the prediction (output) of the time step.</li>
<li>The output gate is like the other gates, in that it contains values that range from 0 to 1.</li>
</ul>
<h5 id="equation_4">Equation</h5>
<div class="arithmatex">\[ \mathbf{\Gamma}_o^{\langle t \rangle}=  \sigma(\mathbf{W}_o[\mathbf{a}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_{o})\tag{5}\]</div>
<h5 id="explanation-of-the-equation_3">Explanation of the equation</h5>
<ul>
<li>The output gate is determined by the previous hidden state <span class="arithmatex">\(\mathbf{a}^{\langle t-1 \rangle}\)</span> and the current input <span class="arithmatex">\(\mathbf{x}^{\langle t \rangle}\)</span></li>
<li>The sigmoid makes the gate range from 0 to 1.</li>
</ul>
<h5 id="variable-names-in-the-code_2">Variable names in the code</h5>
<ul>
<li><code>Wo</code>: output gate weight, <span class="arithmatex">\(\mathbf{W_o}\)</span></li>
<li><code>bo</code>: output gate bias, <span class="arithmatex">\(\mathbf{b_o}\)</span></li>
<li><code>ot</code>: output gate, <span class="arithmatex">\(\mathbf{\Gamma}_{o}^{\langle t \rangle}\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="hidden-state-mathbfalangle-t-rangle">Hidden state <span class="arithmatex">\(\mathbf{a}^{\langle t \rangle}\)</span></h4>
<ul>
<li>The hidden state gets passed to the LSTM cell's next time step.</li>
<li>It is used to determine the three gates (<span class="arithmatex">\(\mathbf{\Gamma}_{f}, \mathbf{\Gamma}_{u}, \mathbf{\Gamma}_{o}\)</span>) of the next time step.</li>
<li>The hidden state is also used for the prediction <span class="arithmatex">\(y^{\langle t \rangle}\)</span>.</li>
</ul>
<h5 id="equation_5">Equation</h5>
<div class="arithmatex">\[ \mathbf{a}^{\langle t \rangle} = \mathbf{\Gamma}_o^{\langle t \rangle} * \tanh(\mathbf{c}^{\langle t \rangle})\tag{6} \]</div>
<h5 id="explanation-of-equation_1">Explanation of equation</h5>
<ul>
<li>The hidden state <span class="arithmatex">\(\mathbf{a}^{\langle t \rangle}\)</span> is determined by the cell state <span class="arithmatex">\(\mathbf{c}^{\langle t \rangle}\)</span> in combination with the output gate <span class="arithmatex">\(\mathbf{\Gamma}_{o}\)</span>.</li>
<li>The cell state state is passed through the <code>tanh</code> function to rescale values between -1 and 1.</li>
<li>The output gate acts like a "mask" that either preserves the values of <span class="arithmatex">\(\tanh(\mathbf{c}^{\langle t \rangle})\)</span> or keeps those values from being included in the hidden state <span class="arithmatex">\(\mathbf{a}^{\langle t \rangle}\)</span></li>
</ul>
<h5 id="variable-names-and-shapes-in-the-code_1">Variable names  and shapes in the code</h5>
<ul>
<li><code>a</code>: hidden state, including time steps.  <span class="arithmatex">\(\mathbf{a}\)</span> has shape <span class="arithmatex">\((n_{a}, m, T_{x})\)</span></li>
<li><code>a_prev</code>: hidden state from previous time step. <span class="arithmatex">\(\mathbf{a}^{\langle t-1 \rangle}\)</span> has shape <span class="arithmatex">\((n_{a}, m)\)</span></li>
<li><code>a_next</code>: hidden state for next time step.  <span class="arithmatex">\(\mathbf{a}^{\langle t \rangle}\)</span> has shape <span class="arithmatex">\((n_{a}, m)\)</span> </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="prediction-mathbfylangle-t-rangle_pred">Prediction <span class="arithmatex">\(\mathbf{y}^{\langle t \rangle}_{pred}\)</span></h4>
<ul>
<li>The prediction in this use case is a classification, so you'll use a softmax.</li>
</ul>
<p>The equation is:
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}^{\langle t \rangle}_{pred} = \textrm{softmax}(\mathbf{W}_{y} \mathbf{a}^{\langle t \rangle} + \mathbf{b}_{y})\)</span>\)</span></p>
<h5 id="variable-names-and-shapes-in-the-code_2">Variable names and shapes in the code</h5>
<ul>
<li><code>y_pred</code>: prediction, including all time steps. <span class="arithmatex">\(\mathbf{y}_{pred}\)</span> has shape <span class="arithmatex">\((n_{y}, m, T_{x})\)</span>.  Note that <span class="arithmatex">\((T_{y} = T_{x})\)</span> for this example.</li>
<li><code>yt_pred</code>: prediction for the current time step <span class="arithmatex">\(t\)</span>. <span class="arithmatex">\(\mathbf{y}^{\langle t \rangle}_{pred}\)</span> has shape <span class="arithmatex">\((n_{y}, m)\)</span></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='2-1'></a></p>
<h3 id="21-lstm-cell">2.1 - LSTM Cell</h3>
<p><a name='ex-3'></a></p>
<h3 id="exercise-3-lstm_cell_forward">Exercise 3 - lstm_cell_forward</h3>
<p>Implement the LSTM cell described in Figure 4.</p>
<p><strong>Instructions</strong>:
1. Concatenate the hidden state <span class="arithmatex">\(a^{\langle t-1 \rangle}\)</span> and input <span class="arithmatex">\(x^{\langle t \rangle}\)</span> into a single matrix:  </p>
<div class="arithmatex">\[concat = \begin{bmatrix} a^{\langle t-1 \rangle} \\ x^{\langle t \rangle} \end{bmatrix}\]</div>
<ol>
<li>Compute all formulas (1 through 6) for the gates, hidden state, and cell state.</li>
<li>Compute the prediction <span class="arithmatex">\(y^{\langle t \rangle}\)</span>.</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="additional-hints_2">Additional Hints</h4>
<ul>
<li>You can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html">numpy.concatenate</a>.  Check which value to use for the <code>axis</code> parameter.</li>
<li>The functions <code>sigmoid()</code> and <code>softmax</code> are imported from <code>rnn_utils.py</code>.</li>
<li>Some docs for <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html">numpy.tanh</a></li>
<li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot</a> for matrix multiplication.</li>
<li>Notice that the variable names <code>Wi</code>, <code>bi</code> refer to the weights and biases of the <strong>update</strong> gate.  There are no variables named "Wu" or "bu" in this function.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: lstm_cell_forward</span>

<span class="k">def</span> <span class="nf">lstm_cell_forward</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span>

<span class="sd">    Arguments:</span>
<span class="sd">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span>
<span class="sd">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span>
<span class="sd">    c_prev -- Memory state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span>
<span class="sd">    parameters -- python dictionary containing:</span>
<span class="sd">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bc --  Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span>
<span class="sd">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span>

<span class="sd">    Returns:</span>
<span class="sd">    a_next -- next hidden state, of shape (n_a, m)</span>
<span class="sd">    c_next -- next memory state, of shape (n_a, m)</span>
<span class="sd">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span>
<span class="sd">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span>

<span class="sd">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span>
<span class="sd">          c stands for the cell state (memory)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve parameters from &quot;parameters&quot;</span>
    <span class="n">Wf</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wf&quot;</span><span class="p">]</span> <span class="c1"># forget gate weight</span>
    <span class="n">bf</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bf&quot;</span><span class="p">]</span>
    <span class="n">Wi</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wi&quot;</span><span class="p">]</span> <span class="c1"># update gate weight (notice the variable name)</span>
    <span class="n">bi</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bi&quot;</span><span class="p">]</span> <span class="c1"># (notice the variable name)</span>
    <span class="n">Wc</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wc&quot;</span><span class="p">]</span> <span class="c1"># candidate value weight</span>
    <span class="n">bc</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bc&quot;</span><span class="p">]</span>
    <span class="n">Wo</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wo&quot;</span><span class="p">]</span> <span class="c1"># output gate weight</span>
    <span class="n">bo</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bo&quot;</span><span class="p">]</span>
    <span class="n">Wy</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wy&quot;</span><span class="p">]</span> <span class="c1"># prediction weight</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;by&quot;</span><span class="p">]</span>

    <span class="c1"># Retrieve dimensions from shapes of xt and Wy</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">xt</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_y</span><span class="p">,</span> <span class="n">n_a</span> <span class="o">=</span> <span class="n">Wy</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Concatenate a_prev and xt (≈1 line)</span>
    <span class="n">concat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">a_prev</span><span class="p">,</span> <span class="n">xt</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span>
    <span class="n">ft</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wf</span><span class="p">,</span> <span class="n">concat</span><span class="p">)</span><span class="o">+</span><span class="n">bf</span><span class="p">)</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wi</span><span class="p">,</span> <span class="n">concat</span><span class="p">)</span><span class="o">+</span><span class="n">bi</span><span class="p">)</span>
    <span class="n">cct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wc</span><span class="p">,</span> <span class="n">concat</span><span class="p">)</span><span class="o">+</span><span class="n">bc</span><span class="p">)</span>
    <span class="n">c_next</span> <span class="o">=</span> <span class="n">ft</span><span class="o">*</span><span class="n">c_prev</span> <span class="o">+</span> <span class="n">it</span><span class="o">*</span><span class="n">cct</span>
    <span class="n">ot</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wo</span><span class="p">,</span> <span class="n">concat</span><span class="p">)</span><span class="o">+</span><span class="n">bo</span><span class="p">)</span>
    <span class="n">a_next</span> <span class="o">=</span> <span class="n">ot</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_next</span><span class="p">)</span>

    <span class="c1"># Compute prediction of the LSTM cell (≈1 line)</span>
    <span class="n">yt_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wy</span><span class="p">,</span> <span class="n">a_next</span><span class="p">)</span><span class="o">+</span><span class="n">by</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># store values needed for backward propagation in cache</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_next</span><span class="p">,</span> <span class="n">c_next</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">,</span> <span class="n">ft</span><span class="p">,</span> <span class="n">it</span><span class="p">,</span> <span class="n">cct</span><span class="p">,</span> <span class="n">ot</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a_next</span><span class="p">,</span> <span class="n">c_next</span><span class="p">,</span> <span class="n">yt_pred</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xt_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">a_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">c_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">a_next_tmp</span><span class="p">,</span> <span class="n">c_next_tmp</span><span class="p">,</span> <span class="n">yt_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span> <span class="o">=</span> <span class="n">lstm_cell_forward</span><span class="p">(</span><span class="n">xt_tmp</span><span class="p">,</span> <span class="n">a_prev_tmp</span><span class="p">,</span> <span class="n">c_prev_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a_next[4] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a_next_tmp</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a_next.shape = &quot;</span><span class="p">,</span> <span class="n">a_next_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c_next[2] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">c_next_tmp</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c_next.shape = &quot;</span><span class="p">,</span> <span class="n">c_next_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;yt[1] =&quot;</span><span class="p">,</span> <span class="n">yt_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;yt.shape = &quot;</span><span class="p">,</span> <span class="n">yt_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cache[1][3] =</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cache_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;len(cache) = &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cache_tmp</span><span class="p">))</span>

<span class="c1"># UNIT TEST</span>
<span class="n">lstm_cell_forward_test</span><span class="p">(</span><span class="n">lstm_cell_forward</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>a_next[4] = 
 [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482
  0.76566531  0.34631421 -0.00215674  0.43827275]
a_next.shape =  (5, 10)
c_next[2] = 
 [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942
  0.76449811 -0.0981561  -0.74348425 -0.26810932]
c_next.shape =  (5, 10)
yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381
 0.00943007 0.12666353 0.39380172 0.07828381]
yt.shape =  (2, 10)
cache[1][3] =
 [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874
  0.07651101 -1.03752894  1.41219977 -0.37647422]
len(cache) =  10
<span class="ansi-green-intense-fg">All tests passed
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">a_next</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> 
 <span class="p">[</span><span class="o">-</span><span class="mf">0.66408471</span>  <span class="mf">0.0036921</span>   <span class="mf">0.02088357</span>  <span class="mf">0.22834167</span> <span class="o">-</span><span class="mf">0.85575339</span>  <span class="mf">0.00138482</span>
  <span class="mf">0.76566531</span>  <span class="mf">0.34631421</span> <span class="o">-</span><span class="mf">0.00215674</span>  <span class="mf">0.43827275</span><span class="p">]</span>
<span class="n">a_next</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">c_next</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> 
 <span class="p">[</span> <span class="mf">0.63267805</span>  <span class="mf">1.00570849</span>  <span class="mf">0.35504474</span>  <span class="mf">0.20690913</span> <span class="o">-</span><span class="mf">1.64566718</span>  <span class="mf">0.11832942</span>
  <span class="mf">0.76449811</span> <span class="o">-</span><span class="mf">0.0981561</span>  <span class="o">-</span><span class="mf">0.74348425</span> <span class="o">-</span><span class="mf">0.26810932</span><span class="p">]</span>
<span class="n">c_next</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">yt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.79913913</span>  <span class="mf">0.15986619</span>  <span class="mf">0.22412122</span>  <span class="mf">0.15606108</span>  <span class="mf">0.97057211</span>  <span class="mf">0.31146381</span>
  <span class="mf">0.00943007</span>  <span class="mf">0.12666353</span>  <span class="mf">0.39380172</span>  <span class="mf">0.07828381</span><span class="p">]</span>
<span class="n">yt</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.16263996</span>  <span class="mf">1.03729328</span>  <span class="mf">0.72938082</span> <span class="o">-</span><span class="mf">0.54101719</span>  <span class="mf">0.02752074</span> <span class="o">-</span><span class="mf">0.30821874</span>
  <span class="mf">0.07651101</span> <span class="o">-</span><span class="mf">1.03752894</span>  <span class="mf">1.41219977</span> <span class="o">-</span><span class="mf">0.37647422</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">=</span>  <span class="mi">10</span>
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='2-2'></a></p>
<h3 id="22-forward-pass-for-lstm">2.2 - Forward Pass for LSTM</h3>
<p>Now that you have implemented one step of an LSTM, you can iterate this over it using a for loop to process a sequence of <span class="arithmatex">\(T_x\)</span> inputs. </p>
<p><img src="images/LSTM_rnn.png" style="width:500;height:300px;">
<caption><center><font color='purple'><b>Figure 5</b>: LSTM over multiple time steps. </center></caption></p>
<p><a name='ex-4'></a>    </p>
<h3 id="exercise-4-lstm_forward">Exercise 4 - lstm_forward</h3>
<p>Implement <code>lstm_forward()</code> to run an LSTM over <span class="arithmatex">\(T_x\)</span> time steps. </p>
<p><strong>Instructions</strong>
* Get the dimensions <span class="arithmatex">\(n_x, n_a, n_y, m, T_x\)</span> from the shape of the variables: <code>x</code> and <code>parameters</code>
* Initialize the 3D tensors <span class="arithmatex">\(a\)</span>, <span class="arithmatex">\(c\)</span> and <span class="arithmatex">\(y\)</span>
    - <span class="arithmatex">\(a\)</span>: hidden state, shape <span class="arithmatex">\((n_{a}, m, T_{x})\)</span>
    - <span class="arithmatex">\(c\)</span>: cell state, shape <span class="arithmatex">\((n_{a}, m, T_{x})\)</span>
    - <span class="arithmatex">\(y\)</span>: prediction, shape <span class="arithmatex">\((n_{y}, m, T_{x})\)</span> (Note that <span class="arithmatex">\(T_{y} = T_{x}\)</span> in this example)
    - <strong>Note</strong> Setting one variable equal to the other is a "copy by reference".  In other words, don't do <code>c = a', otherwise both these variables point to the same underlying variable.
* Initialize the 2D tensor $a^{\langle t \rangle}$ 
    - $a^{\langle t \rangle}$ stores the hidden state for time step $t$.  The variable name is</code>a_next<code>.
    - $a^{\langle 0 \rangle}$, the initial hidden state at time step 0, is passed in when calling the function. The variable name is</code>a0<code>.
    - $a^{\langle t \rangle}$ and $a^{\langle 0 \rangle}$ represent a single time step, so they both have the shape  $(n_{a}, m)$ 
    - Initialize $a^{\langle t \rangle}$ by setting it to the initial hidden state ($a^{\langle 0 \rangle}$) that is passed into the function.
* Initialize $c^{\langle t \rangle}$ with zeros. 
    - The variable name is</code>c_next<code>- $c^{\langle t \rangle}$ represents a single time step, so its shape is $(n_{a}, m)$
    - **Note**: create</code>c_next<code>as its own variable with its own location in memory.  Do not initialize it as a slice of the 3D tensor $c$.  In other words, **don't** do</code>c_next = c[:,:,0]<code>.
* For each time step, do the following:
    - From the 3D tensor $x$, get a 2D slice $x^{\langle t \rangle}$ at time step $t$
    - Call the</code>lstm_cell_forward` function that you defined previously, to get the hidden state, cell state, prediction, and cache
    - Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors
    - Append the cache to the list of caches</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: lstm_forward</span>

<span class="k">def</span> <span class="nf">lstm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span>
<span class="sd">    a0 -- Initial hidden state, of shape (n_a, m)</span>
<span class="sd">    parameters -- python dictionary containing:</span>
<span class="sd">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bc -- Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span>
<span class="sd">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span>
<span class="sd">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span>

<span class="sd">    Returns:</span>
<span class="sd">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span>
<span class="sd">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span>
<span class="sd">    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)</span>
<span class="sd">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Initialize &quot;caches&quot;, which will track the list of all the caches</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">Wy</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;Wy&#39;</span><span class="p">]</span> <span class="c1"># saving parameters[&#39;Wy&#39;] in a local variable in case students use Wy instead of parameters[&#39;Wy&#39;]</span>
    <span class="c1"># Retrieve dimensions from shapes of x and parameters[&#39;Wy&#39;] (≈2 lines)</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_y</span><span class="p">,</span> <span class="n">n_a</span> <span class="o">=</span> <span class="n">Wy</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># initialize &quot;a&quot;, &quot;c&quot; and &quot;y&quot; with zeros (≈3 lines)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span><span class="p">))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span><span class="p">))</span>

    <span class="c1"># Initialize a_next and c_next (≈2 lines)</span>
    <span class="n">a_next</span> <span class="o">=</span> <span class="n">a0</span>
    <span class="n">c_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

    <span class="c1"># loop over all time-steps</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T_x</span><span class="p">):</span>
        <span class="c1"># Get the 2D slice &#39;xt&#39; from the 3D input &#39;x&#39; at time step &#39;t&#39;</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span>
        <span class="c1"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span>
        <span class="n">a_next</span><span class="p">,</span> <span class="n">c_next</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">lstm_cell_forward</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">a_next</span><span class="p">,</span> <span class="n">c_next</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span>
        <span class="n">a</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_next</span>
        <span class="c1"># Save the value of the next cell state (≈1 line)</span>
        <span class="n">c</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">c_next</span>
        <span class="c1"># Save the value of the prediction in y (≈1 line)</span>
        <span class="n">y</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">yt</span>
        <span class="c1"># Append the cache into caches (≈1 line)</span>
        <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># store values needed for backward propagation in cache</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">(</span><span class="n">caches</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">caches</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">a0_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">a_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">c_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span> <span class="o">=</span> <span class="n">lstm_forward</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">,</span> <span class="n">a0_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[4][3][6] = &quot;</span><span class="p">,</span> <span class="n">a_tmp</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">6</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a.shape = &quot;</span><span class="p">,</span> <span class="n">a_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y[1][4][3] =&quot;</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">4</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.shape = &quot;</span><span class="p">,</span> <span class="n">y_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;caches[1][1][1] =</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">caches_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c[1][2][1]&quot;</span><span class="p">,</span> <span class="n">c_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;len(caches) = &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches_tmp</span><span class="p">))</span>

<span class="c1"># UNIT TEST    </span>
<span class="n">lstm_forward_test</span><span class="p">(</span><span class="n">lstm_forward</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>a[4][3][6] =  0.17211776753291672
a.shape =  (5, 10, 7)
y[1][4][3] = 0.9508734618501101
y.shape =  (2, 10, 7)
caches[1][1][1] =
 [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139
  0.41005165]
c[1][2][1] -0.8555449167181981
len(caches) =  2
<span class="ansi-green-intense-fg">All tests passed
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">a</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span>  <span class="mf">0.172117767533</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">4</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.95087346185</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span>
 <span class="p">[</span> <span class="mf">0.82797464</span>  <span class="mf">0.23009474</span>  <span class="mf">0.76201118</span> <span class="o">-</span><span class="mf">0.22232814</span> <span class="o">-</span><span class="mf">0.20075807</span>  <span class="mf">0.18656139</span>
  <span class="mf">0.41005165</span><span class="p">]</span>
<span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span><span class="mf">0.855544916718</span>
<span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span> <span class="o">=</span>  <span class="mi">2</span>
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="congratulations_1">Congratulations!</h3>
<p>You have now implemented the forward passes for both the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. The framework will take care of the rest. </p>
<p><font color='blue'><b>What you should remember</b>:</p>
<ul>
<li>An LSTM is similar to an RNN in that they both use hidden states to pass along information, but an LSTM also uses a cell state, which is like a long-term memory, to help deal with the issue of vanishing gradients</li>
<li>An LSTM cell consists of a cell state, or long-term memory, a hidden state, or short-term memory, along with 3 gates that constantly update the relevancy of its inputs:<ul>
<li>A <b>forget</b> gate, which decides which input units should be remembered and passed along. It's a tensor with values between 0 and 1. <ul>
<li>If a unit has a value close to 0, the LSTM will "forget" the stored state in the previous cell state.</li>
<li>If it has a value close to 1, the LSTM will mostly remember the corresponding value.</li>
</ul>
</li>
<li>An <b>update</b> gate, again a tensor containing values between 0 and 1. It decides on what information to throw away, and what new information to add.<ul>
<li>When a unit in the update gate is close to 1, the value of its candidate is passed on to the hidden state.</li>
<li>When a unit in the update gate is close to 0, it's prevented from being passed onto the hidden state.</li>
</ul>
</li>
<li>And an <b>output</b> gate, which decides what gets sent as the output of the time step
</font> </li>
</ul>
</li>
</ul>
<p>Let's recap all you've accomplished so far. You have: </p>
<ul>
<li>Used notation for building sequence models</li>
<li>Become familiar with the architecture of a basic RNN and an LSTM, and can describe their components</li>
</ul>
<p>The rest of this notebook is optional, and will not be graded, but as always, you are encouraged to push your own understanding! Good luck and have fun. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='3'></a>    </p>
<h2 id="3-backpropagation-in-recurrent-neural-networks-optional-ungraded">3 - Backpropagation in Recurrent Neural Networks (OPTIONAL / UNGRADED)</h2>
<p>In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If, however, you are an expert in calculus (or are just curious) and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. </p>
<p>When in an earlier <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph">course</a>  you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated, and so were not derived in lecture. However, they're briefly presented for your viewing pleasure below. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that this notebook does not implement the backward path from the Loss 'J' backwards to 'a'. This would have included the dense layer and softmax, which are a part of the forward path. This is assumed to be calculated elsewhere and the result passed to <code>rnn_backward</code> in 'da'. It is further assumed that loss has been adjusted for batch size (m) and division by the number of examples is not required here.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This section is optional and ungraded, because it's more difficult and has fewer details regarding its implementation. Note that this section only implements key elements of the full path! </p>
<p>Onward, brave one: </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='3-1'></a>    </p>
<h3 id="31-basic-rnn-backward-pass">3.1 - Basic RNN  Backward Pass</h3>
<p>Begin by computing the backward pass for the basic RNN cell. Then, in the following sections, iterate through the cells.</p>
<p><img src="images/rnn_backward_overview_3a_1.png" style="width:500;height:300px;"> <br>
<caption><center><font color='purple'><b>Figure 6</b>: The RNN cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function <span class="arithmatex">\(J\)</span> backpropagates through the time steps of the RNN by following the chain rule from calculus. Internal to the cell, the chain rule is also used to calculate <span class="arithmatex">\((\frac{\partial J}{\partial W_{ax}},\frac{\partial J}{\partial W_{aa}},\frac{\partial J}{\partial b})\)</span> to update the parameters <span class="arithmatex">\((W_{ax}, W_{aa}, b_a)\)</span>. The operation can utilize the cached results from the forward path. </center></caption></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall from lecture that the shorthand for the partial derivative of cost relative to a variable is <code>dVariable</code>. For example, <span class="arithmatex">\(\frac{\partial J}{\partial W_{ax}}\)</span> is <span class="arithmatex">\(dW_{ax}\)</span>. This will be used throughout the remaining sections.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/rnn_cell_backward_3a_c.png" style="width:800;height:500px;"> <br>
<caption><center><font color='purple'><b>Figure 7</b>: This implementation of <code>rnn_cell_backward</code> does <strong>not</strong> include the output dense layer and softmax which are included in <code>rnn_cell_forward</code>.  </p>
<p><span class="arithmatex">\(da_{next}\)</span> is <span class="arithmatex">\(\frac{\partial{J}}{\partial a^{\langle t \rangle}}\)</span> and includes loss from previous stages and current stage output logic. The addition shown in green will be part of your implementation of <code>rnn_backward</code>.  </center></caption></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="equations">Equations</h5>
<p>To compute <code>rnn_cell_backward</code>, you can use the following equations. It's a good exercise to derive them by hand. Here, <span class="arithmatex">\(*\)</span> denotes element-wise multiplication while the absence of a symbol indicates matrix multiplication.</p>
<div class="arithmatex">\[\begin{align}
\displaystyle a^{\langle t \rangle} &amp;= \tanh(W_{ax} x^{\langle t \rangle} + W_{aa} a^{\langle t-1 \rangle} + b_{a})\tag{-} \\[8pt]
\displaystyle \frac{\partial \tanh(x)} {\partial x} &amp;= 1 - \tanh^2(x) \tag{-} \\[8pt]
\displaystyle {dtanh} &amp;= da_{next} * ( 1 - \tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})) \tag{0} \\[8pt]
\displaystyle  {dW_{ax}} &amp;= dtanh \cdot x^{\langle t \rangle T}\tag{1} \\[8pt]
\displaystyle dW_{aa} &amp;= dtanh \cdot a^{\langle t-1 \rangle T}\tag{2} \\[8pt]
\displaystyle db_a&amp; = \sum_{batch}dtanh\tag{3} \\[8pt]
\displaystyle dx^{\langle t \rangle} &amp;= { W_{ax}}^T \cdot dtanh\tag{4} \\[8pt]
\displaystyle da_{prev} &amp;= { W_{aa}}^T \cdot dtanh\tag{5}
\end{align}\]</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='ex-5'></a></p>
<h3 id="exercise-5-rnn_cell_backward">Exercise 5 - rnn_cell_backward</h3>
<p>Implementing <code>rnn_cell_backward</code>.</p>
<p>The results can be computed directly by implementing the equations above. However, you have an option to simplify them by computing 'dz' and using the chain rule.<br />
This can be further simplified by noting that <span class="arithmatex">\(\tanh(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})\)</span> was computed and saved as <code>a_next</code> in the forward pass. </p>
<p>To calculate <code>dba</code>, the 'batch' above is a sum across all 'm' examples (axis= 1). Note that you should use the <code>keepdims = True</code> option.</p>
<p>It may be worthwhile to review Course 1 <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a>  through  <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional">Backpropagation Intuition</a>, which decompose the calculation into steps using the chain rule.<br />
Matrix vector derivatives are described <a href="http://cs231n.stanford.edu/vecDerivs.pdf">here</a>, though the equations above incorporate the required transformations.</p>
<p><strong>Note</strong>: <code>rnn_cell_backward</code> does <strong>not</strong> include the calculation of loss from <span class="arithmatex">\(y \langle t \rangle\)</span>. This is incorporated into the incoming <code>da_next</code>. This is a slight mismatch with <code>rnn_cell_forward</code>, which includes a dense layer and softmax. </p>
<p><strong>Note on the code</strong>: </p>
<p><span class="arithmatex">\(\displaystyle dx^{\langle t \rangle}\)</span> is represented by dxt, <br />
<span class="arithmatex">\(\displaystyle d W_{ax}\)</span> is represented by dWax, <br />
<span class="arithmatex">\(\displaystyle da_{prev}\)</span> is represented by da_prev,  <br />
<span class="arithmatex">\(\displaystyle dW_{aa}\)</span> is represented by dWaa, <br />
<span class="arithmatex">\(\displaystyle db_{a}\)</span> is represented by dba, <br />
<code>dz</code> is not derived above but can optionally be derived by students to simplify the repeated calculations.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNGRADED FUNCTION: rnn_cell_backward</span>

<span class="k">def</span> <span class="nf">rnn_cell_backward</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the backward pass for the RNN-cell (single time-step).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    da_next -- Gradient of loss with respect to next hidden state</span>
<span class="sd">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span>

<span class="sd">    Returns:</span>
<span class="sd">    gradients -- python dictionary containing:</span>
<span class="sd">                        dx -- Gradients of input data, of shape (n_x, m)</span>
<span class="sd">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span>
<span class="sd">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span>
<span class="sd">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span>
<span class="sd">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve values from cache</span>
    <span class="p">(</span><span class="n">a_next</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>

    <span class="c1"># Retrieve values from parameters</span>
    <span class="n">Wax</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wax&quot;</span><span class="p">]</span>
    <span class="n">Waa</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Waa&quot;</span><span class="p">]</span>
    <span class="n">Wya</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;Wya&quot;</span><span class="p">]</span>
    <span class="n">ba</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;ba&quot;</span><span class="p">]</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;by&quot;</span><span class="p">]</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># compute the gradient of dtanh term using a_next and da_next (≈1 line)</span>
    <span class="n">dtanh</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># compute the gradient of the loss with respect to Wax (≈2 lines)</span>
    <span class="n">dxt</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWax</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># compute the gradient with respect to Waa (≈2 lines)</span>
    <span class="n">da_prev</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWaa</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># compute the gradient with respect to b (≈1 line)</span>
    <span class="n">dba</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Store the gradients in a python dictionary</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dxt&quot;</span><span class="p">:</span> <span class="n">dxt</span><span class="p">,</span> <span class="s2">&quot;da_prev&quot;</span><span class="p">:</span> <span class="n">da_prev</span><span class="p">,</span> <span class="s2">&quot;dWax&quot;</span><span class="p">:</span> <span class="n">dWax</span><span class="p">,</span> <span class="s2">&quot;dWaa&quot;</span><span class="p">:</span> <span class="n">dWaa</span><span class="p">,</span> <span class="s2">&quot;dba&quot;</span><span class="p">:</span> <span class="n">dba</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xt_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">a_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wax&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Waa&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wya&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;ba&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a_next_tmp</span><span class="p">,</span> <span class="n">yt_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span> <span class="o">=</span> <span class="n">rnn_cell_forward</span><span class="p">(</span><span class="n">xt_tmp</span><span class="p">,</span> <span class="n">a_prev_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>

<span class="n">da_next_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">gradients_tmp</span> <span class="o">=</span> <span class="n">rnn_cell_backward</span><span class="p">(</span><span class="n">da_next_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dxt</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dxt&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dxt</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dxt&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da_prev</span><span class="se">\&quot;</span><span class="s2">][2][3] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da_prev&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da_prev</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da_prev&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWax</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWax&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWax</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWax&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWaa</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWaa&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWaa</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWaa&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dba</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dba&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dba</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dba&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<table>
    <tr>
        <td>
            <b>gradients["dxt"][1][2]</b> =
        </td>
        <td>
           -1.3872130506
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dxt"].shape</b> =
        </td>
        <td>
           (3, 10)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da_prev"][2][3]</b> =
        </td>
        <td>
           -0.152399493774
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da_prev"].shape</b> =
        </td>
        <td>
           (5, 10)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWax"][3][1]</b> =
        </td>
        <td>
           0.410772824935
        </td>
    </tr>
            <tr>
        <td>
            <b>gradients["dWax"].shape</b> =
        </td>
        <td>
           (5, 3)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWaa"][1][2]</b> = 
        </td>
        <td>
           1.15034506685
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWaa"].shape</b> =
        </td>
        <td>
           (5, 5)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dba"][4]</b> = 
        </td>
        <td>
           [ 0.20023491]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dba"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='ex-6'></a></p>
<h3 id="exercise-6-rnn_backward">Exercise 6 - rnn_backward</h3>
<p>Computing the gradients of the cost with respect to <span class="arithmatex">\(a^{\langle t \rangle}\)</span> at every time step <span class="arithmatex">\(t\)</span> is useful because it is what helps the gradient backpropagate to the previous RNN cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall <span class="arithmatex">\(db_a\)</span>, <span class="arithmatex">\(dW_{aa}\)</span>, <span class="arithmatex">\(dW_{ax}\)</span> and you store <span class="arithmatex">\(dx\)</span>.</p>
<p><strong>Instructions</strong>:</p>
<p>Implement the <code>rnn_backward</code> function. Initialize the return variables with zeros first, and then loop through all the time steps while calling the <code>rnn_cell_backward</code> at each time time step, updating the other variables accordingly.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Note that this notebook does not implement the backward path from the Loss 'J' backwards to 'a'. <ul>
<li>This would have included the dense layer and softmax which are a part of the forward path. </li>
<li>This is assumed to be calculated elsewhere and the result passed to <code>rnn_backward</code> in 'da'. </li>
<li>You must combine this with the loss from the previous stages when calling <code>rnn_cell_backward</code> (see figure 7 above).</li>
</ul>
</li>
<li>It is further assumed that loss has been adjusted for batch size (m).<ul>
<li>Therefore, division by the number of examples is not required here.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNGRADED FUNCTION: rnn_backward</span>

<span class="k">def</span> <span class="nf">rnn_backward</span><span class="p">(</span><span class="n">da</span><span class="p">,</span> <span class="n">caches</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward pass for a RNN over an entire sequence of input data.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span>
<span class="sd">    caches -- tuple containing information from the forward pass (rnn_forward)</span>

<span class="sd">    Returns:</span>
<span class="sd">    gradients -- python dictionary containing:</span>
<span class="sd">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span>
<span class="sd">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span>
<span class="sd">                        dWax -- Gradient w.r.t the input&#39;s weight matrix, numpy-array of shape (n_a, n_x)</span>
<span class="sd">                        dWaa -- Gradient w.r.t the hidden state&#39;s weight matrix, numpy-arrayof shape (n_a, n_a)</span>
<span class="sd">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">### START CODE HERE ###</span>

    <span class="c1"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span>
    <span class="p">(</span><span class="n">caches</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">caches</span>
    <span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Retrieve dimensions from da&#39;s and x1&#39;s shapes (≈2 lines)</span>
    <span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">shape</span> 

    <span class="c1"># initialize the gradients with the right sizes (≈6 lines)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWax</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWaa</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dba</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">da0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">da_prevt</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Loop through all the time steps</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T_x</span><span class="p">)):</span>
        <span class="c1"># Compute gradients at time step t. Choose wisely the &quot;da_next&quot; and the &quot;cache&quot; to use in the backward propagation step. (≈1 line)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Retrieve derivatives from gradients (≈ 1 line)</span>
        <span class="n">dxt</span><span class="p">,</span> <span class="n">da_prevt</span><span class="p">,</span> <span class="n">dWaxt</span><span class="p">,</span> <span class="n">dWaat</span><span class="p">,</span> <span class="n">dbat</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dxt&quot;</span><span class="p">],</span> <span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;da_prev&quot;</span><span class="p">],</span> <span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dWax&quot;</span><span class="p">],</span> <span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dWaa&quot;</span><span class="p">],</span> <span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dba&quot;</span><span class="p">]</span>
        <span class="c1"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span>
        <span class="n">dx</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  
        <span class="n">dWax</span> <span class="o">+=</span> <span class="kc">None</span>  
        <span class="n">dWaa</span> <span class="o">+=</span> <span class="kc">None</span>  
        <span class="n">dba</span> <span class="o">+=</span> <span class="kc">None</span>  

    <span class="c1"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span>
    <span class="n">da0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Store the gradients in a python dictionary</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dx&quot;</span><span class="p">:</span> <span class="n">dx</span><span class="p">,</span> <span class="s2">&quot;da0&quot;</span><span class="p">:</span> <span class="n">da0</span><span class="p">,</span> <span class="s2">&quot;dWax&quot;</span><span class="p">:</span> <span class="n">dWax</span><span class="p">,</span> <span class="s2">&quot;dWaa&quot;</span><span class="p">:</span> <span class="n">dWaa</span><span class="p">,</span><span class="s2">&quot;dba&quot;</span><span class="p">:</span> <span class="n">dba</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a0_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wax&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Waa&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wya&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;ba&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">,</span> <span class="n">a0_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>
<span class="n">da_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">gradients_tmp</span> <span class="o">=</span> <span class="n">rnn_backward</span><span class="p">(</span><span class="n">da_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dx</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dx&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dx</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dx&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da0</span><span class="se">\&quot;</span><span class="s2">][2][3] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da0&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da0</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWax</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWax&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWax</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWax&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWaa</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWaa&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWaa</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWaa&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dba</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dba&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dba</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dba&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<table>
    <tr>
        <td>
            <b>gradients["dx"][1][2]</b> =
        </td>
        <td>
           [-2.07101689 -0.59255627  0.02466855  0.01483317]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dx"].shape</b> =
        </td>
        <td>
           (3, 10, 4)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da0"][2][3]</b> =
        </td>
        <td>
           -0.314942375127
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da0"].shape</b> =
        </td>
        <td>
           (5, 10)
        </td>
    </tr>
         <tr>
        <td>
            <b>gradients["dWax"][3][1]</b> =
        </td>
        <td>
           11.2641044965
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWax"].shape</b> =
        </td>
        <td>
           (5, 3)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWaa"][1][2]</b> = 
        </td>
        <td>
           2.30333312658
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWaa"].shape</b> =
        </td>
        <td>
           (5, 5)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dba"][4]</b> = 
        </td>
        <td>
           [-0.74747722]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dba"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='3-2'></a></p>
<h3 id="32-lstm-backward-pass">3.2 - LSTM Backward Pass</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1-one-step-backward">1. One Step Backward</h4>
<p>The LSTM backward pass is slightly more complicated than the forward pass.</p>
<p><img src="images/LSTM_cell_backward_rev3a_c2.png" style="width:500;height:400px;"> <br>
<caption><center><font color='purple'><b>Figure 8</b>: LSTM Cell Backward. Note the output functions, while part of the <code>lstm_cell_forward</code>, are not included in <code>lstm_cell_backward</code> </center></caption></p>
<p>The equations for the LSTM backward pass are provided below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2-gate-derivatives">2. Gate Derivatives</h4>
<p>Note the location of the gate derivatives (<span class="arithmatex">\(\gamma\)</span>..) between the dense layer and the activation function (see graphic above). This is convenient for computing parameter derivatives in the next step. 
\begin{align}
d\gamma_o^{\langle t \rangle} &amp;= da_{next}<em>\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}</em>\left(1-\Gamma_o^{\langle t \rangle}\right)\tag{7} \[8pt]
dp\widetilde{c}^{\langle t \rangle} &amp;= \left(dc_{next}<em>\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle}</em> (1-\tanh^2(c_{next})) * \Gamma_u^{\langle t \rangle} * da_{next} \right) * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \tag{8} \[8pt]
d\gamma_u^{\langle t \rangle} &amp;= \left(dc_{next}<em>\widetilde{c}^{\langle t \rangle} + \Gamma_o^{\langle t \rangle}</em> (1-\tanh^2(c_{next})) * \widetilde{c}^{\langle t \rangle} * da_{next}\right)<em>\Gamma_u^{\langle t \rangle}</em>\left(1-\Gamma_u^{\langle t \rangle}\right)\tag{9} \[8pt]
d\gamma_f^{\langle t \rangle} &amp;= \left(dc_{next}<em> c_{prev} + \Gamma_o^{\langle t \rangle} * (1-\tanh^2(c_{next})) * c_{prev} * da_{next}\right)</em>\Gamma_f^{\langle t \rangle}*\left(1-\Gamma_f^{\langle t \rangle}\right)\tag{10}
\end{align}</p>
<h4 id="3-parameter-derivatives">3. Parameter Derivatives</h4>
<p>$ dW_f = d\gamma_f^{\langle t \rangle} \begin{bmatrix} a_{prev} \ x_t\end{bmatrix}^T \tag{11} $
$ dW_u = d\gamma_u^{\langle t \rangle} \begin{bmatrix} a_{prev} \ x_t\end{bmatrix}^T \tag{12} $
$ dW_c = dp\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{prev} \ x_t\end{bmatrix}^T \tag{13} $
$ dW_o = d\gamma_o^{\langle t \rangle} \begin{bmatrix} a_{prev} \ x_t\end{bmatrix}^T \tag{14}$</p>
<p>To calculate <span class="arithmatex">\(db_f, db_u, db_c, db_o\)</span> you just need to sum across all 'm' examples (axis= 1) on <span class="arithmatex">\(d\gamma_f^{\langle t \rangle}, d\gamma_u^{\langle t \rangle}, dp\widetilde c^{\langle t \rangle}, d\gamma_o^{\langle t \rangle}\)</span> respectively. Note that you should have the <code>keepdims = True</code> option.</p>
<p><span class="arithmatex">\(\displaystyle db_f = \sum_{batch}d\gamma_f^{\langle t \rangle}\tag{15}\)</span>
<span class="arithmatex">\(\displaystyle db_u = \sum_{batch}d\gamma_u^{\langle t \rangle}\tag{16}\)</span>
<span class="arithmatex">\(\displaystyle db_c = \sum_{batch}d\gamma_c^{\langle t \rangle}\tag{17}\)</span>
<span class="arithmatex">\(\displaystyle db_o = \sum_{batch}d\gamma_o^{\langle t \rangle}\tag{18}\)</span></p>
<p>Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.</p>
<p>$ da_{prev} = W_f^T d\gamma_f^{\langle t \rangle} + W_u^T   d\gamma_u^{\langle t \rangle}+ W_c^T dp\widetilde c^{\langle t \rangle} + W_o^T d\gamma_o^{\langle t \rangle} \tag{19}$</p>
<p>Here, to account for concatenation, the weights for equations 19 are the first n_a, (i.e. <span class="arithmatex">\(W_f = W_f[:,:n_a]\)</span> etc...)</p>
<p>$ dc_{prev} = dc_{next}<em>\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh^2(c_{next}))</em>\Gamma_f^{\langle t \rangle}*da_{next} \tag{20}$</p>
<p>$ dx^{\langle t \rangle} = W_f^T d\gamma_f^{\langle t \rangle} + W_u^T  d\gamma_u^{\langle t \rangle}+ W_c^T dp\widetilde c^{\langle t \rangle} + W_o^T d\gamma_o^{\langle t \rangle}\tag{21} $</p>
<p>where the weights for equation 21 are from n_a to the end, (i.e. <span class="arithmatex">\(W_f = W_f[:,n_a:]\)</span> etc...)</p>
<p><a name='ex-7'></a></p>
<h3 id="exercise-7-lstm_cell_backward">Exercise 7 - lstm_cell_backward</h3>
<p>Implement <code>lstm_cell_backward</code> by implementing equations <span class="arithmatex">\(7-21\)</span> below. </p>
<p><strong>Note</strong>: </p>
<p>In the code:</p>
<p><span class="arithmatex">\(d\gamma_o^{\langle t \rangle}\)</span> is represented by  <code>dot</code>,  <br />
<span class="arithmatex">\(dp\widetilde{c}^{\langle t \rangle}\)</span> is represented by  <code>dcct</code>,<br />
<span class="arithmatex">\(d\gamma_u^{\langle t \rangle}\)</span> is represented by  <code>dit</code>,<br />
<span class="arithmatex">\(d\gamma_f^{\langle t \rangle}\)</span> is represented by  <code>dft</code></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNGRADED FUNCTION: lstm_cell_backward</span>

<span class="k">def</span> <span class="nf">lstm_cell_backward</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">dc_next</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward pass for the LSTM-cell (single time-step).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span>
<span class="sd">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span>
<span class="sd">    cache -- cache storing information from the forward pass</span>

<span class="sd">    Returns:</span>
<span class="sd">    gradients -- python dictionary containing:</span>
<span class="sd">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span>
<span class="sd">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span>
<span class="sd">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span>
<span class="sd">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span>
<span class="sd">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span>
<span class="sd">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span>
<span class="sd">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve information from &quot;cache&quot;</span>
    <span class="p">(</span><span class="n">a_next</span><span class="p">,</span> <span class="n">c_next</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">,</span> <span class="n">ft</span><span class="p">,</span> <span class="n">it</span><span class="p">,</span> <span class="n">cct</span><span class="p">,</span> <span class="n">ot</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Retrieve dimensions from xt&#39;s and a_next&#39;s shape (≈2 lines)</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">n_a</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Compute gates related derivatives. Their values can be found by looking carefully at equations (7) to (10) (≈4 lines)</span>
    <span class="n">dot</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dcct</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dit</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dft</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Compute parameters related derivatives. Use equations (11)-(18) (≈8 lines)</span>
    <span class="n">dWf</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWi</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWc</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWo</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbf</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbi</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbc</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbo</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (19)-(21). (≈3 lines)</span>
    <span class="n">da_prev</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dc_prev</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dxt</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">### END CODE HERE ###</span>



    <span class="c1"># Save gradients in dictionary</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dxt&quot;</span><span class="p">:</span> <span class="n">dxt</span><span class="p">,</span> <span class="s2">&quot;da_prev&quot;</span><span class="p">:</span> <span class="n">da_prev</span><span class="p">,</span> <span class="s2">&quot;dc_prev&quot;</span><span class="p">:</span> <span class="n">dc_prev</span><span class="p">,</span> <span class="s2">&quot;dWf&quot;</span><span class="p">:</span> <span class="n">dWf</span><span class="p">,</span><span class="s2">&quot;dbf&quot;</span><span class="p">:</span> <span class="n">dbf</span><span class="p">,</span> <span class="s2">&quot;dWi&quot;</span><span class="p">:</span> <span class="n">dWi</span><span class="p">,</span><span class="s2">&quot;dbi&quot;</span><span class="p">:</span> <span class="n">dbi</span><span class="p">,</span>
                <span class="s2">&quot;dWc&quot;</span><span class="p">:</span> <span class="n">dWc</span><span class="p">,</span><span class="s2">&quot;dbc&quot;</span><span class="p">:</span> <span class="n">dbc</span><span class="p">,</span> <span class="s2">&quot;dWo&quot;</span><span class="p">:</span> <span class="n">dWo</span><span class="p">,</span><span class="s2">&quot;dbo&quot;</span><span class="p">:</span> <span class="n">dbo</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xt_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">a_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">c_prev_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a_next_tmp</span><span class="p">,</span> <span class="n">c_next_tmp</span><span class="p">,</span> <span class="n">yt_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span> <span class="o">=</span> <span class="n">lstm_cell_forward</span><span class="p">(</span><span class="n">xt_tmp</span><span class="p">,</span> <span class="n">a_prev_tmp</span><span class="p">,</span> <span class="n">c_prev_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>

<span class="n">da_next_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dc_next_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">gradients_tmp</span> <span class="o">=</span> <span class="n">lstm_cell_backward</span><span class="p">(</span><span class="n">da_next_tmp</span><span class="p">,</span> <span class="n">dc_next_tmp</span><span class="p">,</span> <span class="n">cache_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dxt</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dxt&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dxt</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dxt&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da_prev</span><span class="se">\&quot;</span><span class="s2">][2][3] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da_prev&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da_prev</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da_prev&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dc_prev</span><span class="se">\&quot;</span><span class="s2">][2][3] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dc_prev&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dc_prev</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dc_prev&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWf</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWf&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWf</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWf&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWi</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWi&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWi</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWi&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWc</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWc&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWc</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWo</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWo&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWo</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWo&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbf</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbf&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbf</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbf&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbi</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbi&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbi</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbi&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbc</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbc&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbc</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbo</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbo&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbo</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbo&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<table>
    <tr>
        <td>
            <b>gradients["dxt"][1][2]</b> =
        </td>
        <td>
           3.23055911511
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dxt"].shape</b> =
        </td>
        <td>
           (3, 10)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da_prev"][2][3]</b> =
        </td>
        <td>
           -0.0639621419711
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da_prev"].shape</b> =
        </td>
        <td>
           (5, 10)
        </td>
    </tr>
         <tr>
        <td>
            <b>gradients["dc_prev"][2][3]</b> =
        </td>
        <td>
           0.797522038797
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dc_prev"].shape</b> =
        </td>
        <td>
           (5, 10)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWf"][3][1]</b> = 
        </td>
        <td>
           -0.147954838164
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWf"].shape</b> =
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWi"][1][2]</b> = 
        </td>
        <td>
           1.05749805523
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWi"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dWc"][3][1]</b> = 
        </td>
        <td>
           2.30456216369
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWc"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dWo"][1][2]</b> = 
        </td>
        <td>
           0.331311595289
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWo"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dbf"][4]</b> = 
        </td>
        <td>
           [ 0.18864637]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbf"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dbi"][4]</b> = 
        </td>
        <td>
           [-0.40142491]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbi"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbc"][4]</b> = 
        </td>
        <td>
           [ 0.25587763]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbc"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbo"][4]</b> = 
        </td>
        <td>
           [ 0.13893342]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbo"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name='3-3'></a></p>
<h3 id="33-backward-pass-through-the-lstm-rnn">3.3 Backward Pass through the LSTM RNN</h3>
<p>This part is very similar to the <code>rnn_backward</code> function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. </p>
<p><a name='ex-8'></a></p>
<h3 id="exercise-8-lstm_backward">Exercise 8 - lstm_backward</h3>
<p>Implement the <code>lstm_backward</code> function.</p>
<p><strong>Instructions</strong>: Create a for loop starting from <span class="arithmatex">\(T_x\)</span> and going backward. For each step, call <code>lstm_cell_backward</code> and update your old gradients by adding the new gradients to them. Note that <code>dxt</code> is not updated, but is stored.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNGRADED FUNCTION: lstm_backward</span>

<span class="k">def</span> <span class="nf">lstm_backward</span><span class="p">(</span><span class="n">da</span><span class="p">,</span> <span class="n">caches</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span>
<span class="sd">    caches -- cache storing information from the forward pass (lstm_forward)</span>

<span class="sd">    Returns:</span>
<span class="sd">    gradients -- python dictionary containing:</span>
<span class="sd">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span>
<span class="sd">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span>
<span class="sd">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span>
<span class="sd">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span>
<span class="sd">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span>
<span class="sd">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span>
<span class="sd">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve values from the first cache (t=1) of caches.</span>
    <span class="p">(</span><span class="n">caches</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">caches</span>
    <span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">c0</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">cc1</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Retrieve dimensions from da&#39;s and x1&#39;s shapes (≈2 lines)</span>
    <span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T_x</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">n_x</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># initialize the gradients with the right sizes (≈12 lines)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">da0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">da_prevt</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dc_prevt</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWf</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWi</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWc</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dWo</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbf</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbi</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbc</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dbo</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># loop back over the whole sequence</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="kc">None</span><span class="p">)):</span>
        <span class="c1"># Compute all gradients using lstm_cell_backward. Choose wisely the &quot;da_next&quot; (same as done for Ex 6).</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Store or add the gradient to the parameters&#39; previous step&#39;s gradient</span>
        <span class="n">da_prevt</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">dc_prevt</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">dx</span><span class="p">[:,:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">dWf</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dWi</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dWc</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dWo</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dbf</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dbi</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dbc</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">dbo</span> <span class="o">+=</span> <span class="kc">None</span>
    <span class="c1"># Set the first activation&#39;s gradient to the backpropagated gradient da_prev.</span>
    <span class="n">da0</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Store the gradients in a python dictionary</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dx&quot;</span><span class="p">:</span> <span class="n">dx</span><span class="p">,</span> <span class="s2">&quot;da0&quot;</span><span class="p">:</span> <span class="n">da0</span><span class="p">,</span> <span class="s2">&quot;dWf&quot;</span><span class="p">:</span> <span class="n">dWf</span><span class="p">,</span><span class="s2">&quot;dbf&quot;</span><span class="p">:</span> <span class="n">dbf</span><span class="p">,</span> <span class="s2">&quot;dWi&quot;</span><span class="p">:</span> <span class="n">dWi</span><span class="p">,</span><span class="s2">&quot;dbi&quot;</span><span class="p">:</span> <span class="n">dbi</span><span class="p">,</span>
                <span class="s2">&quot;dWc&quot;</span><span class="p">:</span> <span class="n">dWc</span><span class="p">,</span><span class="s2">&quot;dbc&quot;</span><span class="p">:</span> <span class="n">dbc</span><span class="p">,</span> <span class="s2">&quot;dWo&quot;</span><span class="p">:</span> <span class="n">dWo</span><span class="p">,</span><span class="s2">&quot;dbo&quot;</span><span class="p">:</span> <span class="n">dbo</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="n">a0_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

<span class="n">parameters_tmp</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;bc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;Wy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>       <span class="c1"># unused, but needed for lstm_forward</span>
<span class="n">parameters_tmp</span><span class="p">[</span><span class="s1">&#39;by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>       <span class="c1"># unused, but needed for lstm_forward</span>

<span class="n">a_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">c_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span> <span class="o">=</span> <span class="n">lstm_forward</span><span class="p">(</span><span class="n">x_tmp</span><span class="p">,</span> <span class="n">a0_tmp</span><span class="p">,</span> <span class="n">parameters_tmp</span><span class="p">)</span>

<span class="n">da_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">gradients_tmp</span> <span class="o">=</span> <span class="n">lstm_backward</span><span class="p">(</span><span class="n">da_tmp</span><span class="p">,</span> <span class="n">caches_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dx</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dx&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dx</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dx&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da0</span><span class="se">\&quot;</span><span class="s2">][2][3] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da0&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">da0</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;da0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWf</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWf&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWf</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWf&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWi</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWi&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWi</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWi&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWc</span><span class="se">\&quot;</span><span class="s2">][3][1] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWc&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWc</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWo</span><span class="se">\&quot;</span><span class="s2">][1][2] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWo&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dWo</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dWo&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbf</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbf&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbf</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbf&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbi</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbi&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbi</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbi&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbc</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbc&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbc</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbo</span><span class="se">\&quot;</span><span class="s2">][4] =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbo&quot;</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gradients[</span><span class="se">\&quot;</span><span class="s2">dbo</span><span class="se">\&quot;</span><span class="s2">].shape =&quot;</span><span class="p">,</span> <span class="n">gradients_tmp</span><span class="p">[</span><span class="s2">&quot;dbo&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<table>
    <tr>
        <td>
            <b>gradients["dx"][1][2]</b> =
        </td>
        <td>
           [0.00218254  0.28205375 -0.48292508 -0.43281115]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dx"].shape</b> =
        </td>
        <td>
           (3, 10, 4)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da0"][2][3]</b> =
        </td>
        <td>
           0.312770310257
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["da0"].shape</b> =
        </td>
        <td>
           (5, 10)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWf"][3][1]</b> = 
        </td>
        <td>
           -0.0809802310938
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWf"].shape</b> =
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWi"][1][2]</b> = 
        </td>
        <td>
           0.40512433093
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWi"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dWc"][3][1]</b> = 
        </td>
        <td>
           -0.0793746735512
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWc"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dWo"][1][2]</b> = 
        </td>
        <td>
           0.038948775763
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dWo"].shape</b> = 
        </td>
        <td>
           (5, 8)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dbf"][4]</b> = 
        </td>
        <td>
           [-0.15745657]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbf"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
    <tr>
        <td>
            <b>gradients["dbi"][4]</b> = 
        </td>
        <td>
           [-0.50848333]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbi"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbc"][4]</b> = 
        </td>
        <td>
           [-0.42510818]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbc"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbo"][4]</b> = 
        </td>
        <td>
           [ -0.17958196]
        </td>
    </tr>
        <tr>
        <td>
            <b>gradients["dbo"].shape</b> = 
        </td>
        <td>
           (5, 1)
        </td>
    </tr>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="congratulations-on-completing-this-assignment">Congratulations on completing this assignment!</h3>
<p>You now understand how recurrent neural networks work! In the next exercise, you'll use an RNN to build a character-level language model. See you there! </p>
</div>
</div>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/hari31416" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/hari31416" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/hari31416" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.kaggle.com/hari31416" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.instagram.com/hari31416" target="_blank" rel="noopener" title="www.instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.top", "toc.integrate"], "search": "../../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>