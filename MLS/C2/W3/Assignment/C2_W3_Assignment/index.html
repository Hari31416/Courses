
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This is a collection of all the notebooks from various courses I have taken.">
      
      
        <meta name="author" content="Harikesh Kushwaha">
      
      
        <link rel="canonical" href="https://hari31416.github.io/Courses/MLS/C2/W3/Assignment/C2_W3_Assignment/">
      
      
        <link rel="prev" href="../../../W2/Lab/C2_W2_SoftMax/">
      
      
        <link rel="next" href="../../../W4/Assignment/C2_W4_Decision_Tree_with_Markdown/">
      
      <link rel="icon" href="../../../../../assets/ml.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.15">
    
    
      
        <title>C2 W3 Assignment - Courses</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../../../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../../../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#practice-lab-advice-for-applying-machine-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Courses" class="md-header__button md-logo" aria-label="Courses" data-md-component="logo">
      
  <img src="../../../../../assets/ml.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Courses
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              C2 W3 Assignment
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/Hari31416/Courses" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Courses
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Courses" class="md-nav__button md-logo" aria-label="Courses" data-md-component="logo">
      
  <img src="../../../../../assets/ml.png" alt="logo">

    </a>
    Courses
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Hari31416/Courses" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Courses
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Deep Learning Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step/" class="md-nav__link">
        Building your Deep Neural Network Step by Step
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C1/Assignments/Deep_Neural_Network_-_Application/Deep_Neural_Network_-_Application/" class="md-nav__link">
        Deep Neural Network   Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C1/Assignments/Logistic_Regression_with_a_Neural_Network_mindset/Logistic_Regression_with_a_Neural_Network_mindset/" class="md-nav__link">
        Logistic Regression with a Neural Network mindset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C1/Assignments/Planar_data_classification_with_one_hidden_layer/Planar_data_classification_with_one_hidden_layer/" class="md-nav__link">
        Planar data classification with one hidden layer
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C2/Assignments/Gradient_Checking/Gradient_Checking/" class="md-nav__link">
        Gradient Checking
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C2/Assignments/Initialization/Initialization/" class="md-nav__link">
        Initialization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C2/Assignments/Optimization_Methods/Optimization_methods/" class="md-nav__link">
        Optimization methods
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C2/Assignments/Regularization/Regularization/" class="md-nav__link">
        Regularization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C2/Assignments/Tensorflow_introduction/Tensorflow_introduction/" class="md-nav__link">
        Tensorflow introduction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
          C4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          C4
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C4/Assignments/Car_detection_with_YOLO/Autonomous_driving_application_Car_detection/" class="md-nav__link">
        Autonomous driving application Car detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C4/Assignments/Convolution_model_Application/Convolution_model_Application/" class="md-nav__link">
        Convolution model Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C4/Assignments/Image_segmentation_Unet/Image_segmentation_Unet_v2/" class="md-nav__link">
        Image segmentation Unet v2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C4/Assignments/Residual_Networks/Residual_Networks/" class="md-nav__link">
        Residual Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C4/Assignments/Transfer_learning_with_MobileNet/Transfer_learning_with_MobileNet_v1/" class="md-nav__link">
        Transfer learning with MobileNet v1
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
          C5
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_4">
          <span class="md-nav__icon md-icon"></span>
          C5
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Building_a_Recurrent_Neural_Network_Step_by_Step/Building_a_Recurrent_Neural_Network_Step_by_Step/" class="md-nav__link">
        Building a Recurrent Neural Network Step by Step
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Dinosaurus_Island_Character_level_language_model/Dinosaurus_Island_Character_level_language_model/" class="md-nav__link">
        Dinosaurus Island Character level language model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Emojify/Emoji_v3a/" class="md-nav__link">
        Emoji v3a
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Improvise_a_Jazz_Solo_with_an_LSTM_Network/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4/" class="md-nav__link">
        Improvise a Jazz Solo with an LSTM Network v4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Neural_machine_translation_with_attention/Neural_machine_translation_with_attention_v4a/" class="md-nav__link">
        Neural machine translation with attention v4a
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLS/C5/Assignments/Operations_on_Word_Vectors_Debiasing/Operations_on_word_vectors_v2a/" class="md-nav__link">
        Operations on word vectors v2a
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Machine Learning Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/C1_W1_Lab03_Model_Representation_Soln/" class="md-nav__link">
        C1 W1 Lab03 Model Representation Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/C1_W1_Lab04_Cost_function_Soln/" class="md-nav__link">
        C1 W1 Lab04 Cost function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/C1_W1_Lab05_Gradient_Descent_Soln/" class="md-nav__link">
        C1 W1 Lab05 Gradient Descent Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab01_Python_Jupyter_Soln/" class="md-nav__link">
        C1 W1 Lab01 Python Jupyter Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab02_Course_Preview_Soln/" class="md-nav__link">
        C1 W1 Lab02 Course Preview Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab03_Model_Representation_Soln/" class="md-nav__link">
        C1 W1 Lab03 Model Representation Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab04_Cost_function_Soln/" class="md-nav__link">
        C1 W1 Lab04 Cost function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Assignments/Gradient_Descent/betaversion/C1_W1_Lab05_Gradient_Descent_Soln/" class="md-nav__link">
        C1 W1 Lab05 Gradient Descent Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Notes/Linear_Regression/" class="md-nav__link">
        Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W1/Notes/Overview/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab01_Python_Numpy_Vectorization_Soln/" class="md-nav__link">
        C1 W2 Lab01 Python Numpy Vectorization Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab02_Multiple_Variable_Soln/" class="md-nav__link">
        C1 W2 Lab02 Multiple Variable Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln/" class="md-nav__link">
        C1 W2 Lab03 Feature Scaling and Learning Rate Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab04_FeatEng_PolyReg_Soln/" class="md-nav__link">
        C1 W2 Lab04 FeatEng PolyReg Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab05_Sklearn_GD_Soln/" class="md-nav__link">
        C1 W2 Lab05 Sklearn GD Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/C1_W2_Lab06_Sklearn_Normal_Soln/" class="md-nav__link">
        C1 W2 Lab06 Sklearn Normal Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W2/Assignments/Linear_Regression/C1_W2_Linear_Regression/" class="md-nav__link">
        C1 W2 Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab01_Classification_Soln/" class="md-nav__link">
        C1 W3 Lab01 Classification Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab02_Sigmoid_function_Soln/" class="md-nav__link">
        C1 W3 Lab02 Sigmoid function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab03_Decision_Boundary_Soln/" class="md-nav__link">
        C1 W3 Lab03 Decision Boundary Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab04_LogisticLoss_Soln/" class="md-nav__link">
        C1 W3 Lab04 LogisticLoss Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab05_Cost_Function_Soln/" class="md-nav__link">
        C1 W3 Lab05 Cost Function Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab07_Scikit_Learn_Soln/" class="md-nav__link">
        C1 W3 Lab07 Scikit Learn Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab08_Overfitting_Soln/" class="md-nav__link">
        C1 W3 Lab08 Overfitting Soln
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C1/W3/Assignments/C1_W3_Lab09_Regularization_Soln/" class="md-nav__link">
        C1 W3 Lab09 Regularization Soln
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W1/Assignment/C2_W1_Assignment/" class="md-nav__link">
        C2 W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W1/Lab/C2_W1_Lab01_Neurons_and_Layers/" class="md-nav__link">
        C2 W1 Lab01 Neurons and Layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W1/Lab/C2_W1_Lab02_CoffeeRoasting_TF/" class="md-nav__link">
        C2 W1 Lab02 CoffeeRoasting TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W1/Lab/C2_W1_Lab03_CoffeeRoasting_Numpy/" class="md-nav__link">
        C2 W1 Lab03 CoffeeRoasting Numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W2/Assignment/C2_W2_Assignment/" class="md-nav__link">
        C2 W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W2/Lab/C2_W2_Multiclass_TF/" class="md-nav__link">
        C2 W2 Multiclass TF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W2/Lab/C2_W2_Relu/" class="md-nav__link">
        C2 W2 Relu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W2/Lab/C2_W2_SoftMax/" class="md-nav__link">
        C2 W2 SoftMax
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        C2 W3 Assignment
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../W4/Assignment/C2_W4_Decision_Tree_with_Markdown/" class="md-nav__link">
        C2 W4 Decision Tree with Markdown
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W1/Assignment/A1/C3_W1_KMeans_Assignment/" class="md-nav__link">
        C3 W1 KMeans Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W1/Assignment/A2/C3_W1_Anomaly_Detection/" class="md-nav__link">
        C3 W1 Anomaly Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W2/Assignment/A1/C3_W2_Collaborative_RecSys_Assignment/" class="md-nav__link">
        C3 W2 Collaborative RecSys Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W2/Assignment/A2/C3_W2_RecSysNN_Assignment/" class="md-nav__link">
        C3 W2 RecSysNN Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W3/Assignment/C3_W3_A1_Assignment/" class="md-nav__link">
        C3 W3 A1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../C3/W3/Lab/State-action%20value%20function%20example/" class="md-nav__link">
        State action value function example
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          TensorFlow Advanced Techniques Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          TensorFlow Advanced Techniques Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_1_functional-practice/" class="md-nav__link">
        C1 W1 Lab 1 functional practice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_2_multi-output/" class="md-nav__link">
        C1 W1 Lab 2 multi output
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Labs/C1_W1_Lab_3_siamese-network/" class="md-nav__link">
        C1 W1 Lab 3 siamese network
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W1/Multiple_Output_Models_using_the_Keras_Functional_API/C1W1_Assignment/" class="md-nav__link">
        C1W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Creating_a_Custom_Loss_Function/C1W2_Assignment/" class="md-nav__link">
        C1W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Labs/C1_W2_Lab_1_huber-loss/" class="md-nav__link">
        C1 W2 Lab 1 huber loss
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W2/Labs/C1_W2_Lab_2_huber-object-loss/" class="md-nav__link">
        C1 W2 Lab 2 huber object loss
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Implement_a_Quadratic_Layer/C1W3_Assignment/" class="md-nav__link">
        C1W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_1_lambda-layer/" class="md-nav__link">
        C1 W3 Lab 1 lambda layer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_2_custom-dense-layer/" class="md-nav__link">
        C1 W3 Lab 2 custom dense layer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W3/Labs/C1_W3_Lab_3_custom-layer-activation/" class="md-nav__link">
        C1 W3 Lab 3 custom layer activation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Create_a_VGG_network/C1W4_Assignment/" class="md-nav__link">
        C1W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Labs/C1_W4_Lab_1_basic-model/" class="md-nav__link">
        C1 W4 Lab 1 basic model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C1/W4/Labs/C1_W4_Lab_2_resnet-example/" class="md-nav__link">
        C1 W4 Lab 2 resnet example
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Assignment/C2W1_Assignment/" class="md-nav__link">
        C2W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Labs/C2_W1_Lab_1_basic-tensors/" class="md-nav__link">
        C2 W1 Lab 1 basic tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W1/Labs/C2_W1_Lab_2_gradient-tape-basics/" class="md-nav__link">
        C2 W1 Lab 2 gradient tape basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Assignment/C2W2_Assignment/" class="md-nav__link">
        C2W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Labs/C2_W2_Lab_1_training-basics/" class="md-nav__link">
        C2 W2 Lab 1 training basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W2/Labs/C2_W2_Lab_2_training-categorical/" class="md-nav__link">
        C2 W2 Lab 2 training categorical
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Assignment/C2W3_Assignment/" class="md-nav__link">
        C2W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Labs/C2_W3_Lab_1_autograph-basics/" class="md-nav__link">
        C2 W3 Lab 1 autograph basics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W3/Labs/C2_W3_Lab_2-graphs-for-complex-code/" class="md-nav__link">
        C2 W3 Lab 2 graphs for complex code
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Assignment/C2W4_Assignment/" class="md-nav__link">
        C2W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_1_basic-mirrored-strategy/" class="md-nav__link">
        C2 W4 Lab 1 basic mirrored strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_2_multi-GPU-mirrored-strategy/" class="md-nav__link">
        C2 W4 Lab 2 multi GPU mirrored strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_3_using-TPU-strategy/" class="md-nav__link">
        C2 W4 Lab 3 using TPU strategy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C2/W4/Labs/C2_W4_Lab_4_one-device-strategy/" class="md-nav__link">
        C2 W4 Lab 4 one device strategy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Assignment/C3W1_Assignment/" class="md-nav__link">
        C3W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Labs/C3_W1_Lab_2_Transfer_Learning_CIFAR_10/" class="md-nav__link">
        C3 W1 Lab 2 Transfer Learning CIFAR 10
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W1/Labs/C3_W1_Lab_3_Object_Localization/" class="md-nav__link">
        C3 W1 Lab 3 Object Localization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Assignment/C3W2_Assignment/" class="md-nav__link">
        C3W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Labs/C3_W2_Lab_1_Simple_Object_Detection/" class="md-nav__link">
        C3 W2 Lab 1 Simple Object Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W2/Labs/Object_Detection_Inference_on_TF_2_and_TF_Hub/" class="md-nav__link">
        Object Detection Inference on TF 2 and TF Hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Assignment/C3W3_Assignment/" class="md-nav__link">
        C3W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_1_VGG16_FCN8_CamVid/" class="md-nav__link">
        C3 W3 Lab 1 VGG16 FCN8 CamVid
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_2_OxfordPets_UNet/" class="md-nav__link">
        C3 W3 Lab 2 OxfordPets UNet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W3/Labs/C3_W3_Lab_3_Mask_RCNN_ImageSegmentation/" class="md-nav__link">
        C3 W3 Lab 3 Mask RCNN ImageSegmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Assignment/C3_W4_Assignment/" class="md-nav__link">
        C3 W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_1_FashionMNIST_CAM/" class="md-nav__link">
        C3 W4 Lab 1 FashionMNIST CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_2_CatsDogs_CAM/" class="md-nav__link">
        C3 W4 Lab 2 CatsDogs CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C3/W4/Labs/C3_W4_Lab_3_Saliency/" class="md-nav__link">
        C3 W4 Lab 3 Saliency
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          C4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          C4
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Assignment/C4W1_Assignment/" class="md-nav__link">
        C4W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Labs/C4_W1_Lab_1_Neural_Style_Transfer/" class="md-nav__link">
        C4 W1 Lab 1 Neural Style Transfer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W1/Labs/C4_W1_Lab_2_Fast_NST/" class="md-nav__link">
        C4 W1 Lab 2 Fast NST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Assignment/C4W2_Assignment/" class="md-nav__link">
        C4W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_1_FirstAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 1 FirstAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_2_MNIST_Autoencoder/" class="md-nav__link">
        C4 W2 Lab 2 MNIST Autoencoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_3_MNIST_DeepAutoencoder/" class="md-nav__link">
        C4 W2 Lab 3 MNIST DeepAutoencoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_4_FashionMNIST_CNNAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 4 FashionMNIST CNNAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W2/Labs/C4_W2_Lab_5_FashionMNIST_NoisyCNNAutoEncoder/" class="md-nav__link">
        C4 W2 Lab 5 FashionMNIST NoisyCNNAutoEncoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W3/Assignment/C4W3_Assignment/" class="md-nav__link">
        C4W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W3/Labs/C4_W3_Lab_1_VAE_MNIST/" class="md-nav__link">
        C4 W3 Lab 1 VAE MNIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Assignment/C4W4_Assignment/" class="md-nav__link">
        C4W4 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_1_First_GAN/" class="md-nav__link">
        C4 W4 Lab 1 First GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_2_First_DCGAN/" class="md-nav__link">
        C4 W4 Lab 2 First DCGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../TF_Specialization/C4/W4/Labs/C4_W4_Lab_3_CelebA_GAN_Experiments/" class="md-nav__link">
        C4 W4 Lab 3 CelebA GAN Experiments
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Generative Adversarial Networks Specialization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Generative Adversarial Networks Specialization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          C1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          C1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W1/Assignments/C1W1_Your_First_GAN/" class="md-nav__link">
        C1W1 Your First GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W1/Labs/Intro_to_PyTorch/" class="md-nav__link">
        Intro to PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W2/Assignments/C1_W2_Assignment/" class="md-nav__link">
        C1 W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W2/Labs/C1W2_Video_Generation_%28Optional%29/" class="md-nav__link">
        C1W2 Video Generation (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W3/Assignments/C1W3_WGAN_GP/" class="md-nav__link">
        C1W3 WGAN GP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W3/Labs/SNGAN/" class="md-nav__link">
        SNGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W4/Assignments/C1W4A_Build_a_Conditional_GAN/" class="md-nav__link">
        C1W4A Build a Conditional GAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C1/W4/Assignments/C1W4B_Controllable_Generation/" class="md-nav__link">
        C1W4B Controllable Generation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          C2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          C2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W1/Assignments/C2W1_Assignment/" class="md-nav__link">
        C2W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W1/Labs/PPL/" class="md-nav__link">
        PPL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W2/Assignments/C2W2_Assignment/" class="md-nav__link">
        C2W2 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W2/Labs/C2W2_VAE/" class="md-nav__link">
        C2W2 VAE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Assignments/C2W3_Assignment/" class="md-nav__link">
        C2W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Labs/BigGAN/" class="md-nav__link">
        BigGAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C2/W3/Labs/StyleGAN2/" class="md-nav__link">
        StyleGAN2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          C3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          C3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W1/Assignments/C3W1_Assignment/" class="md-nav__link">
        C3W1 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W1/Labs/C3W1_Generative_Teaching_Networks_%28Optional%29/" class="md-nav__link">
        C3W1 Generative Teaching Networks (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Assignments/C3W2A_Assignment/" class="md-nav__link">
        C3W2A Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Assignments/C3W2B_Assignment/" class="md-nav__link">
        C3W2B Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Labs/C3W2_GauGAN_%28Optional%29/" class="md-nav__link">
        C3W2 GauGAN (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W2/Labs/C3W2_Pix2PixHD_%28Optional%29/" class="md-nav__link">
        C3W2 Pix2PixHD (Optional)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W3/Assignments/C3W3_Assignment/" class="md-nav__link">
        C3W3 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../GAN/C3/W3/Labs/C3W3_MUNIT_%28Optional%29/" class="md-nav__link">
        C3W3 MUNIT (Optional)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Deep Learning AI
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning AI
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          How Diffusion Models Work
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          How Diffusion Models Work
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab1/L1_Sampling/" class="md-nav__link">
        L1 Sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab2/L2_Training/" class="md-nav__link">
        L2 Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab3/L3_Context/" class="md-nav__link">
        L3 Context
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Diffusion/Lab4/L4_FastSampling/" class="md-nav__link">
        L4 FastSampling
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          LangChain for LLM Application Development
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          LangChain for LLM Application Development
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L1-Model_prompt_parser/" class="md-nav__link">
        L1 Model prompt parser
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L2-Memory/" class="md-nav__link">
        L2 Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L3-Chains/" class="md-nav__link">
        L3 Chains
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L4-QnA/" class="md-nav__link">
        L4 QnA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L5-Evaluation/" class="md-nav__link">
        L5 Evaluation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Langchain/L6-Agents/" class="md-nav__link">
        L6 Agents
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
      
      
      
        <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
          ChatGPT Prompt Engineering for Developers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          ChatGPT Prompt Engineering for Developers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l2-guidelines/" class="md-nav__link">
        L2 guidelines
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l4-summarizing/" class="md-nav__link">
        L4 summarizing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l5-inferring/" class="md-nav__link">
        L5 inferring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l6-transforming/" class="md-nav__link">
        L6 transforming
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l7-expanding/" class="md-nav__link">
        L7 expanding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Prompt/l8-chatbot/" class="md-nav__link">
        L8 chatbot
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_4" >
      
      
      
        <label class="md-nav__link" for="__nav_6_4" id="__nav_6_4_label" tabindex="0">
          Building Systems with the ChatGPT API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_4">
          <span class="md-nav__icon md-icon"></span>
          Building Systems with the ChatGPT API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/1_Language_Models%2C_the_Chat_Format_and_Tokens/" class="md-nav__link">
        1 Language Models, the Chat Format and Tokens
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/2_Classification/" class="md-nav__link">
        2 Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/3_Moderation/" class="md-nav__link">
        3 Moderation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/4_Chain_of_Thoughts/" class="md-nav__link">
        4 Chain of Thoughts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/5_Chaining_Prompts/" class="md-nav__link">
        5 Chaining Prompts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/6_Check_Outputs/" class="md-nav__link">
        6 Check Outputs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/7_Build_an_End_to_End_System/" class="md-nav__link">
        7 Build an End to End System
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/8_Evaluation_Part_1/" class="md-nav__link">
        8 Evaluation Part 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../DLAI/Building_System/9_Evaluation_Part_2/" class="md-nav__link">
        9 Evaluation Part 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../../license/" class="md-nav__link">
        License
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script>
(function() {
  function addWidgetsRenderer() {
    var requireJsScript = document.createElement('script');
    requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';

    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var jupyterWidgetsScript = document.createElement('script');
    var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    jupyterWidgetsScript.src = widgetRendererSrc;

    document.body.appendChild(requireJsScript);
    document.body.appendChild(jupyterWidgetsScript);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="practice-lab-advice-for-applying-machine-learning">Practice Lab: Advice for Applying Machine Learning</h1>
<p>In this lab, you will explore techniques to evaluate and improve your machine learning models.</p>
<h1 id="outline">Outline</h1>
<ul>
<li><a href="#1"> 1 - Packages </a></li>
<li><a href="#2"> 2 - Evaluating a Learning Algorithm (Polynomial Regression)</a></li>
<li><a href="#2.1"> 2.1 Splitting your data set</a></li>
<li><a href="#2.2"> 2.2 Error calculation for model evaluation, linear regression</a><ul>
<li><a href="#ex01"> Exercise 1</a></li>
</ul>
</li>
<li><a href="#2.3"> 2.3 Compare performance on training and test data</a></li>
<li><a href="#3"> 3 - Bias and Variance<img align="Right" src="./images/C2_W3_BiasVarianceDegree.png"  style=" width:500px; padding: 10px 20px ; "> </a></li>
<li><a href="#3.1"> 3.1 Plot Train, Cross-Validation, Test</a></li>
<li><a href="#3.2"> 3.2 Finding the optimal degree</a></li>
<li><a href="#3.3"> 3.3 Tuning Regularization.</a></li>
<li><a href="#3.4"> 3.4 Getting more data: Increasing Training Set Size (m)</a></li>
<li><a href="#4"> 4 - Evaluating a Learning Algorithm (Neural Network)</a></li>
<li><a href="#4.1"> 4.1 Data Set</a></li>
<li><a href="#4.2"> 4.2 Evaluating categorical model by calculating classification error</a><ul>
<li><a href="#ex02"> Exercise 2</a></li>
</ul>
</li>
<li><a href="#5"> 5 - Model Complexity</a></li>
<li><a href="#ex03"> Exercise 3</a></li>
<li><a href="#5.1"> 5.1 Simple model</a><ul>
<li><a href="#ex04"> Exercise 4</a></li>
</ul>
</li>
<li><a href="#6"> 6 - Regularization</a></li>
<li><a href="#ex05"> Exercise 5</a></li>
<li><a href="#7"> 7 - Iterate to find optimal regularization value</a></li>
<li><a href="#7.1"> 7.1 Test</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="1"></a></p>
<h2 id="1-packages">1 - Packages</h2>
<p>First, let's run the cell below to import all the packages that you will need during this assignment.
- <a href="https://numpy.org/">numpy</a> is the fundamental package for scientific computing Python.
- <a href="http://matplotlib.org">matplotlib</a> is a popular library to plot graphs in Python.
- <a href="https://scikit-learn.org/stable/">scikitlearn</a> is a basic library for data mining
- <a href="https://www.tensorflow.org/">tensorflow</a> a popular platform for machine learning.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">widget</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.activations</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span><span class="n">linear</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.losses</span> <span class="kn">import</span> <span class="n">SparseCategoricalCrossentropy</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">public_tests_a1</span> <span class="kn">import</span> <span class="o">*</span> 

<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">assigment_utils</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="2"></a></p>
<h2 id="2-evaluating-a-learning-algorithm-polynomial-regression">2 - Evaluating a Learning Algorithm (Polynomial Regression)</h2>
<p><img align="Right" src="./images/C2_W3_TrainingVsNew.png"  style=" width:350px; padding: 10px 20px ; "> Let's say you have created a machine learning model and you find it <em>fits</em> your training data very well. You're done? Not quite. The goal of creating the model was to be able to predict values for <span style="color:blue"><em>new</em> </span> examples. </p>
<p>How can you test your model's performance on new data before deploying it? <br />
The answer has two parts:
* Split your original data set into "Training" and "Test" sets. 
    * Use the training data to fit the parameters of the model
    * Use the test data to evaluate the model on <em>new</em> data
* Develop an error function to evaluate your model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="2.1"></a></p>
<h3 id="21-splitting-your-data-set">2.1 Splitting your data set</h3>
<p>Lectures advised reserving 20-40% of your data set for testing. Let's use an <code>sklearn</code> function <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a> to perform the split. Double-check the shapes after running the following cell.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># Generate some data</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">x_ideal</span><span class="p">,</span><span class="n">y_ideal</span> <span class="o">=</span> <span class="n">gen_data</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X.shape&quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y.shape&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#split the data using sklearn routine </span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train.shape&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y_train.shape&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_test.shape&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y_test.shape&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>X.shape (18,) y.shape (18,)
X_train.shape (12,) y_train.shape (12,)
X_test.shape (6,) y_test.shape (6,)
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="211-plot-train-test-sets">2.1.1 Plot Train, Test sets</h4>
<p>You can see below the data points that will be part of training (in red) are intermixed with those that the model is not trained on (test). This particular data set is a quadratic function with noise added. The "ideal" curve is shown for reference.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_ideal</span><span class="p">,</span> <span class="n">y_ideal</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;orangered&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y_ideal&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training, Test&quot;</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span>           <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>   <span class="n">color</span> <span class="o">=</span> <span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span>   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="2.2"></a></p>
<h3 id="22-error-calculation-for-model-evaluation-linear-regression">2.2 Error calculation for model evaluation, linear regression</h3>
<p>When <em>evaluating</em> a linear regression model, you average the squared error difference of the predicted values and the target values.</p>
<div class="arithmatex">\[ J_\text{test}(\mathbf{w},b) = 
            \frac{1}{2m_\text{test}}\sum_{i=0}^{m_\text{test}-1} ( f_{\mathbf{w},b}(\mathbf{x}^{(i)}_\text{test}) - y^{(i)}_\text{test} )^2 
            \tag{1}
\]</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="ex01"></a></p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Below, create a function to evaluate the error on a data set for a linear regression model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED CELL: eval_mse</span>
<span class="k">def</span> <span class="nf">eval_mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Calculate the mean squared error on a data set.</span>
<span class="sd">    Args:</span>
<span class="sd">      y    : (ndarray  Shape (m,) or (m,1))  target value of each example</span>
<span class="sd">      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example</span>
<span class="sd">    Returns:</span>
<span class="sd">      err: (scalar)             </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">err</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="c1">### START CODE HERE ### </span>
        <span class="n">err</span><span class="o">+=</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
    <span class="c1">### END CODE HERE ### </span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">err</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">])</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">])</span>
<span class="n">eval_mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">)</span>

<span class="c1"># BEGIN UNIT TEST</span>
<span class="n">test_eval_mse</span><span class="p">(</span><span class="n">eval_mse</span><span class="p">)</span>   
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code><span class="ansi-green-intense-fg"> All tests passed.
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<details>
<summary><font size="3" color="darkgreen"><b>Click for hints</b></font></summary>


<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">eval_mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Calculate the mean squared error on a data set.</span>
<span class="sd">    Args:</span>
<span class="sd">      y    : (ndarray  Shape (m,) or (m,1))  target value of each example</span>
<span class="sd">      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example</span>
<span class="sd">    Returns:</span>
<span class="sd">      err: (scalar)             </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">err</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="p">(</span> <span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span> 
        <span class="n">err</span>   <span class="o">+=</span> <span class="n">err_i</span>                                                                
    <span class="n">err</span> <span class="o">=</span> <span class="n">err</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>                    
    <span class="k">return</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</code></pre></div>
</details>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="2.3"></a></p>
<h3 id="23-compare-performance-on-training-and-test-data">2.3 Compare performance on training and test data</h3>
<p>Let's build a high degree polynomial model to minimize training error. This will use the linear_regression functions from <code>sklearn</code>. The code is in the imported utility file if you would like to see the details. The steps below are:
* create and fit the model. ('fit' is another name for training or running gradient descent).
* compute the error on the training data.
* compute the error on the test data.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># create a model in sklearn, train on training data</span>
<span class="n">degree</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">lmodel</span> <span class="o">=</span> <span class="n">lin_model</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
<span class="n">lmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># predict on training data, find training error</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">err_train</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>

<span class="c1"># predict on test data, find error</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">err_test</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The computed error on the training set is substantially less than that of the test set. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;training err </span><span class="si">{</span><span class="n">err_train</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, test err </span><span class="si">{</span><span class="n">err_test</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>training err 58.01, test err 171215.01
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following plot shows why this is. The model fits the training data very well. To do so, it has created a complex function. The test data was not part of the training and the model does a poor job of predicting on this data.<br />
This model would be described as 1) is overfitting, 2) has high variance 3) 'generalizes' poorly.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># plot predictions over data range </span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># predict values for plot</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt_train_test</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">x_ideal</span><span class="p">,</span> <span class="n">y_ideal</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The test set error shows this model will not work well on new data. If you use the test error to guide improvements in the model, then the model will perform well on the test data... but the test data was meant to represent <em>new</em> data.
You need yet another set of data to test new data performance.</p>
<p>The proposal made during lecture is to separate data into three groups. The distribution of training, cross-validation and test sets shown in the below table is a typical distribution, but can be varied depending on the amount of data available.</p>
<table>
<thead>
<tr>
<th>data</th>
<th align="center">% of total</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>training</td>
<td align="center">60</td>
<td align="left">Data used to tune model parameters <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span> in training or fitting</td>
</tr>
<tr>
<td>cross-validation</td>
<td align="center">20</td>
<td align="left">Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.</td>
</tr>
<tr>
<td>test</td>
<td align="center">20</td>
<td align="left">Data used to test the model after tuning to gauge performance on new data</td>
</tr>
</tbody>
</table>
<p>Let's generate three data sets below. We'll once again use <code>train_test_split</code> from <code>sklearn</code> but will call it twice to get three splits:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># Generate  data</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">x_ideal</span><span class="p">,</span><span class="n">y_ideal</span> <span class="o">=</span> <span class="n">gen_data</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X.shape&quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y.shape&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#split the data using sklearn routine </span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.40</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_cv</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_</span><span class="p">,</span><span class="n">y_</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train.shape&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y_train.shape&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_cv.shape&quot;</span><span class="p">,</span> <span class="n">X_cv</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y_cv.shape&quot;</span><span class="p">,</span> <span class="n">y_cv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_test.shape&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y_test.shape&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>X.shape (40,) y.shape (40,)
X_train.shape (24,) y_train.shape (24,)
X_cv.shape (8,) y_cv.shape (8,)
X_test.shape (8,) y_test.shape (8,)
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="3"></a></p>
<h2 id="3-bias-and-variance">3 - Bias and Variance<img align="Right" src="./images/C2_W3_BiasVarianceDegree.png"  style=" width:500px; padding: 10px 20px ; "></h2>
<p>Above, it was clear the degree of the polynomial model was too high. How can you choose a good value? It turns out, as shown in the diagram, the training and cross-validation performance can provide guidance. By trying a range of degree values, the training and cross-validation performance can be evaluated. As the degree becomes too large, the cross-validation performance will start to degrade relative to the training performance. Let's try this on our example.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="3.1"></a></p>
<h3 id="31-plot-train-cross-validation-test">3.1 Plot Train, Cross-Validation, Test</h3>
<p>You can see below the datapoints that will be part of training (in red) are intermixed with those that the model is not trained on (test and cv).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_ideal</span><span class="p">,</span> <span class="n">y_ideal</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;orangered&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y_ideal&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training, CV, Test&quot;</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span>           <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span>       <span class="n">color</span> <span class="o">=</span> <span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlorange&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>   <span class="n">color</span> <span class="o">=</span> <span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span>   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="3.2"></a></p>
<h3 id="32-finding-the-optimal-degree">3.2 Finding the optimal degree</h3>
<p>In previous labs, you found that you could create a model capable of fitting complex curves by utilizing a polynomial (See Course1, Week2 Feature Engineering and Polynomial Regression Lab).  Further, you demonstrated that by increasing the <em>degree</em> of the polynomial, you could <em>create</em> overfitting. (See Course 1, Week3, Over-Fitting Lab). Let's use that knowledge here to test our ability to tell the difference between over-fitting and under-fitting.</p>
<p>Let's train the model repeatedly, increasing the degree of the polynomial each iteration. Here, we're going to use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression">scikit-learn</a> linear regression model for speed and simplicity.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">max_degree</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_degree</span><span class="p">)</span>    
<span class="n">err_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_degree</span><span class="p">)</span>      
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span><span class="mi">100</span><span class="p">)</span>  
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="n">max_degree</span><span class="p">))</span>  <span class="c1">#columns are lines to plot</span>

<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_degree</span><span class="p">):</span>
    <span class="n">lmodel</span> <span class="o">=</span> <span class="n">lin_model</span><span class="p">(</span><span class="n">degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">lmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">err_train</span><span class="p">[</span><span class="n">degree</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cv</span><span class="p">)</span>
    <span class="n">err_cv</span><span class="p">[</span><span class="n">degree</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_cv</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="n">y_pred</span><span class="p">[:,</span><span class="n">degree</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">optimal_degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err_cv</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><font size="4">Let's plot the result:</font></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">plt_optimal_degree</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">x_ideal</span><span class="p">,</span> <span class="n">y_ideal</span><span class="p">,</span> 
                   <span class="n">err_train</span><span class="p">,</span> <span class="n">err_cv</span><span class="p">,</span> <span class="n">optimal_degree</span><span class="p">,</span> <span class="n">max_degree</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The plot above demonstrates that separating data into two groups, data the model is trained on and data the model has not been trained on, can be used to determine if the model is underfitting or overfitting. In our example, we created a variety of models varying from underfitting to overfitting by increasing the degree of the polynomial used. 
- On the left plot, the solid lines represent the predictions from these models. A polynomial model with degree 1 produces a straight line that intersects very few data points, while the maximum degree hews very closely to every data point. 
- on the right:
    - the error on the trained data (blue) decreases as the model complexity increases as expected
    - the error of the cross-validation data decreases initially as the model starts to conform to the data, but then increases as the model starts to over-fit on the training data (fails to <em>generalize</em>).     </p>
<p>It's worth noting that the curves in these examples as not as smooth as one might draw for a lecture. It's clear the specific data points assigned to each group can change your results significantly. The general trend is what is important.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="3.3"></a></p>
<h3 id="33-tuning-regularization">3.3 Tuning Regularization.</h3>
<p>In previous labs, you have utilized <em>regularization</em> to reduce overfitting. Similar to degree, one can use the same methodology to tune the regularization parameter lambda (<span class="arithmatex">\(\lambda\)</span>).</p>
<p>Let's demonstrate this by starting with a high degree polynomial and varying the regularization parameter.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">lambda_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span><span class="mf">1e-3</span><span class="p">,</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">])</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lambda_range</span><span class="p">)</span>
<span class="n">degree</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">err_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)</span>    
<span class="n">err_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)</span>       
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span><span class="mi">100</span><span class="p">)</span> 
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="n">num_steps</span><span class="p">))</span>  <span class="c1">#columns are lines to plot</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">lambda_</span><span class="o">=</span> <span class="n">lambda_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lmodel</span> <span class="o">=</span> <span class="n">lin_model</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">regularization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="n">lambda_</span><span class="p">)</span>
    <span class="n">lmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">err_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cv</span><span class="p">)</span>
    <span class="n">err_cv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_cv</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="n">y_pred</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">optimal_reg_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err_cv</span><span class="p">)</span> 
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">plt_tune_regularization</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">err_train</span><span class="p">,</span> <span class="n">err_cv</span><span class="p">,</span> <span class="n">optimal_reg_idx</span><span class="p">,</span> <span class="n">lambda_range</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above, the plots show that as regularization increases, the model moves from a high variance (overfitting) model to a high bias (underfitting) model. The vertical line in the right plot shows the optimal value of lambda. In this example, the polynomial degree was set to 10. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="3.4"></a></p>
<h3 id="34-getting-more-data-increasing-training-set-size-m">3.4 Getting more data: Increasing Training Set Size (m)</h3>
<p>When a model is overfitting (high variance), collecting additional data can improve performance. Let's try that here.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">err_train</span><span class="p">,</span> <span class="n">err_cv</span><span class="p">,</span> <span class="n">m_range</span><span class="p">,</span><span class="n">degree</span> <span class="o">=</span> <span class="n">tune_m</span><span class="p">()</span>
<span class="n">plt_tune_m</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">err_train</span><span class="p">,</span> <span class="n">err_cv</span><span class="p">,</span> <span class="n">m_range</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above plots show that when a model has high variance and is overfitting, adding more examples improves performance. Note the curves on the left plot. The final curve with the highest value of <span class="arithmatex">\(m\)</span> is a smooth curve that is in the center of the data. On the right, as the number of examples increases, the performance of the training set and cross-validation set converge to similar values. Note that the curves are not as smooth as one might see in a lecture. That is to be expected. The trend remains clear: more data improves generalization. </p>
<blockquote>
<p>Note that adding more examples when the model has high bias (underfitting) does not improve performance.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="4"></a></p>
<h2 id="4-evaluating-a-learning-algorithm-neural-network">4 - Evaluating a Learning Algorithm (Neural Network)</h2>
<p>Above, you tuned aspects of a polynomial regression model. Here, you will work with a neural network model. Let's start by creating a classification data set. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="4.1"></a></p>
<h3 id="41-data-set">4.1 Data Set</h3>
<p>Run the cell below to generate a data set and split it into training, cross-validation (CV) and test sets. In this example, we're increasing the percentage of cross-validation data points for emphasis.  </p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># Generate and split data set</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">centers</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">gen_blobs</span><span class="p">()</span>

<span class="c1"># split the data. Large CV population for demonstration</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_cv</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_</span><span class="p">,</span><span class="n">y_</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train.shape:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;X_cv.shape:&quot;</span><span class="p">,</span> <span class="n">X_cv</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;X_test.shape:&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>X_train.shape: (400, 2) X_cv.shape: (320, 2) X_test.shape: (80, 2)
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">plt_train_eq_dist</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">classes</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">centers</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above, you can see the data on the left. There are six clusters identified by color. Both training points (dots) and cross-validataion points (triangles) are shown. The interesting points are those that fall in ambiguous locations where either cluster might consider them members. What would you expect a neural network model to do? What would be an example of overfitting? underfitting?<br />
On the right is an example of an 'ideal' model, or a model one might create knowing the source of the data. The lines represent 'equal distance' boundaries where the distance between center points is equal. It's worth noting that this model would "misclassify" roughly 8% of the total data set.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="4.2"></a></p>
<h3 id="42-evaluating-categorical-model-by-calculating-classification-error">4.2 Evaluating categorical model by calculating classification error</h3>
<p>The evaluation function for categorical models used here is simply the fraction of incorrect predictions:<br />
$$ J_{cv} =\frac{1}{m}\sum_{i=0}^{m-1} 
\begin{cases}
    1, &amp; \text{if <span class="arithmatex">\(\hat{y}^{(i)} \neq y^{(i)}\)</span>}\
    0, &amp; \text{otherwise}
\end{cases}
$$</p>
<p><a name="ex02"></a></p>
<h3 id="exercise-2">Exercise 2</h3>
<p>Below, complete the routine to calculate classification error. Note, in this lab, target values are the index of the category and are not <a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoded</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED CELL: eval_cat_err</span>
<span class="k">def</span> <span class="nf">eval_cat_err</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Calculate the categorization error</span>
<span class="sd">    Args:</span>
<span class="sd">      y    : (ndarray  Shape (m,) or (m,1))  target value of each example</span>
<span class="sd">      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example</span>
<span class="sd">    Returns:|</span>
<span class="sd">      cerr: (scalar)             </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">incorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="c1">### START CODE HERE ### </span>
        <span class="k">if</span> <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">!=</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">incorrect</span> <span class="o">+=</span><span class="mi">1</span>
    <span class="c1">### END CODE HERE ### </span>
    <span class="n">cerr</span> <span class="o">=</span> <span class="n">incorrect</span><span class="o">/</span><span class="n">m</span>
    <span class="k">return</span><span class="p">(</span><span class="n">cerr</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span><span class="w"> </span><span class="n">y_tmp</span><span class="p">))</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, expected:0.333&quot;</span> <span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span><span class="w"> </span><span class="n">y_tmp</span><span class="p">))</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, expected:0.250&quot;</span> <span class="p">)</span>

<span class="c1"># BEGIN UNIT TEST  </span>
<span class="n">test_eval_cat_err</span><span class="p">(</span><span class="n">eval_cat_err</span><span class="p">)</span>
<span class="c1"># END UNIT TEST</span>
<span class="c1"># BEGIN UNIT TEST  </span>
<span class="n">test_eval_cat_err</span><span class="p">(</span><span class="n">eval_cat_err</span><span class="p">)</span>
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>categorization error 0.333, expected:0.333
categorization error 0.250, expected:0.250
<span class="ansi-green-intense-fg"> All tests passed.
</span><span class="ansi-green-intense-fg"> All tests passed.
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<details>
<summary><font size="3" color="darkgreen"><b>Click for hints</b></font></summary>

<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">eval_cat_err</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Calculate the categorization error</span>
<span class="sd">    Args:</span>
<span class="sd">      y    : (ndarray  Shape (m,) or (m,1))  target value of each example</span>
<span class="sd">      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example</span>
<span class="sd">    Returns:|</span>
<span class="sd">      cerr: (scalar)             </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">incorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>    <span class="c1"># @REPLACE</span>
            <span class="n">incorrect</span> <span class="o">+=</span> <span class="mi">1</span>     <span class="c1"># @REPLACE</span>
    <span class="n">cerr</span> <span class="o">=</span> <span class="n">incorrect</span><span class="o">/</span><span class="n">m</span>         <span class="c1"># @REPLACE</span>
    <span class="k">return</span><span class="p">(</span><span class="n">cerr</span><span class="p">)</span>                                    
</code></pre></div>
</details>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="5"></a></p>
<h2 id="5-model-complexity">5 - Model Complexity</h2>
<p>Below, you will build two models. A complex model and a simple model. You will evaluate the models to determine if they are likely to overfit or underfit.</p>
<h3 id="51-complex-model">5.1 Complex model</h3>
<p><a name="ex03"></a></p>
<h3 id="exercise-3">Exercise 3</h3>
<p>Below, compose a three-layer model:
* Dense layer with 120 units, relu activation
* Dense layer with 40 units, relu activation
* Dense layer with 6 units and a linear activation (not softmax)<br />
Compile using
* loss with <code>SparseCategoricalCrossentropy</code>, remember to use  <code>from_logits=True</code>
* Adam optimizer with learning rate of 0.01.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C3</span>
<span class="c1"># GRADED CELL: model</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1">### START CODE HERE ### </span>
      <span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
        <span class="c1">### END CODE HERE ### </span>

    <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Complex&quot;</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="c1">### START CODE HERE ### </span>
    <span class="n">loss</span><span class="o">=</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="c1">### END CODE HERE ### </span>
<span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Epoch 1/1000
13/13 [==============================] - 0s 1ms/step - loss: 1.1106
Epoch 2/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4281
Epoch 3/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3345
Epoch 4/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.2896
Epoch 5/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2867
Epoch 6/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2918
Epoch 7/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2497
Epoch 8/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2298
Epoch 9/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2307
Epoch 10/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2071
Epoch 11/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2115
Epoch 12/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2070
Epoch 13/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2366
Epoch 14/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2261
Epoch 15/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2224
Epoch 16/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2055
Epoch 17/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2044
Epoch 18/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2006
Epoch 19/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2168
Epoch 20/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2047
Epoch 21/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2237
Epoch 22/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2497
Epoch 23/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2113
Epoch 24/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2025
Epoch 25/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2107
Epoch 26/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2000
Epoch 27/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1935
Epoch 28/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1963
Epoch 29/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2188
Epoch 30/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2424
Epoch 31/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1969
Epoch 32/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1950
Epoch 33/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1904
Epoch 34/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2173
Epoch 35/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2074
Epoch 36/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1768
Epoch 37/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1794
Epoch 38/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1733
Epoch 39/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1955
Epoch 40/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1870
Epoch 41/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2128
Epoch 42/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1987
Epoch 43/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1895
Epoch 44/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2073
Epoch 45/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2148
Epoch 46/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1774
Epoch 47/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1886
Epoch 48/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1763
Epoch 49/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1769
Epoch 50/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1763
Epoch 51/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.2020
Epoch 52/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1889
Epoch 53/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2035
Epoch 54/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1761
Epoch 55/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.1838
Epoch 56/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1774
Epoch 57/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1953
Epoch 58/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1882
Epoch 59/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1860
Epoch 60/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1919
Epoch 61/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1848
Epoch 62/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1630
Epoch 63/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1616
Epoch 64/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2008
Epoch 65/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1936
Epoch 66/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1824
Epoch 67/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2092
Epoch 68/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2287
Epoch 69/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.1877
Epoch 70/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1716
Epoch 71/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1917
Epoch 72/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1703
Epoch 73/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1750
Epoch 74/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1836
Epoch 75/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1696
Epoch 76/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1542
Epoch 77/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1715
Epoch 78/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1545
Epoch 79/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1593
Epoch 80/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1844
Epoch 81/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1881
Epoch 82/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1696
Epoch 83/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1614
Epoch 84/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1762
Epoch 85/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1779
Epoch 86/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1658
Epoch 87/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1614
Epoch 88/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1639
Epoch 89/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1629
Epoch 90/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1475
Epoch 91/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1452
Epoch 92/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1473
Epoch 93/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1490
Epoch 94/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1650
Epoch 95/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1706
Epoch 96/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1704
Epoch 97/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1764
Epoch 98/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1855
Epoch 99/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1685
Epoch 100/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1569
Epoch 101/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1645
Epoch 102/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1737
Epoch 103/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1935
Epoch 104/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1600
Epoch 105/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1483
Epoch 106/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1555
Epoch 107/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1678
Epoch 108/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1435
Epoch 109/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1419
Epoch 110/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1494
Epoch 111/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1538
Epoch 112/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1682
Epoch 113/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1687
Epoch 114/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1436
Epoch 115/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1366
Epoch 116/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1485
Epoch 117/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1400
Epoch 118/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1357
Epoch 119/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1444
Epoch 120/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1403
Epoch 121/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1465
Epoch 122/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1549
Epoch 123/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1402
Epoch 124/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1337
Epoch 125/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1422
Epoch 126/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1560
Epoch 127/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1319
Epoch 128/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1389
Epoch 129/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1404
Epoch 130/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1299
Epoch 131/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1247
Epoch 132/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1244
Epoch 133/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1260
Epoch 134/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1158
Epoch 135/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1343
Epoch 136/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1306
Epoch 137/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1294
Epoch 138/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1297
Epoch 139/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1342
Epoch 140/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1255
Epoch 141/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1232
Epoch 142/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1199
Epoch 143/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1192
Epoch 144/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1192
Epoch 145/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1342
Epoch 146/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1477
Epoch 147/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1780
Epoch 148/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1673
Epoch 149/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1402
Epoch 150/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1292
Epoch 151/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1296
Epoch 152/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1221
Epoch 153/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1300
Epoch 154/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1316
Epoch 155/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1274
Epoch 156/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1192
Epoch 157/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1266
Epoch 158/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1185
Epoch 159/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1197
Epoch 160/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1148
Epoch 161/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1137
Epoch 162/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1427
Epoch 163/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1420
Epoch 164/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1327
Epoch 165/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1276
Epoch 166/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1099
Epoch 167/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.1205
Epoch 168/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1307
Epoch 169/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1476
Epoch 170/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1673
Epoch 171/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1349
Epoch 172/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1183
Epoch 173/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1225
Epoch 174/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1276
Epoch 175/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1029
Epoch 176/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1134
Epoch 177/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1081
Epoch 178/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1245
Epoch 179/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1346
Epoch 180/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1233
Epoch 181/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1113
Epoch 182/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1040
Epoch 183/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1155
Epoch 184/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1049
Epoch 185/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1111
Epoch 186/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1079
Epoch 187/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1021
Epoch 188/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1048
Epoch 189/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0971
Epoch 190/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0985
Epoch 191/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1026
Epoch 192/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1111
Epoch 193/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0991
Epoch 194/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0890
Epoch 195/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.0880
Epoch 196/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1006
Epoch 197/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0974
Epoch 198/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1141
Epoch 199/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1423
Epoch 200/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1381
Epoch 201/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1105
Epoch 202/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1005
Epoch 203/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0846
Epoch 204/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1125
Epoch 205/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1129
Epoch 206/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1219
Epoch 207/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1161
Epoch 208/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1137
Epoch 209/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1178
Epoch 210/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1017
Epoch 211/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1051
Epoch 212/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1014
Epoch 213/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1096
Epoch 214/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1087
Epoch 215/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1047
Epoch 216/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1044
Epoch 217/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1044
Epoch 218/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1006
Epoch 219/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1093
Epoch 220/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1041
Epoch 221/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0956
Epoch 222/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1109
Epoch 223/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.1041
Epoch 224/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1000
Epoch 225/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0968
Epoch 226/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0951
Epoch 227/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1092
Epoch 228/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1041
Epoch 229/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1032
Epoch 230/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1153
Epoch 231/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1237
Epoch 232/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0978
Epoch 233/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1074
Epoch 234/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1059
Epoch 235/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1122
Epoch 236/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0974
Epoch 237/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0879
Epoch 238/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0913
Epoch 239/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0831
Epoch 240/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0752
Epoch 241/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0733
Epoch 242/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0886
Epoch 243/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0837
Epoch 244/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0866
Epoch 245/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0933
Epoch 246/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0976
Epoch 247/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1150
Epoch 248/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0904
Epoch 249/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1073
Epoch 250/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1296
Epoch 251/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1022
Epoch 252/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0987
Epoch 253/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0846
Epoch 254/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0813
Epoch 255/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0924
Epoch 256/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0799
Epoch 257/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0947
Epoch 258/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0956
Epoch 259/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0788
Epoch 260/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1018
Epoch 261/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0942
Epoch 262/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0780
Epoch 263/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0821
Epoch 264/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0795
Epoch 265/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0924
Epoch 266/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0948
Epoch 267/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0767
Epoch 268/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0720
Epoch 269/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0742
Epoch 270/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0747
Epoch 271/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0726
Epoch 272/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0984
Epoch 273/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1074
Epoch 274/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0836
Epoch 275/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0783
Epoch 276/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0799
Epoch 277/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1225
Epoch 278/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1017
Epoch 279/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0990
Epoch 280/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1014
Epoch 281/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0808
Epoch 282/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0798
Epoch 283/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0847
Epoch 284/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0755
Epoch 285/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0631
Epoch 286/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0651
Epoch 287/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0602
Epoch 288/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0733
Epoch 289/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0659
Epoch 290/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0682
Epoch 291/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0745
Epoch 292/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0848
Epoch 293/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0701
Epoch 294/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0828
Epoch 295/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0741
Epoch 296/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0890
Epoch 297/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0800
Epoch 298/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0803
Epoch 299/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0765
Epoch 300/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0733
Epoch 301/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0544
Epoch 302/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0718
Epoch 303/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0877
Epoch 304/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0687
Epoch 305/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0671
Epoch 306/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0575
Epoch 307/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0773
Epoch 308/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0779
Epoch 309/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0696
Epoch 310/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0883
Epoch 311/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0880
Epoch 312/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0707
Epoch 313/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0603
Epoch 314/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0772
Epoch 315/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0660
Epoch 316/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0586
Epoch 317/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0618
Epoch 318/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0588
Epoch 319/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0674
Epoch 320/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0598
Epoch 321/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0670
Epoch 322/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0970
Epoch 323/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1366
Epoch 324/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1148
Epoch 325/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0837
Epoch 326/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0749
Epoch 327/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0746
Epoch 328/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0698
Epoch 329/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0691
Epoch 330/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0541
Epoch 331/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0558
Epoch 332/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0653
Epoch 333/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0593
Epoch 334/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0606
Epoch 335/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0696
Epoch 336/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0713
Epoch 337/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0628
Epoch 338/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0752
Epoch 339/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0723
Epoch 340/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0647
Epoch 341/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0688
Epoch 342/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0793
Epoch 343/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0595
Epoch 344/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0528
Epoch 345/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0552
Epoch 346/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0534
Epoch 347/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0471
Epoch 348/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0491
Epoch 349/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0524
Epoch 350/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0696
Epoch 351/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0690
Epoch 352/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0864
Epoch 353/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0999
Epoch 354/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1094
Epoch 355/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1189
Epoch 356/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1059
Epoch 357/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0655
Epoch 358/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0652
Epoch 359/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0544
Epoch 360/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0545
Epoch 361/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0549
Epoch 362/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0581
Epoch 363/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0506
Epoch 364/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0579
Epoch 365/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0583
Epoch 366/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0607
Epoch 367/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0428
Epoch 368/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0495
Epoch 369/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0721
Epoch 370/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0817
Epoch 371/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0588
Epoch 372/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0516
Epoch 373/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0526
Epoch 374/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0463
Epoch 375/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0447
Epoch 376/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0441
Epoch 377/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0422
Epoch 378/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0391
Epoch 379/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0343
Epoch 380/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0461
Epoch 381/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0442
Epoch 382/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0496
Epoch 383/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0509
Epoch 384/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0479
Epoch 385/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0520
Epoch 386/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0391
Epoch 387/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0394
Epoch 388/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0510
Epoch 389/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0525
Epoch 390/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0666
Epoch 391/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0490
Epoch 392/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0551
Epoch 393/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0689
Epoch 394/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0663
Epoch 395/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0844
Epoch 396/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0704
Epoch 397/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0700
Epoch 398/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0591
Epoch 399/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0586
Epoch 400/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0628
Epoch 401/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1717
Epoch 402/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1648
Epoch 403/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1616
Epoch 404/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1326
Epoch 405/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1367
Epoch 406/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1098
Epoch 407/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1122
Epoch 408/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1798
Epoch 409/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1268
Epoch 410/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1123
Epoch 411/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0720
Epoch 412/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0774
Epoch 413/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0661
Epoch 414/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0720
Epoch 415/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0580
Epoch 416/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0572
Epoch 417/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0586
Epoch 418/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0546
Epoch 419/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0573
Epoch 420/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0721
Epoch 421/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0658
Epoch 422/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0686
Epoch 423/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0491
Epoch 424/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0647
Epoch 425/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0465
Epoch 426/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0435
Epoch 427/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0362
Epoch 428/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0411
Epoch 429/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0374
Epoch 430/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0412
Epoch 431/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0391
Epoch 432/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0412
Epoch 433/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0479
Epoch 434/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0436
Epoch 435/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0482
Epoch 436/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0420
Epoch 437/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0347
Epoch 438/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0390
Epoch 439/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0328
Epoch 440/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0371
Epoch 441/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0334
Epoch 442/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0348
Epoch 443/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0370
Epoch 444/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0408
Epoch 445/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0329
Epoch 446/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0318
Epoch 447/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0391
Epoch 448/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0408
Epoch 449/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0346
Epoch 450/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0340
Epoch 451/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0332
Epoch 452/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0325
Epoch 453/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0406
Epoch 454/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0394
Epoch 455/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0584
Epoch 456/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0440
Epoch 457/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0412
Epoch 458/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0468
Epoch 459/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0373
Epoch 460/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0329
Epoch 461/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0390
Epoch 462/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0284
Epoch 463/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0310
Epoch 464/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0348
Epoch 465/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0302
Epoch 466/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0348
Epoch 467/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0350
Epoch 468/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0347
Epoch 469/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0305
Epoch 470/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0369
Epoch 471/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0436
Epoch 472/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0543
Epoch 473/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0477
Epoch 474/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0630
Epoch 475/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1523
Epoch 476/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3248
Epoch 477/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1600
Epoch 478/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1623
Epoch 479/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1206
Epoch 480/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0955
Epoch 481/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1595
Epoch 482/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1626
Epoch 483/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1170
Epoch 484/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1481
Epoch 485/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0686
Epoch 486/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0590
Epoch 487/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0651
Epoch 488/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0575
Epoch 489/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0593
Epoch 490/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0539
Epoch 491/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0451
Epoch 492/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0436
Epoch 493/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0484
Epoch 494/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0639
Epoch 495/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0497
Epoch 496/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0787
Epoch 497/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0805
Epoch 498/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0639
Epoch 499/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0504
Epoch 500/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0478
Epoch 501/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0466
Epoch 502/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0419
Epoch 503/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0365
Epoch 504/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0352
Epoch 505/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0368
Epoch 506/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0337
Epoch 507/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0375
Epoch 508/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0317
Epoch 509/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0318
Epoch 510/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0364
Epoch 511/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0337
Epoch 512/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0290
Epoch 513/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0317
Epoch 514/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0320
Epoch 515/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0271
Epoch 516/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0343
Epoch 517/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0308
Epoch 518/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0388
Epoch 519/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0444
Epoch 520/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0381
Epoch 521/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0356
Epoch 522/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0324
Epoch 523/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0292
Epoch 524/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0308
Epoch 525/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0308
Epoch 526/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0365
Epoch 527/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0351
Epoch 528/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0305
Epoch 529/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0320
Epoch 530/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0351
Epoch 531/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0290
Epoch 532/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0329
Epoch 533/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0387
Epoch 534/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0431
Epoch 535/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0414
Epoch 536/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0318
Epoch 537/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0285
Epoch 538/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0278
Epoch 539/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0274
Epoch 540/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0338
Epoch 541/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0262
Epoch 542/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0283
Epoch 543/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0265
Epoch 544/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0267
Epoch 545/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0278
Epoch 546/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0256
Epoch 547/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0302
Epoch 548/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0323
Epoch 549/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0262
Epoch 550/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0288
Epoch 551/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0283
Epoch 552/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0315
Epoch 553/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0411
Epoch 554/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0376
Epoch 555/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0346
Epoch 556/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0296
Epoch 557/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0307
Epoch 558/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0270
Epoch 559/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0268
Epoch 560/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0303
Epoch 561/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0251
Epoch 562/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0267
Epoch 563/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 564/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0265
Epoch 565/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0297
Epoch 566/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0338
Epoch 567/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0432
Epoch 568/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0483
Epoch 569/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1205
Epoch 570/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1063
Epoch 571/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1035
Epoch 572/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1415
Epoch 573/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1534
Epoch 574/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1474
Epoch 575/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0772
Epoch 576/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0691
Epoch 577/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0770
Epoch 578/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0637
Epoch 579/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0528
Epoch 580/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0371
Epoch 581/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0356
Epoch 582/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0431
Epoch 583/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0300
Epoch 584/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0309
Epoch 585/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0307
Epoch 586/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0321
Epoch 587/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0266
Epoch 588/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0274
Epoch 589/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0276
Epoch 590/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0267
Epoch 591/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0305
Epoch 592/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0278
Epoch 593/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0343
Epoch 594/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0259
Epoch 595/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0259
Epoch 596/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0258
Epoch 597/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0262
Epoch 598/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0254
Epoch 599/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0251
Epoch 600/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0241
Epoch 601/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0269
Epoch 602/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0287
Epoch 603/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0257
Epoch 604/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0254
Epoch 605/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0232
Epoch 606/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0281
Epoch 607/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 608/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0254
Epoch 609/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0237
Epoch 610/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0253
Epoch 611/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0256
Epoch 612/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0235
Epoch 613/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0290
Epoch 614/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0236
Epoch 615/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 616/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0253
Epoch 617/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0231
Epoch 618/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0241
Epoch 619/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0253
Epoch 620/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0290
Epoch 621/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0456
Epoch 622/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0647
Epoch 623/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1078
Epoch 624/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1180
Epoch 625/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0837
Epoch 626/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0510
Epoch 627/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0333
Epoch 628/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0327
Epoch 629/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0389
Epoch 630/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0347
Epoch 631/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0342
Epoch 632/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0272
Epoch 633/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0240
Epoch 634/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0235
Epoch 635/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0243
Epoch 636/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0225
Epoch 637/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0222
Epoch 638/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0223
Epoch 639/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0215
Epoch 640/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 641/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0248
Epoch 642/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0257
Epoch 643/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0213
Epoch 644/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0277
Epoch 645/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0266
Epoch 646/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0320
Epoch 647/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0269
Epoch 648/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0357
Epoch 649/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0321
Epoch 650/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0255
Epoch 651/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0287
Epoch 652/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0251
Epoch 653/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0242
Epoch 654/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0239
Epoch 655/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0218
Epoch 656/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0227
Epoch 657/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 658/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0265
Epoch 659/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0257
Epoch 660/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0233
Epoch 661/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0246
Epoch 662/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0313
Epoch 663/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0238
Epoch 664/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0277
Epoch 665/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0205
Epoch 666/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0238
Epoch 667/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 668/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0441
Epoch 669/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0441
Epoch 670/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0305
Epoch 671/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0323
Epoch 672/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0356
Epoch 673/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0670
Epoch 674/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1732
Epoch 675/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0889
Epoch 676/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1098
Epoch 677/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0468
Epoch 678/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0532
Epoch 679/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0577
Epoch 680/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0880
Epoch 681/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1123
Epoch 682/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1581
Epoch 683/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1343
Epoch 684/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1065
Epoch 685/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1236
Epoch 686/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1184
Epoch 687/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1218
Epoch 688/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1673
Epoch 689/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1437
Epoch 690/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0897
Epoch 691/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0665
Epoch 692/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0579
Epoch 693/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0563
Epoch 694/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0425
Epoch 695/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0441
Epoch 696/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0411
Epoch 697/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0429
Epoch 698/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0347
Epoch 699/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0367
Epoch 700/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0311
Epoch 701/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0333
Epoch 702/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0308
Epoch 703/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0287
Epoch 704/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0297
Epoch 705/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0282
Epoch 706/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0263
Epoch 707/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0286
Epoch 708/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0275
Epoch 709/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0274
Epoch 710/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0252
Epoch 711/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0277
Epoch 712/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0261
Epoch 713/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0311
Epoch 714/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0265
Epoch 715/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0281
Epoch 716/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0275
Epoch 717/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0264
Epoch 718/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0240
Epoch 719/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0234
Epoch 720/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0284
Epoch 721/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0311
Epoch 722/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0244
Epoch 723/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 724/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0269
Epoch 725/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0224
Epoch 726/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0238
Epoch 727/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0234
Epoch 728/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0223
Epoch 729/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0220
Epoch 730/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0268
Epoch 731/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0363
Epoch 732/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0300
Epoch 733/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0208
Epoch 734/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0254
Epoch 735/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0264
Epoch 736/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0230
Epoch 737/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0224
Epoch 738/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0270
Epoch 739/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0257
Epoch 740/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0228
Epoch 741/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 742/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0241
Epoch 743/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0210
Epoch 744/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0216
Epoch 745/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0208
Epoch 746/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0227
Epoch 747/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0193
Epoch 748/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0241
Epoch 749/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0217
Epoch 750/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0248
Epoch 751/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0203
Epoch 752/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0194
Epoch 753/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0252
Epoch 754/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0203
Epoch 755/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0206
Epoch 756/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0192
Epoch 757/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0213
Epoch 758/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0206
Epoch 759/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 760/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0227
Epoch 761/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0204
Epoch 762/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0219
Epoch 763/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0266
Epoch 764/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0699
Epoch 765/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0436
Epoch 766/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0451
Epoch 767/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.1029
Epoch 768/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1082
Epoch 769/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0924
Epoch 770/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0936
Epoch 771/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0690
Epoch 772/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0589
Epoch 773/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0519
Epoch 774/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0714
Epoch 775/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1015
Epoch 776/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0932
Epoch 777/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1891
Epoch 778/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1356
Epoch 779/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1081
Epoch 780/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0973
Epoch 781/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0768
Epoch 782/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0761
Epoch 783/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1075
Epoch 784/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0789
Epoch 785/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0467
Epoch 786/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0394
Epoch 787/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0360
Epoch 788/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0324
Epoch 789/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0329
Epoch 790/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0291
Epoch 791/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0283
Epoch 792/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0291
Epoch 793/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0261
Epoch 794/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0294
Epoch 795/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0250
Epoch 796/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0292
Epoch 797/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0286
Epoch 798/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0271
Epoch 799/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0307
Epoch 800/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0298
Epoch 801/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0371
Epoch 802/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0259
Epoch 803/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0274
Epoch 804/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0266
Epoch 805/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0260
Epoch 806/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0254
Epoch 807/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0258
Epoch 808/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0252
Epoch 809/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0280
Epoch 810/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0249
Epoch 811/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0255
Epoch 812/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0259
Epoch 813/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0310
Epoch 814/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0258
Epoch 815/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0246
Epoch 816/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0263
Epoch 817/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0328
Epoch 818/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 819/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0250
Epoch 820/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0258
Epoch 821/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0252
Epoch 822/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.0256
Epoch 823/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0299
Epoch 824/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0312
Epoch 825/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0243
Epoch 826/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0263
Epoch 827/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0247
Epoch 828/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0233
Epoch 829/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0246
Epoch 830/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0262
Epoch 831/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0259
Epoch 832/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0238
Epoch 833/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0221
Epoch 834/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0240
Epoch 835/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0248
Epoch 836/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0253
Epoch 837/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0340
Epoch 838/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0229
Epoch 839/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0294
Epoch 840/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0286
Epoch 841/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0268
Epoch 842/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0283
Epoch 843/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0271
Epoch 844/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0247
Epoch 845/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0235
Epoch 846/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0300
Epoch 847/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0246
Epoch 848/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0244
Epoch 849/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0219
Epoch 850/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0258
Epoch 851/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0244
Epoch 852/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0257
Epoch 853/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0220
Epoch 854/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0221
Epoch 855/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0256
Epoch 856/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0211
Epoch 857/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0227
Epoch 858/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0252
Epoch 859/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0224
Epoch 860/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0214
Epoch 861/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0204
Epoch 862/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0228
Epoch 863/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0206
Epoch 864/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0198
Epoch 865/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0200
Epoch 866/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0273
Epoch 867/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0271
Epoch 868/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0217
Epoch 869/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.0231
Epoch 870/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0325
Epoch 871/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0354
Epoch 872/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0321
Epoch 873/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0216
Epoch 874/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0201
Epoch 875/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0218
Epoch 876/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0217
Epoch 877/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0275
Epoch 878/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0305
Epoch 879/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0440
Epoch 880/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0466
Epoch 881/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0729
Epoch 882/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0460
Epoch 883/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0439
Epoch 884/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0811
Epoch 885/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0291
Epoch 886/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0309
Epoch 887/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0289
Epoch 888/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0294
Epoch 889/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0283
Epoch 890/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0240
Epoch 891/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0232
Epoch 892/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0225
Epoch 893/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0196
Epoch 894/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0218
Epoch 895/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0189
Epoch 896/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0221
Epoch 897/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.0204
Epoch 898/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0200
Epoch 899/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0208
Epoch 900/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0205
Epoch 901/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0199
Epoch 902/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0298
Epoch 903/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0185
Epoch 904/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0290
Epoch 905/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0272
Epoch 906/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0237
Epoch 907/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0190
Epoch 908/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0210
Epoch 909/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0189
Epoch 910/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0199
Epoch 911/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0688
Epoch 912/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1337
Epoch 913/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1883
Epoch 914/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2096
Epoch 915/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.1323
Epoch 916/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0795
Epoch 917/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1167
Epoch 918/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0621
Epoch 919/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0929
Epoch 920/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0352
Epoch 921/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0303
Epoch 922/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0287
Epoch 923/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0457
Epoch 924/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0712
Epoch 925/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0553
Epoch 926/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0385
Epoch 927/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0311
Epoch 928/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0394
Epoch 929/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0261
Epoch 930/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0346
Epoch 931/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0332
Epoch 932/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0322
Epoch 933/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0311
Epoch 934/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0493
Epoch 935/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0289
Epoch 936/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0325
Epoch 937/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0255
Epoch 938/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0210
Epoch 939/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0235
Epoch 940/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0259
Epoch 941/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0371
Epoch 942/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0300
Epoch 943/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0265
Epoch 944/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0327
Epoch 945/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0367
Epoch 946/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0307
Epoch 947/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0376
Epoch 948/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0375
Epoch 949/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0350
Epoch 950/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0284
Epoch 951/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0293
Epoch 952/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0374
Epoch 953/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0353
Epoch 954/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0395
Epoch 955/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0405
Epoch 956/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0432
Epoch 957/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0234
Epoch 958/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0266
Epoch 959/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0213
Epoch 960/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0200
Epoch 961/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0203
Epoch 962/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0190
Epoch 963/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0239
Epoch 964/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0240
Epoch 965/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0261
Epoch 966/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0197
Epoch 967/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0206
Epoch 968/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0188
Epoch 969/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0200
Epoch 970/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0169
Epoch 971/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0161
Epoch 972/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0176
Epoch 973/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0218
Epoch 974/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0161
Epoch 975/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0203
Epoch 976/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0384
Epoch 977/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0292
Epoch 978/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0234
Epoch 979/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0522
Epoch 980/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0851
Epoch 981/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0541
Epoch 982/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0380
Epoch 983/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0328
Epoch 984/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0276
Epoch 985/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0227
Epoch 986/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0235
Epoch 987/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0287
Epoch 988/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0170
Epoch 989/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0166
Epoch 990/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0175
Epoch 991/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0149
Epoch 992/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0152
Epoch 993/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0153
Epoch 994/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0142
Epoch 995/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0199
Epoch 996/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0231
Epoch 997/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0199
Epoch 998/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0188
Epoch 999/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.0155
Epoch 1000/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.0172
</code>
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>&lt;keras.callbacks.History at 0x7f890fe3cad0&gt;</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">model_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> 
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Model: &#34;Complex&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_3 (Dense)             (None, 120)               360       

 dense_4 (Dense)             (None, 40)                4840      

 dense_5 (Dense)             (None, 6)                 246       

=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
<span class="ansi-green-intense-fg">All tests passed!
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<details>
<summary><font size="3" color="darkgreen"><b>Click for hints</b></font></summary>

Summary should match this (layer instance names may increment )
<div class="highlight"><pre><span></span><code>Model: &quot;Complex&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
L1 (Dense)                   (None, 120)               360       
_________________________________________________________________
L2 (Dense)                   (None, 40)                4840      
_________________________________________________________________
L3 (Dense)                   (None, 6)                 246       
=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>
  <details>
<summary><font size="3" color="darkgreen"><b>Click for more hints</b></font></summary>

<div class="highlight"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;L1&quot;</span><span class="p">),</span>      
        <span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;L2&quot;</span><span class="p">),</span>         
        <span class="n">Dense</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;L3&quot;</span><span class="p">)</span>  
    <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Complex&quot;</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>          
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>   
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>                                  
</code></pre></div>
</details>
</details>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1">#make a model for plotting routines to call</span>
<span class="n">model_predict</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">Xl</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xl</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt_nn</span><span class="p">(</span><span class="n">model_predict</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">suptitle</span><span class="o">=</span><span class="s2">&quot;Complex Model&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This model has worked very hard to capture outliers of each category. As a result, it has miscategorized some of the cross-validation data. Let's calculate the classification error.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">training_cerr_complex</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">model_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">cv_cerr_complex</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_cv</span><span class="p">,</span> <span class="n">model_predict</span><span class="p">(</span><span class="n">X_cv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, training, complex model: </span><span class="si">{</span><span class="n">training_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, cv,       complex model: </span><span class="si">{</span><span class="n">cv_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>categorization error, training, complex model: 0.003
categorization error, cv,       complex model: 0.122
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="5.1"></a></p>
<h3 id="51-simple-model">5.1 Simple model</h3>
<p>Now, let's try a simple model</p>
<p><a name="ex04"></a></p>
<h3 id="exercise-4">Exercise 4</h3>
<p>Below, compose a two-layer model:
* Dense layer with 6 units, relu activation
* Dense layer with 6 units and a linear activation. 
Compile using
* loss with <code>SparseCategoricalCrossentropy</code>, remember to use  <code>from_logits=True</code>
* Adam optimizer with learning rate of 0.01.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C4</span>
<span class="c1"># GRADED CELL: model_s</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model_s</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1">### START CODE HERE ### </span>
      <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>

        <span class="c1">### END CODE HERE ### </span>
    <span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Simple&quot;</span>
<span class="p">)</span>
<span class="n">model_s</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="c1">### START CODE HERE ### </span>
    <span class="n">loss</span><span class="o">=</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="c1">### START CODE HERE ### </span>
<span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model_s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Epoch 1/1000
13/13 [==============================] - 0s 1ms/step - loss: 1.7306
Epoch 2/1000
13/13 [==============================] - 0s 825us/step - loss: 1.4468
Epoch 3/1000
13/13 [==============================] - 0s 836us/step - loss: 1.2902
Epoch 4/1000
13/13 [==============================] - 0s 869us/step - loss: 1.1367
Epoch 5/1000
13/13 [==============================] - 0s 865us/step - loss: 0.9710
Epoch 6/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7947
Epoch 7/1000
13/13 [==============================] - 0s 873us/step - loss: 0.6499
Epoch 8/1000
13/13 [==============================] - 0s 817us/step - loss: 0.5378
Epoch 9/1000
13/13 [==============================] - 0s 814us/step - loss: 0.4652
Epoch 10/1000
13/13 [==============================] - 0s 816us/step - loss: 0.4184
Epoch 11/1000
13/13 [==============================] - 0s 819us/step - loss: 0.3860
Epoch 12/1000
13/13 [==============================] - 0s 925us/step - loss: 0.3641
Epoch 13/1000
13/13 [==============================] - 0s 941us/step - loss: 0.3487
Epoch 14/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3316
Epoch 15/1000
13/13 [==============================] - 0s 878us/step - loss: 0.3201
Epoch 16/1000
13/13 [==============================] - 0s 805us/step - loss: 0.3110
Epoch 17/1000
13/13 [==============================] - 0s 802us/step - loss: 0.3026
Epoch 18/1000
13/13 [==============================] - 0s 832us/step - loss: 0.2953
Epoch 19/1000
13/13 [==============================] - 0s 835us/step - loss: 0.2880
Epoch 20/1000
13/13 [==============================] - 0s 836us/step - loss: 0.2824
Epoch 21/1000
13/13 [==============================] - 0s 835us/step - loss: 0.2768
Epoch 22/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2716
Epoch 23/1000
13/13 [==============================] - 0s 834us/step - loss: 0.2690
Epoch 24/1000
13/13 [==============================] - 0s 857us/step - loss: 0.2618
Epoch 25/1000
13/13 [==============================] - 0s 828us/step - loss: 0.2606
Epoch 26/1000
13/13 [==============================] - 0s 838us/step - loss: 0.2560
Epoch 27/1000
13/13 [==============================] - 0s 849us/step - loss: 0.2516
Epoch 28/1000
13/13 [==============================] - 0s 834us/step - loss: 0.2500
Epoch 29/1000
13/13 [==============================] - 0s 819us/step - loss: 0.2497
Epoch 30/1000
13/13 [==============================] - 0s 933us/step - loss: 0.2424
Epoch 31/1000
13/13 [==============================] - 0s 844us/step - loss: 0.2406
Epoch 32/1000
13/13 [==============================] - 0s 841us/step - loss: 0.2386
Epoch 33/1000
13/13 [==============================] - 0s 843us/step - loss: 0.2371
Epoch 34/1000
13/13 [==============================] - 0s 839us/step - loss: 0.2355
Epoch 35/1000
13/13 [==============================] - 0s 866us/step - loss: 0.2328
Epoch 36/1000
13/13 [==============================] - 0s 853us/step - loss: 0.2311
Epoch 37/1000
13/13 [==============================] - 0s 851us/step - loss: 0.2289
Epoch 38/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2271
Epoch 39/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2278
Epoch 40/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2269
Epoch 41/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2244
Epoch 42/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2250
Epoch 43/1000
13/13 [==============================] - 0s 869us/step - loss: 0.2228
Epoch 44/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2227
Epoch 45/1000
13/13 [==============================] - 0s 924us/step - loss: 0.2230
Epoch 46/1000
13/13 [==============================] - 0s 885us/step - loss: 0.2198
Epoch 47/1000
13/13 [==============================] - 0s 879us/step - loss: 0.2188
Epoch 48/1000
13/13 [==============================] - 0s 887us/step - loss: 0.2156
Epoch 49/1000
13/13 [==============================] - 0s 886us/step - loss: 0.2156
Epoch 50/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.2165
Epoch 51/1000
13/13 [==============================] - 0s 973us/step - loss: 0.2155
Epoch 52/1000
13/13 [==============================] - 0s 870us/step - loss: 0.2130
Epoch 53/1000
13/13 [==============================] - 0s 850us/step - loss: 0.2121
Epoch 54/1000
13/13 [==============================] - 0s 862us/step - loss: 0.2122
Epoch 55/1000
13/13 [==============================] - 0s 837us/step - loss: 0.2105
Epoch 56/1000
13/13 [==============================] - 0s 867us/step - loss: 0.2116
Epoch 57/1000
13/13 [==============================] - 0s 883us/step - loss: 0.2121
Epoch 58/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2084
Epoch 59/1000
13/13 [==============================] - 0s 819us/step - loss: 0.2122
Epoch 60/1000
13/13 [==============================] - 0s 808us/step - loss: 0.2101
Epoch 61/1000
13/13 [==============================] - 0s 820us/step - loss: 0.2095
Epoch 62/1000
13/13 [==============================] - 0s 814us/step - loss: 0.2092
Epoch 63/1000
13/13 [==============================] - 0s 809us/step - loss: 0.2116
Epoch 64/1000
13/13 [==============================] - 0s 809us/step - loss: 0.2085
Epoch 65/1000
13/13 [==============================] - 0s 797us/step - loss: 0.2120
Epoch 66/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2087
Epoch 67/1000
13/13 [==============================] - 0s 912us/step - loss: 0.2107
Epoch 68/1000
13/13 [==============================] - 0s 919us/step - loss: 0.2090
Epoch 69/1000
13/13 [==============================] - 0s 887us/step - loss: 0.2084
Epoch 70/1000
13/13 [==============================] - 0s 914us/step - loss: 0.2053
Epoch 71/1000
13/13 [==============================] - 0s 924us/step - loss: 0.2060
Epoch 72/1000
13/13 [==============================] - 0s 930us/step - loss: 0.2061
Epoch 73/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2075
Epoch 74/1000
13/13 [==============================] - 0s 941us/step - loss: 0.2067
Epoch 75/1000
13/13 [==============================] - 0s 941us/step - loss: 0.2039
Epoch 76/1000
13/13 [==============================] - 0s 929us/step - loss: 0.2036
Epoch 77/1000
13/13 [==============================] - 0s 880us/step - loss: 0.2062
Epoch 78/1000
13/13 [==============================] - 0s 821us/step - loss: 0.2017
Epoch 79/1000
13/13 [==============================] - 0s 818us/step - loss: 0.2044
Epoch 80/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2055
Epoch 81/1000
13/13 [==============================] - 0s 827us/step - loss: 0.1999
Epoch 82/1000
13/13 [==============================] - 0s 823us/step - loss: 0.2028
Epoch 83/1000
13/13 [==============================] - 0s 810us/step - loss: 0.2019
Epoch 84/1000
13/13 [==============================] - 0s 815us/step - loss: 0.2042
Epoch 85/1000
13/13 [==============================] - 0s 812us/step - loss: 0.2016
Epoch 86/1000
13/13 [==============================] - 0s 810us/step - loss: 0.2068
Epoch 87/1000
13/13 [==============================] - 0s 854us/step - loss: 0.2005
Epoch 88/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2011
Epoch 89/1000
13/13 [==============================] - 0s 848us/step - loss: 0.2000
Epoch 90/1000
13/13 [==============================] - 0s 837us/step - loss: 0.1998
Epoch 91/1000
13/13 [==============================] - 0s 813us/step - loss: 0.1992
Epoch 92/1000
13/13 [==============================] - 0s 801us/step - loss: 0.2001
Epoch 93/1000
13/13 [==============================] - 0s 791us/step - loss: 0.1997
Epoch 94/1000
13/13 [==============================] - 0s 786us/step - loss: 0.2008
Epoch 95/1000
13/13 [==============================] - 0s 793us/step - loss: 0.2015
Epoch 96/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.2011
Epoch 97/1000
13/13 [==============================] - 0s 828us/step - loss: 0.2006
Epoch 98/1000
13/13 [==============================] - 0s 824us/step - loss: 0.2031
Epoch 99/1000
13/13 [==============================] - 0s 806us/step - loss: 0.1991
Epoch 100/1000
13/13 [==============================] - 0s 804us/step - loss: 0.2006
Epoch 101/1000
13/13 [==============================] - 0s 816us/step - loss: 0.2010
Epoch 102/1000
13/13 [==============================] - 0s 812us/step - loss: 0.2018
Epoch 103/1000
13/13 [==============================] - 0s 804us/step - loss: 0.2026
Epoch 104/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1988
Epoch 105/1000
13/13 [==============================] - 0s 838us/step - loss: 0.1974
Epoch 106/1000
13/13 [==============================] - 0s 827us/step - loss: 0.1966
Epoch 107/1000
13/13 [==============================] - 0s 858us/step - loss: 0.1963
Epoch 108/1000
13/13 [==============================] - 0s 809us/step - loss: 0.1969
Epoch 109/1000
13/13 [==============================] - 0s 813us/step - loss: 0.1987
Epoch 110/1000
13/13 [==============================] - 0s 802us/step - loss: 0.1978
Epoch 111/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1962
Epoch 112/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1979
Epoch 113/1000
13/13 [==============================] - 0s 829us/step - loss: 0.1944
Epoch 114/1000
13/13 [==============================] - 0s 800us/step - loss: 0.1987
Epoch 115/1000
13/13 [==============================] - 0s 806us/step - loss: 0.1934
Epoch 116/1000
13/13 [==============================] - 0s 798us/step - loss: 0.2009
Epoch 117/1000
13/13 [==============================] - 0s 796us/step - loss: 0.1943
Epoch 118/1000
13/13 [==============================] - 0s 814us/step - loss: 0.1969
Epoch 119/1000
13/13 [==============================] - 0s 808us/step - loss: 0.1951
Epoch 120/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1964
Epoch 121/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1957
Epoch 122/1000
13/13 [==============================] - 0s 877us/step - loss: 0.1970
Epoch 123/1000
13/13 [==============================] - 0s 863us/step - loss: 0.1960
Epoch 124/1000
13/13 [==============================] - 0s 876us/step - loss: 0.1973
Epoch 125/1000
13/13 [==============================] - 0s 850us/step - loss: 0.1961
Epoch 126/1000
13/13 [==============================] - 0s 845us/step - loss: 0.1957
Epoch 127/1000
13/13 [==============================] - 0s 985us/step - loss: 0.1949
Epoch 128/1000
13/13 [==============================] - 0s 892us/step - loss: 0.1946
Epoch 129/1000
13/13 [==============================] - 0s 827us/step - loss: 0.1944
Epoch 130/1000
13/13 [==============================] - 0s 841us/step - loss: 0.1969
Epoch 131/1000
13/13 [==============================] - 0s 837us/step - loss: 0.1926
Epoch 132/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1925
Epoch 133/1000
13/13 [==============================] - 0s 849us/step - loss: 0.1933
Epoch 134/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1942
Epoch 135/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1976
Epoch 136/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1939
Epoch 137/1000
13/13 [==============================] - 0s 859us/step - loss: 0.1931
Epoch 138/1000
13/13 [==============================] - 0s 843us/step - loss: 0.1947
Epoch 139/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1941
Epoch 140/1000
13/13 [==============================] - 0s 872us/step - loss: 0.1917
Epoch 141/1000
13/13 [==============================] - 0s 888us/step - loss: 0.1922
Epoch 142/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1917
Epoch 143/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1944
Epoch 144/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1948
Epoch 145/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1921
Epoch 146/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1920
Epoch 147/1000
13/13 [==============================] - 0s 819us/step - loss: 0.1925
Epoch 148/1000
13/13 [==============================] - 0s 811us/step - loss: 0.1899
Epoch 149/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1913
Epoch 150/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.1914
Epoch 151/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1944
Epoch 152/1000
13/13 [==============================] - 0s 900us/step - loss: 0.1920
Epoch 153/1000
13/13 [==============================] - 0s 888us/step - loss: 0.1949
Epoch 154/1000
13/13 [==============================] - 0s 849us/step - loss: 0.1904
Epoch 155/1000
13/13 [==============================] - 0s 889us/step - loss: 0.1917
Epoch 156/1000
13/13 [==============================] - 0s 900us/step - loss: 0.1898
Epoch 157/1000
13/13 [==============================] - 0s 893us/step - loss: 0.1913
Epoch 158/1000
13/13 [==============================] - 0s 860us/step - loss: 0.1905
Epoch 159/1000
13/13 [==============================] - 0s 892us/step - loss: 0.1898
Epoch 160/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1910
Epoch 161/1000
13/13 [==============================] - 0s 885us/step - loss: 0.1913
Epoch 162/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1930
Epoch 163/1000
13/13 [==============================] - 0s 925us/step - loss: 0.1913
Epoch 164/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1907
Epoch 165/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1910
Epoch 166/1000
13/13 [==============================] - 0s 924us/step - loss: 0.1891
Epoch 167/1000
13/13 [==============================] - 0s 892us/step - loss: 0.1940
Epoch 168/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1914
Epoch 169/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1914
Epoch 170/1000
13/13 [==============================] - 0s 861us/step - loss: 0.1893
Epoch 171/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1894
Epoch 172/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1879
Epoch 173/1000
13/13 [==============================] - 0s 937us/step - loss: 0.1924
Epoch 174/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1887
Epoch 175/1000
13/13 [==============================] - 0s 932us/step - loss: 0.1876
Epoch 176/1000
13/13 [==============================] - 0s 854us/step - loss: 0.1861
Epoch 177/1000
13/13 [==============================] - 0s 871us/step - loss: 0.1922
Epoch 178/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1977
Epoch 179/1000
13/13 [==============================] - 0s 886us/step - loss: 0.1881
Epoch 180/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1894
Epoch 181/1000
13/13 [==============================] - 0s 895us/step - loss: 0.1906
Epoch 182/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1894
Epoch 183/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1872
Epoch 184/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1893
Epoch 185/1000
13/13 [==============================] - 0s 902us/step - loss: 0.1885
Epoch 186/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1867
Epoch 187/1000
13/13 [==============================] - 0s 878us/step - loss: 0.1866
Epoch 188/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1884
Epoch 189/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1907
Epoch 190/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1890
Epoch 191/1000
13/13 [==============================] - 0s 869us/step - loss: 0.1880
Epoch 192/1000
13/13 [==============================] - 0s 826us/step - loss: 0.1863
Epoch 193/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1904
Epoch 194/1000
13/13 [==============================] - 0s 907us/step - loss: 0.1857
Epoch 195/1000
13/13 [==============================] - 0s 814us/step - loss: 0.1859
Epoch 196/1000
13/13 [==============================] - 0s 809us/step - loss: 0.1856
Epoch 197/1000
13/13 [==============================] - 0s 829us/step - loss: 0.1879
Epoch 198/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1884
Epoch 199/1000
13/13 [==============================] - 0s 830us/step - loss: 0.1894
Epoch 200/1000
13/13 [==============================] - 0s 809us/step - loss: 0.1860
Epoch 201/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1869
Epoch 202/1000
13/13 [==============================] - 0s 843us/step - loss: 0.1837
Epoch 203/1000
13/13 [==============================] - 0s 845us/step - loss: 0.1861
Epoch 204/1000
13/13 [==============================] - 0s 925us/step - loss: 0.1869
Epoch 205/1000
13/13 [==============================] - 0s 874us/step - loss: 0.1846
Epoch 206/1000
13/13 [==============================] - 0s 841us/step - loss: 0.1881
Epoch 207/1000
13/13 [==============================] - 0s 838us/step - loss: 0.1841
Epoch 208/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1902
Epoch 209/1000
13/13 [==============================] - 0s 879us/step - loss: 0.1850
Epoch 210/1000
13/13 [==============================] - 0s 875us/step - loss: 0.1883
Epoch 211/1000
13/13 [==============================] - 0s 872us/step - loss: 0.1863
Epoch 212/1000
13/13 [==============================] - 0s 893us/step - loss: 0.1856
Epoch 213/1000
13/13 [==============================] - 0s 839us/step - loss: 0.1860
Epoch 214/1000
13/13 [==============================] - 0s 825us/step - loss: 0.1890
Epoch 215/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1855
Epoch 216/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1891
Epoch 217/1000
13/13 [==============================] - 0s 878us/step - loss: 0.1834
Epoch 218/1000
13/13 [==============================] - 0s 901us/step - loss: 0.1887
Epoch 219/1000
13/13 [==============================] - 0s 924us/step - loss: 0.1857
Epoch 220/1000
13/13 [==============================] - 0s 881us/step - loss: 0.1844
Epoch 221/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1846
Epoch 222/1000
13/13 [==============================] - 0s 858us/step - loss: 0.1843
Epoch 223/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1878
Epoch 224/1000
13/13 [==============================] - 0s 856us/step - loss: 0.1884
Epoch 225/1000
13/13 [==============================] - 0s 853us/step - loss: 0.1851
Epoch 226/1000
13/13 [==============================] - 0s 829us/step - loss: 0.1844
Epoch 227/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1824
Epoch 228/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1849
Epoch 229/1000
13/13 [==============================] - 0s 825us/step - loss: 0.1879
Epoch 230/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1860
Epoch 231/1000
13/13 [==============================] - 0s 997us/step - loss: 0.1834
Epoch 232/1000
13/13 [==============================] - 0s 819us/step - loss: 0.1882
Epoch 233/1000
13/13 [==============================] - 0s 808us/step - loss: 0.1851
Epoch 234/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1874
Epoch 235/1000
13/13 [==============================] - 0s 815us/step - loss: 0.1822
Epoch 236/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1841
Epoch 237/1000
13/13 [==============================] - 0s 877us/step - loss: 0.1876
Epoch 238/1000
13/13 [==============================] - 0s 800us/step - loss: 0.1923
Epoch 239/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1867
Epoch 240/1000
13/13 [==============================] - 0s 843us/step - loss: 0.1832
Epoch 241/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1863
Epoch 242/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1978
Epoch 243/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1946
Epoch 244/1000
13/13 [==============================] - 0s 831us/step - loss: 0.1871
Epoch 245/1000
13/13 [==============================] - 0s 807us/step - loss: 0.1826
Epoch 246/1000
13/13 [==============================] - 0s 805us/step - loss: 0.1850
Epoch 247/1000
13/13 [==============================] - 0s 994us/step - loss: 0.1836
Epoch 248/1000
13/13 [==============================] - 0s 821us/step - loss: 0.1820
Epoch 249/1000
13/13 [==============================] - 0s 840us/step - loss: 0.1857
Epoch 250/1000
13/13 [==============================] - 0s 849us/step - loss: 0.1829
Epoch 251/1000
13/13 [==============================] - 0s 849us/step - loss: 0.1838
Epoch 252/1000
13/13 [==============================] - 0s 842us/step - loss: 0.1828
Epoch 253/1000
13/13 [==============================] - 0s 840us/step - loss: 0.1842
Epoch 254/1000
13/13 [==============================] - 0s 828us/step - loss: 0.1832
Epoch 255/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1830
Epoch 256/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1830
Epoch 257/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1833
Epoch 258/1000
13/13 [==============================] - 0s 946us/step - loss: 0.1826
Epoch 259/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1796
Epoch 260/1000
13/13 [==============================] - 0s 888us/step - loss: 0.1876
Epoch 261/1000
13/13 [==============================] - 0s 841us/step - loss: 0.1819
Epoch 262/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1826
Epoch 263/1000
13/13 [==============================] - 0s 958us/step - loss: 0.1827
Epoch 264/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1820
Epoch 265/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1831
Epoch 266/1000
13/13 [==============================] - 0s 980us/step - loss: 0.1805
Epoch 267/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1835
Epoch 268/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1812
Epoch 269/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1817
Epoch 270/1000
13/13 [==============================] - 0s 982us/step - loss: 0.1836
Epoch 271/1000
13/13 [==============================] - 0s 950us/step - loss: 0.1801
Epoch 272/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1868
Epoch 273/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1869
Epoch 274/1000
13/13 [==============================] - 0s 863us/step - loss: 0.1815
Epoch 275/1000
13/13 [==============================] - 0s 816us/step - loss: 0.1847
Epoch 276/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1787
Epoch 277/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1841
Epoch 278/1000
13/13 [==============================] - 0s 912us/step - loss: 0.1804
Epoch 279/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1861
Epoch 280/1000
13/13 [==============================] - 0s 924us/step - loss: 0.1816
Epoch 281/1000
13/13 [==============================] - 0s 895us/step - loss: 0.1797
Epoch 282/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1807
Epoch 283/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1815
Epoch 284/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1822
Epoch 285/1000
13/13 [==============================] - 0s 934us/step - loss: 0.1813
Epoch 286/1000
13/13 [==============================] - 0s 991us/step - loss: 0.1815
Epoch 287/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1829
Epoch 288/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1849
Epoch 289/1000
13/13 [==============================] - 0s 832us/step - loss: 0.1805
Epoch 290/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1807
Epoch 291/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1801
Epoch 292/1000
13/13 [==============================] - 0s 955us/step - loss: 0.1793
Epoch 293/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1815
Epoch 294/1000
13/13 [==============================] - 0s 950us/step - loss: 0.1784
Epoch 295/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1867
Epoch 296/1000
13/13 [==============================] - 0s 811us/step - loss: 0.1805
Epoch 297/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1855
Epoch 298/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1816
Epoch 299/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1798
Epoch 300/1000
13/13 [==============================] - 0s 901us/step - loss: 0.1817
Epoch 301/1000
13/13 [==============================] - 0s 932us/step - loss: 0.1823
Epoch 302/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1878
Epoch 303/1000
13/13 [==============================] - 0s 882us/step - loss: 0.1788
Epoch 304/1000
13/13 [==============================] - 0s 809us/step - loss: 0.1850
Epoch 305/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1827
Epoch 306/1000
13/13 [==============================] - 0s 949us/step - loss: 0.1818
Epoch 307/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1811
Epoch 308/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1827
Epoch 309/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1814
Epoch 310/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1854
Epoch 311/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1785
Epoch 312/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1831
Epoch 313/1000
13/13 [==============================] - 0s 978us/step - loss: 0.1775
Epoch 314/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1820
Epoch 315/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1801
Epoch 316/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1792
Epoch 317/1000
13/13 [==============================] - 0s 871us/step - loss: 0.1847
Epoch 318/1000
13/13 [==============================] - 0s 825us/step - loss: 0.1841
Epoch 319/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1811
Epoch 320/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1841
Epoch 321/1000
13/13 [==============================] - 0s 927us/step - loss: 0.1785
Epoch 322/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1815
Epoch 323/1000
13/13 [==============================] - 0s 917us/step - loss: 0.1792
Epoch 324/1000
13/13 [==============================] - 0s 876us/step - loss: 0.1829
Epoch 325/1000
13/13 [==============================] - 0s 809us/step - loss: 0.1800
Epoch 326/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1783
Epoch 327/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1797
Epoch 328/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1846
Epoch 329/1000
13/13 [==============================] - 0s 903us/step - loss: 0.1790
Epoch 330/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1815
Epoch 331/1000
13/13 [==============================] - 0s 962us/step - loss: 0.1801
Epoch 332/1000
13/13 [==============================] - 0s 852us/step - loss: 0.1803
Epoch 333/1000
13/13 [==============================] - 0s 996us/step - loss: 0.1824
Epoch 334/1000
13/13 [==============================] - 0s 977us/step - loss: 0.1849
Epoch 335/1000
13/13 [==============================] - 0s 962us/step - loss: 0.1835
Epoch 336/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1797
Epoch 337/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1805
Epoch 338/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1796
Epoch 339/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1807
Epoch 340/1000
13/13 [==============================] - 0s 994us/step - loss: 0.1794
Epoch 341/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1808
Epoch 342/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1790
Epoch 343/1000
13/13 [==============================] - 0s 934us/step - loss: 0.1797
Epoch 344/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1804
Epoch 345/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1838
Epoch 346/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1832
Epoch 347/1000
13/13 [==============================] - 0s 807us/step - loss: 0.1819
Epoch 348/1000
13/13 [==============================] - 0s 972us/step - loss: 0.1800
Epoch 349/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1789
Epoch 350/1000
13/13 [==============================] - 0s 901us/step - loss: 0.1787
Epoch 351/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1784
Epoch 352/1000
13/13 [==============================] - 0s 957us/step - loss: 0.1846
Epoch 353/1000
13/13 [==============================] - 0s 932us/step - loss: 0.1826
Epoch 354/1000
13/13 [==============================] - 0s 881us/step - loss: 0.1802
Epoch 355/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1792
Epoch 356/1000
13/13 [==============================] - 0s 966us/step - loss: 0.1786
Epoch 357/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1802
Epoch 358/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1781
Epoch 359/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1800
Epoch 360/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1821
Epoch 361/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1789
Epoch 362/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1798
Epoch 363/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1815
Epoch 364/1000
13/13 [==============================] - 0s 916us/step - loss: 0.1799
Epoch 365/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1811
Epoch 366/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1785
Epoch 367/1000
13/13 [==============================] - 0s 825us/step - loss: 0.1776
Epoch 368/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1784
Epoch 369/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1819
Epoch 370/1000
13/13 [==============================] - 0s 950us/step - loss: 0.1771
Epoch 371/1000
13/13 [==============================] - 0s 986us/step - loss: 0.1799
Epoch 372/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1780
Epoch 373/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1773
Epoch 374/1000
13/13 [==============================] - 0s 860us/step - loss: 0.1769
Epoch 375/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1770
Epoch 376/1000
13/13 [==============================] - 0s 988us/step - loss: 0.1766
Epoch 377/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1768
Epoch 378/1000
13/13 [==============================] - 0s 881us/step - loss: 0.1794
Epoch 379/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1799
Epoch 380/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1768
Epoch 381/1000
13/13 [==============================] - 0s 846us/step - loss: 0.1805
Epoch 382/1000
13/13 [==============================] - 0s 838us/step - loss: 0.1782
Epoch 383/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1843
Epoch 384/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1763
Epoch 385/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1790
Epoch 386/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1781
Epoch 387/1000
13/13 [==============================] - 0s 979us/step - loss: 0.1771
Epoch 388/1000
13/13 [==============================] - 0s 869us/step - loss: 0.1809
Epoch 389/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1807
Epoch 390/1000
13/13 [==============================] - 0s 998us/step - loss: 0.1792
Epoch 391/1000
13/13 [==============================] - 0s 882us/step - loss: 0.1767
Epoch 392/1000
13/13 [==============================] - 0s 908us/step - loss: 0.1767
Epoch 393/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1763
Epoch 394/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1768
Epoch 395/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1789
Epoch 396/1000
13/13 [==============================] - 0s 805us/step - loss: 0.1801
Epoch 397/1000
13/13 [==============================] - 0s 991us/step - loss: 0.1805
Epoch 398/1000
13/13 [==============================] - 0s 892us/step - loss: 0.1783
Epoch 399/1000
13/13 [==============================] - 0s 946us/step - loss: 0.1775
Epoch 400/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1796
Epoch 401/1000
13/13 [==============================] - 0s 983us/step - loss: 0.1776
Epoch 402/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1771
Epoch 403/1000
13/13 [==============================] - 0s 815us/step - loss: 0.1765
Epoch 404/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1775
Epoch 405/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1753
Epoch 406/1000
13/13 [==============================] - 0s 894us/step - loss: 0.1759
Epoch 407/1000
13/13 [==============================] - 0s 890us/step - loss: 0.1776
Epoch 408/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1779
Epoch 409/1000
13/13 [==============================] - 0s 877us/step - loss: 0.1759
Epoch 410/1000
13/13 [==============================] - 0s 815us/step - loss: 0.1798
Epoch 411/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1807
Epoch 412/1000
13/13 [==============================] - 0s 969us/step - loss: 0.1778
Epoch 413/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1771
Epoch 414/1000
13/13 [==============================] - 0s 978us/step - loss: 0.1760
Epoch 415/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1760
Epoch 416/1000
13/13 [==============================] - 0s 889us/step - loss: 0.1782
Epoch 417/1000
13/13 [==============================] - 0s 836us/step - loss: 0.1756
Epoch 418/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1762
Epoch 419/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1756
Epoch 420/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1773
Epoch 421/1000
13/13 [==============================] - 0s 902us/step - loss: 0.1761
Epoch 422/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1753
Epoch 423/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1777
Epoch 424/1000
13/13 [==============================] - 0s 812us/step - loss: 0.1754
Epoch 425/1000
13/13 [==============================] - 0s 878us/step - loss: 0.1779
Epoch 426/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1781
Epoch 427/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1739
Epoch 428/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1757
Epoch 429/1000
13/13 [==============================] - 0s 934us/step - loss: 0.1755
Epoch 430/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1775
Epoch 431/1000
13/13 [==============================] - 0s 850us/step - loss: 0.1775
Epoch 432/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1773
Epoch 433/1000
13/13 [==============================] - 0s 951us/step - loss: 0.1777
Epoch 434/1000
13/13 [==============================] - 0s 912us/step - loss: 0.1781
Epoch 435/1000
13/13 [==============================] - 0s 967us/step - loss: 0.1761
Epoch 436/1000
13/13 [==============================] - 0s 917us/step - loss: 0.1775
Epoch 437/1000
13/13 [==============================] - 0s 910us/step - loss: 0.1788
Epoch 438/1000
13/13 [==============================] - 0s 846us/step - loss: 0.1762
Epoch 439/1000
13/13 [==============================] - 0s 831us/step - loss: 0.1752
Epoch 440/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1742
Epoch 441/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1765
Epoch 442/1000
13/13 [==============================] - 0s 940us/step - loss: 0.1776
Epoch 443/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1755
Epoch 444/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1773
Epoch 445/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1763
Epoch 446/1000
13/13 [==============================] - 0s 912us/step - loss: 0.1764
Epoch 447/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1792
Epoch 448/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1746
Epoch 449/1000
13/13 [==============================] - 0s 900us/step - loss: 0.1752
Epoch 450/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1773
Epoch 451/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1772
Epoch 452/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1764
Epoch 453/1000
13/13 [==============================] - 0s 858us/step - loss: 0.1754
Epoch 454/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1748
Epoch 455/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1752
Epoch 456/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1753
Epoch 457/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1785
Epoch 458/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1744
Epoch 459/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1758
Epoch 460/1000
13/13 [==============================] - 0s 906us/step - loss: 0.1759
Epoch 461/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1750
Epoch 462/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1745
Epoch 463/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1792
Epoch 464/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1752
Epoch 465/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1756
Epoch 466/1000
13/13 [==============================] - 0s 890us/step - loss: 0.1752
Epoch 467/1000
13/13 [==============================] - 0s 848us/step - loss: 0.1774
Epoch 468/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1748
Epoch 469/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1767
Epoch 470/1000
13/13 [==============================] - 0s 893us/step - loss: 0.1813
Epoch 471/1000
13/13 [==============================] - 0s 946us/step - loss: 0.1793
Epoch 472/1000
13/13 [==============================] - 0s 902us/step - loss: 0.1748
Epoch 473/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1762
Epoch 474/1000
13/13 [==============================] - 0s 847us/step - loss: 0.1822
Epoch 475/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1788
Epoch 476/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1760
Epoch 477/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1758
Epoch 478/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1763
Epoch 479/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1751
Epoch 480/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1749
Epoch 481/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1742
Epoch 482/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1745
Epoch 483/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1763
Epoch 484/1000
13/13 [==============================] - 0s 925us/step - loss: 0.1767
Epoch 485/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1780
Epoch 486/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1739
Epoch 487/1000
13/13 [==============================] - 0s 853us/step - loss: 0.1781
Epoch 488/1000
13/13 [==============================] - 0s 794us/step - loss: 0.1755
Epoch 489/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1766
Epoch 490/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1783
Epoch 491/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1769
Epoch 492/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1752
Epoch 493/1000
13/13 [==============================] - 0s 916us/step - loss: 0.1772
Epoch 494/1000
13/13 [==============================] - 0s 897us/step - loss: 0.1739
Epoch 495/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1750
Epoch 496/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1798
Epoch 497/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1744
Epoch 498/1000
13/13 [==============================] - 0s 946us/step - loss: 0.1750
Epoch 499/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1750
Epoch 500/1000
13/13 [==============================] - 0s 982us/step - loss: 0.1735
Epoch 501/1000
13/13 [==============================] - 0s 975us/step - loss: 0.1783
Epoch 502/1000
13/13 [==============================] - 0s 881us/step - loss: 0.1749
Epoch 503/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1749
Epoch 504/1000
13/13 [==============================] - 0s 930us/step - loss: 0.1741
Epoch 505/1000
13/13 [==============================] - 0s 924us/step - loss: 0.1767
Epoch 506/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1752
Epoch 507/1000
13/13 [==============================] - 0s 967us/step - loss: 0.1764
Epoch 508/1000
13/13 [==============================] - 0s 891us/step - loss: 0.1719
Epoch 509/1000
13/13 [==============================] - 0s 836us/step - loss: 0.1791
Epoch 510/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1746
Epoch 511/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1786
Epoch 512/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1737
Epoch 513/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1781
Epoch 514/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1766
Epoch 515/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1730
Epoch 516/1000
13/13 [==============================] - 0s 838us/step - loss: 0.1738
Epoch 517/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1729
Epoch 518/1000
13/13 [==============================] - 0s 925us/step - loss: 0.1747
Epoch 519/1000
13/13 [==============================] - 0s 975us/step - loss: 0.1759
Epoch 520/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1748
Epoch 521/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1762
Epoch 522/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1750
Epoch 523/1000
13/13 [==============================] - 0s 827us/step - loss: 0.1751
Epoch 524/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1747
Epoch 525/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1739
Epoch 526/1000
13/13 [==============================] - 0s 956us/step - loss: 0.1731
Epoch 527/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1783
Epoch 528/1000
13/13 [==============================] - 0s 949us/step - loss: 0.1810
Epoch 529/1000
13/13 [==============================] - 0s 884us/step - loss: 0.1770
Epoch 530/1000
13/13 [==============================] - 0s 812us/step - loss: 0.1740
Epoch 531/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1743
Epoch 532/1000
13/13 [==============================] - 0s 937us/step - loss: 0.1759
Epoch 533/1000
13/13 [==============================] - 0s 967us/step - loss: 0.1786
Epoch 534/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1766
Epoch 535/1000
13/13 [==============================] - 0s 898us/step - loss: 0.1755
Epoch 536/1000
13/13 [==============================] - 0s 970us/step - loss: 0.1749
Epoch 537/1000
13/13 [==============================] - 0s 857us/step - loss: 0.1713
Epoch 538/1000
13/13 [==============================] - 0s 903us/step - loss: 0.1774
Epoch 539/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1741
Epoch 540/1000
13/13 [==============================] - 0s 891us/step - loss: 0.1774
Epoch 541/1000
13/13 [==============================] - 0s 981us/step - loss: 0.1734
Epoch 542/1000
13/13 [==============================] - 0s 908us/step - loss: 0.1754
Epoch 543/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1735
Epoch 544/1000
13/13 [==============================] - 0s 894us/step - loss: 0.1758
Epoch 545/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1723
Epoch 546/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1786
Epoch 547/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1743
Epoch 548/1000
13/13 [==============================] - 0s 925us/step - loss: 0.1750
Epoch 549/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1747
Epoch 550/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1768
Epoch 551/1000
13/13 [==============================] - 0s 865us/step - loss: 0.1732
Epoch 552/1000
13/13 [==============================] - 0s 804us/step - loss: 0.1736
Epoch 553/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1725
Epoch 554/1000
13/13 [==============================] - 0s 933us/step - loss: 0.1748
Epoch 555/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1733
Epoch 556/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1727
Epoch 557/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1754
Epoch 558/1000
13/13 [==============================] - 0s 874us/step - loss: 0.1781
Epoch 559/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1805
Epoch 560/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1764
Epoch 561/1000
13/13 [==============================] - 0s 993us/step - loss: 0.1784
Epoch 562/1000
13/13 [==============================] - 0s 908us/step - loss: 0.1715
Epoch 563/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1730
Epoch 564/1000
13/13 [==============================] - 0s 906us/step - loss: 0.1733
Epoch 565/1000
13/13 [==============================] - 0s 879us/step - loss: 0.1718
Epoch 566/1000
13/13 [==============================] - 0s 854us/step - loss: 0.1750
Epoch 567/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1751
Epoch 568/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1728
Epoch 569/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1730
Epoch 570/1000
13/13 [==============================] - 0s 933us/step - loss: 0.1761
Epoch 571/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1798
Epoch 572/1000
13/13 [==============================] - 0s 958us/step - loss: 0.1762
Epoch 573/1000
13/13 [==============================] - 0s 893us/step - loss: 0.1727
Epoch 574/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1722
Epoch 575/1000
13/13 [==============================] - 0s 895us/step - loss: 0.1717
Epoch 576/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1730
Epoch 577/1000
13/13 [==============================] - 0s 977us/step - loss: 0.1751
Epoch 578/1000
13/13 [==============================] - 0s 907us/step - loss: 0.1741
Epoch 579/1000
13/13 [==============================] - 0s 878us/step - loss: 0.1732
Epoch 580/1000
13/13 [==============================] - 0s 814us/step - loss: 0.1725
Epoch 581/1000
13/13 [==============================] - 0s 955us/step - loss: 0.1731
Epoch 582/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1709
Epoch 583/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1727
Epoch 584/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1742
Epoch 585/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1721
Epoch 586/1000
13/13 [==============================] - 0s 972us/step - loss: 0.1730
Epoch 587/1000
13/13 [==============================] - 0s 872us/step - loss: 0.1728
Epoch 588/1000
13/13 [==============================] - 0s 837us/step - loss: 0.1718
Epoch 589/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1710
Epoch 590/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1787
Epoch 591/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1789
Epoch 592/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1745
Epoch 593/1000
13/13 [==============================] - 0s 899us/step - loss: 0.1775
Epoch 594/1000
13/13 [==============================] - 0s 824us/step - loss: 0.1727
Epoch 595/1000
13/13 [==============================] - 0s 986us/step - loss: 0.1738
Epoch 596/1000
13/13 [==============================] - 0s 916us/step - loss: 0.1746
Epoch 597/1000
13/13 [==============================] - 0s 899us/step - loss: 0.1734
Epoch 598/1000
13/13 [==============================] - 0s 882us/step - loss: 0.1738
Epoch 599/1000
13/13 [==============================] - 0s 903us/step - loss: 0.1707
Epoch 600/1000
13/13 [==============================] - 0s 990us/step - loss: 0.1735
Epoch 601/1000
13/13 [==============================] - 0s 805us/step - loss: 0.1731
Epoch 602/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1727
Epoch 603/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1722
Epoch 604/1000
13/13 [==============================] - 0s 969us/step - loss: 0.1720
Epoch 605/1000
13/13 [==============================] - 0s 974us/step - loss: 0.1747
Epoch 606/1000
13/13 [==============================] - 0s 983us/step - loss: 0.1770
Epoch 607/1000
13/13 [==============================] - 0s 954us/step - loss: 0.1741
Epoch 608/1000
13/13 [==============================] - 0s 879us/step - loss: 0.1748
Epoch 609/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1731
Epoch 610/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1743
Epoch 611/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1725
Epoch 612/1000
13/13 [==============================] - 0s 959us/step - loss: 0.1706
Epoch 613/1000
13/13 [==============================] - 0s 933us/step - loss: 0.1732
Epoch 614/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1746
Epoch 615/1000
13/13 [==============================] - 0s 823us/step - loss: 0.1729
Epoch 616/1000
13/13 [==============================] - 0s 824us/step - loss: 0.1711
Epoch 617/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1722
Epoch 618/1000
13/13 [==============================] - 0s 955us/step - loss: 0.1802
Epoch 619/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1725
Epoch 620/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1773
Epoch 621/1000
13/13 [==============================] - 0s 957us/step - loss: 0.1710
Epoch 622/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1746
Epoch 623/1000
13/13 [==============================] - 0s 805us/step - loss: 0.1728
Epoch 624/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1709
Epoch 625/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1776
Epoch 626/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1717
Epoch 627/1000
13/13 [==============================] - 0s 973us/step - loss: 0.1728
Epoch 628/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1711
Epoch 629/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1732
Epoch 630/1000
13/13 [==============================] - 0s 828us/step - loss: 0.1719
Epoch 631/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1711
Epoch 632/1000
13/13 [==============================] - 0s 978us/step - loss: 0.1752
Epoch 633/1000
13/13 [==============================] - 0s 977us/step - loss: 0.1731
Epoch 634/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1758
Epoch 635/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1713
Epoch 636/1000
13/13 [==============================] - 0s 982us/step - loss: 0.1744
Epoch 637/1000
13/13 [==============================] - 0s 812us/step - loss: 0.1728
Epoch 638/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1725
Epoch 639/1000
13/13 [==============================] - 0s 974us/step - loss: 0.1718
Epoch 640/1000
13/13 [==============================] - 0s 960us/step - loss: 0.1732
Epoch 641/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1736
Epoch 642/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1700
Epoch 643/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1705
Epoch 644/1000
13/13 [==============================] - 0s 849us/step - loss: 0.1725
Epoch 645/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1711
Epoch 646/1000
13/13 [==============================] - 0s 945us/step - loss: 0.1723
Epoch 647/1000
13/13 [==============================] - 0s 955us/step - loss: 0.1719
Epoch 648/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1718
Epoch 649/1000
13/13 [==============================] - 0s 934us/step - loss: 0.1740
Epoch 650/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1737
Epoch 651/1000
13/13 [==============================] - 0s 843us/step - loss: 0.1705
Epoch 652/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1699
Epoch 653/1000
13/13 [==============================] - 0s 978us/step - loss: 0.1712
Epoch 654/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1704
Epoch 655/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1705
Epoch 656/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1701
Epoch 657/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1701
Epoch 658/1000
13/13 [==============================] - 0s 853us/step - loss: 0.1739
Epoch 659/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1712
Epoch 660/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1697
Epoch 661/1000
13/13 [==============================] - 0s 994us/step - loss: 0.1718
Epoch 662/1000
13/13 [==============================] - 0s 960us/step - loss: 0.1720
Epoch 663/1000
13/13 [==============================] - 0s 954us/step - loss: 0.1725
Epoch 664/1000
13/13 [==============================] - 0s 881us/step - loss: 0.1694
Epoch 665/1000
13/13 [==============================] - 0s 905us/step - loss: 0.1700
Epoch 666/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1740
Epoch 667/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1693
Epoch 668/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1722
Epoch 669/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1732
Epoch 670/1000
13/13 [==============================] - 0s 956us/step - loss: 0.1704
Epoch 671/1000
13/13 [==============================] - 0s 921us/step - loss: 0.1696
Epoch 672/1000
13/13 [==============================] - 0s 896us/step - loss: 0.1733
Epoch 673/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1726
Epoch 674/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1740
Epoch 675/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1699
Epoch 676/1000
13/13 [==============================] - 0s 980us/step - loss: 0.1712
Epoch 677/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1711
Epoch 678/1000
13/13 [==============================] - 0s 945us/step - loss: 0.1718
Epoch 679/1000
13/13 [==============================] - 0s 851us/step - loss: 0.1795
Epoch 680/1000
13/13 [==============================] - 0s 982us/step - loss: 0.1709
Epoch 681/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1703
Epoch 682/1000
13/13 [==============================] - 0s 965us/step - loss: 0.1717
Epoch 683/1000
13/13 [==============================] - 0s 941us/step - loss: 0.1758
Epoch 684/1000
13/13 [==============================] - 0s 951us/step - loss: 0.1699
Epoch 685/1000
13/13 [==============================] - 0s 906us/step - loss: 0.1753
Epoch 686/1000
13/13 [==============================] - 0s 847us/step - loss: 0.1728
Epoch 687/1000
13/13 [==============================] - 0s 832us/step - loss: 0.1733
Epoch 688/1000
13/13 [==============================] - 0s 995us/step - loss: 0.1706
Epoch 689/1000
13/13 [==============================] - 0s 949us/step - loss: 0.1705
Epoch 690/1000
13/13 [==============================] - 0s 941us/step - loss: 0.1698
Epoch 691/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1721
Epoch 692/1000
13/13 [==============================] - 0s 975us/step - loss: 0.1712
Epoch 693/1000
13/13 [==============================] - 0s 886us/step - loss: 0.1716
Epoch 694/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1692
Epoch 695/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1718
Epoch 696/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1704
Epoch 697/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1711
Epoch 698/1000
13/13 [==============================] - 0s 962us/step - loss: 0.1708
Epoch 699/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1702
Epoch 700/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1737
Epoch 701/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1720
Epoch 702/1000
13/13 [==============================] - 0s 979us/step - loss: 0.1701
Epoch 703/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1710
Epoch 704/1000
13/13 [==============================] - 0s 899us/step - loss: 0.1690
Epoch 705/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1719
Epoch 706/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1718
Epoch 707/1000
13/13 [==============================] - 0s 884us/step - loss: 0.1680
Epoch 708/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1756
Epoch 709/1000
13/13 [==============================] - 0s 976us/step - loss: 0.1754
Epoch 710/1000
13/13 [==============================] - 0s 985us/step - loss: 0.1721
Epoch 711/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1751
Epoch 712/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1714
Epoch 713/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1716
Epoch 714/1000
13/13 [==============================] - 0s 910us/step - loss: 0.1703
Epoch 715/1000
13/13 [==============================] - 0s 811us/step - loss: 0.1704
Epoch 716/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1749
Epoch 717/1000
13/13 [==============================] - 0s 959us/step - loss: 0.1676
Epoch 718/1000
13/13 [==============================] - 0s 953us/step - loss: 0.1713
Epoch 719/1000
13/13 [==============================] - 0s 968us/step - loss: 0.1690
Epoch 720/1000
13/13 [==============================] - 0s 915us/step - loss: 0.1700
Epoch 721/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1713
Epoch 722/1000
13/13 [==============================] - 0s 981us/step - loss: 0.1712
Epoch 723/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1697
Epoch 724/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1718
Epoch 725/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1741
Epoch 726/1000
13/13 [==============================] - 0s 986us/step - loss: 0.1719
Epoch 727/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1716
Epoch 728/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1713
Epoch 729/1000
13/13 [==============================] - 0s 824us/step - loss: 0.1694
Epoch 730/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1764
Epoch 731/1000
13/13 [==============================] - 0s 952us/step - loss: 0.1758
Epoch 732/1000
13/13 [==============================] - 0s 950us/step - loss: 0.1735
Epoch 733/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1700
Epoch 734/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1698
Epoch 735/1000
13/13 [==============================] - 0s 980us/step - loss: 0.1699
Epoch 736/1000
13/13 [==============================] - 0s 847us/step - loss: 0.1716
Epoch 737/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1701
Epoch 738/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1720
Epoch 739/1000
13/13 [==============================] - 0s 953us/step - loss: 0.1737
Epoch 740/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1730
Epoch 741/1000
13/13 [==============================] - 0s 960us/step - loss: 0.1700
Epoch 742/1000
13/13 [==============================] - 0s 899us/step - loss: 0.1684
Epoch 743/1000
13/13 [==============================] - 0s 842us/step - loss: 0.1713
Epoch 744/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1695
Epoch 745/1000
13/13 [==============================] - 0s 984us/step - loss: 0.1715
Epoch 746/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1690
Epoch 747/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1706
Epoch 748/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1687
Epoch 749/1000
13/13 [==============================] - 0s 958us/step - loss: 0.1694
Epoch 750/1000
13/13 [==============================] - 0s 845us/step - loss: 0.1700
Epoch 751/1000
13/13 [==============================] - 0s 820us/step - loss: 0.1697
Epoch 752/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1696
Epoch 753/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1707
Epoch 754/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1719
Epoch 755/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1716
Epoch 756/1000
13/13 [==============================] - 0s 957us/step - loss: 0.1766
Epoch 757/1000
13/13 [==============================] - 0s 870us/step - loss: 0.1752
Epoch 758/1000
13/13 [==============================] - 0s 852us/step - loss: 0.1689
Epoch 759/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1709
Epoch 760/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1696
Epoch 761/1000
13/13 [==============================] - 0s 949us/step - loss: 0.1684
Epoch 762/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1731
Epoch 763/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1725
Epoch 764/1000
13/13 [==============================] - 0s 879us/step - loss: 0.1754
Epoch 765/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1697
Epoch 766/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1735
Epoch 767/1000
13/13 [==============================] - 0s 999us/step - loss: 0.1705
Epoch 768/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1699
Epoch 769/1000
13/13 [==============================] - 0s 950us/step - loss: 0.1701
Epoch 770/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1693
Epoch 771/1000
13/13 [==============================] - 0s 885us/step - loss: 0.1708
Epoch 772/1000
13/13 [==============================] - 0s 847us/step - loss: 0.1693
Epoch 773/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1697
Epoch 774/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1712
Epoch 775/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1704
Epoch 776/1000
13/13 [==============================] - 0s 945us/step - loss: 0.1681
Epoch 777/1000
13/13 [==============================] - 0s 894us/step - loss: 0.1704
Epoch 778/1000
13/13 [==============================] - 0s 914us/step - loss: 0.1721
Epoch 779/1000
13/13 [==============================] - 0s 873us/step - loss: 0.1706
Epoch 780/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1747
Epoch 781/1000
13/13 [==============================] - 0s 966us/step - loss: 0.1722
Epoch 782/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1714
Epoch 783/1000
13/13 [==============================] - 0s 969us/step - loss: 0.1697
Epoch 784/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1691
Epoch 785/1000
13/13 [==============================] - 0s 880us/step - loss: 0.1710
Epoch 786/1000
13/13 [==============================] - 0s 807us/step - loss: 0.1770
Epoch 787/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1710
Epoch 788/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1672
Epoch 789/1000
13/13 [==============================] - 0s 974us/step - loss: 0.1706
Epoch 790/1000
13/13 [==============================] - 0s 901us/step - loss: 0.1718
Epoch 791/1000
13/13 [==============================] - 0s 998us/step - loss: 0.1678
Epoch 792/1000
13/13 [==============================] - 0s 894us/step - loss: 0.1691
Epoch 793/1000
13/13 [==============================] - 0s 822us/step - loss: 0.1715
Epoch 794/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1784
Epoch 795/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1659
Epoch 796/1000
13/13 [==============================] - 0s 988us/step - loss: 0.1756
Epoch 797/1000
13/13 [==============================] - 0s 903us/step - loss: 0.1708
Epoch 798/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1706
Epoch 799/1000
13/13 [==============================] - 0s 985us/step - loss: 0.1695
Epoch 800/1000
13/13 [==============================] - 0s 884us/step - loss: 0.1668
Epoch 801/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1703
Epoch 802/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1683
Epoch 803/1000
13/13 [==============================] - 0s 926us/step - loss: 0.1704
Epoch 804/1000
13/13 [==============================] - 0s 951us/step - loss: 0.1701
Epoch 805/1000
13/13 [==============================] - 0s 892us/step - loss: 0.1691
Epoch 806/1000
13/13 [==============================] - 0s 970us/step - loss: 0.1712
Epoch 807/1000
13/13 [==============================] - 0s 902us/step - loss: 0.1679
Epoch 808/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1688
Epoch 809/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1704
Epoch 810/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1699
Epoch 811/1000
13/13 [==============================] - 0s 909us/step - loss: 0.1693
Epoch 812/1000
13/13 [==============================] - 0s 891us/step - loss: 0.1678
Epoch 813/1000
13/13 [==============================] - 0s 906us/step - loss: 0.1694
Epoch 814/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1676
Epoch 815/1000
13/13 [==============================] - 0s 815us/step - loss: 0.1698
Epoch 816/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1717
Epoch 817/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1712
Epoch 818/1000
13/13 [==============================] - 0s 993us/step - loss: 0.1681
Epoch 819/1000
13/13 [==============================] - 0s 963us/step - loss: 0.1723
Epoch 820/1000
13/13 [==============================] - 0s 998us/step - loss: 0.1733
Epoch 821/1000
13/13 [==============================] - 0s 886us/step - loss: 0.1692
Epoch 822/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.1745
Epoch 823/1000
13/13 [==============================] - 0s 897us/step - loss: 0.1762
Epoch 824/1000
13/13 [==============================] - 0s 893us/step - loss: 0.1713
Epoch 825/1000
13/13 [==============================] - 0s 890us/step - loss: 0.1697
Epoch 826/1000
13/13 [==============================] - 0s 942us/step - loss: 0.1698
Epoch 827/1000
13/13 [==============================] - 0s 891us/step - loss: 0.1720
Epoch 828/1000
13/13 [==============================] - 0s 833us/step - loss: 0.1696
Epoch 829/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1707
Epoch 830/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1693
Epoch 831/1000
13/13 [==============================] - 0s 979us/step - loss: 0.1691
Epoch 832/1000
13/13 [==============================] - 0s 977us/step - loss: 0.1689
Epoch 833/1000
13/13 [==============================] - 0s 953us/step - loss: 0.1716
Epoch 834/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1669
Epoch 835/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1683
Epoch 836/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1673
Epoch 837/1000
13/13 [==============================] - 0s 932us/step - loss: 0.1684
Epoch 838/1000
13/13 [==============================] - 0s 930us/step - loss: 0.1688
Epoch 839/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1695
Epoch 840/1000
13/13 [==============================] - 0s 928us/step - loss: 0.1689
Epoch 841/1000
13/13 [==============================] - 0s 910us/step - loss: 0.1702
Epoch 842/1000
13/13 [==============================] - 0s 907us/step - loss: 0.1711
Epoch 843/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1689
Epoch 844/1000
13/13 [==============================] - 0s 998us/step - loss: 0.1682
Epoch 845/1000
13/13 [==============================] - 0s 959us/step - loss: 0.1694
Epoch 846/1000
13/13 [==============================] - 0s 972us/step - loss: 0.1678
Epoch 847/1000
13/13 [==============================] - 0s 940us/step - loss: 0.1693
Epoch 848/1000
13/13 [==============================] - 0s 919us/step - loss: 0.1707
Epoch 849/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1699
Epoch 850/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1683
Epoch 851/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1688
Epoch 852/1000
13/13 [==============================] - 0s 972us/step - loss: 0.1751
Epoch 853/1000
13/13 [==============================] - 0s 875us/step - loss: 0.1707
Epoch 854/1000
13/13 [==============================] - 0s 928us/step - loss: 0.1680
Epoch 855/1000
13/13 [==============================] - 0s 917us/step - loss: 0.1688
Epoch 856/1000
13/13 [==============================] - 0s 860us/step - loss: 0.1690
Epoch 857/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1676
Epoch 858/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1720
Epoch 859/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1691
Epoch 860/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1692
Epoch 861/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1705
Epoch 862/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1675
Epoch 863/1000
13/13 [==============================] - 0s 877us/step - loss: 0.1715
Epoch 864/1000
13/13 [==============================] - 0s 973us/step - loss: 0.1684
Epoch 865/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1703
Epoch 866/1000
13/13 [==============================] - 0s 964us/step - loss: 0.1702
Epoch 867/1000
13/13 [==============================] - 0s 941us/step - loss: 0.1695
Epoch 868/1000
13/13 [==============================] - 0s 958us/step - loss: 0.1728
Epoch 869/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1682
Epoch 870/1000
13/13 [==============================] - 0s 854us/step - loss: 0.1681
Epoch 871/1000
13/13 [==============================] - 0s 853us/step - loss: 0.1684
Epoch 872/1000
13/13 [==============================] - 0s 983us/step - loss: 0.1680
Epoch 873/1000
13/13 [==============================] - 0s 991us/step - loss: 0.1720
Epoch 874/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1705
Epoch 875/1000
13/13 [==============================] - 0s 911us/step - loss: 0.1686
Epoch 876/1000
13/13 [==============================] - 0s 944us/step - loss: 0.1676
Epoch 877/1000
13/13 [==============================] - 0s 856us/step - loss: 0.1750
Epoch 878/1000
13/13 [==============================] - 0s 845us/step - loss: 0.1728
Epoch 879/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1733
Epoch 880/1000
13/13 [==============================] - 0s 973us/step - loss: 0.1690
Epoch 881/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1721
Epoch 882/1000
13/13 [==============================] - 0s 990us/step - loss: 0.1754
Epoch 883/1000
13/13 [==============================] - 0s 933us/step - loss: 0.1727
Epoch 884/1000
13/13 [==============================] - 0s 863us/step - loss: 0.1697
Epoch 885/1000
13/13 [==============================] - 0s 844us/step - loss: 0.1670
Epoch 886/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1675
Epoch 887/1000
13/13 [==============================] - 0s 945us/step - loss: 0.1723
Epoch 888/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1701
Epoch 889/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1677
Epoch 890/1000
13/13 [==============================] - 0s 991us/step - loss: 0.1712
Epoch 891/1000
13/13 [==============================] - 0s 865us/step - loss: 0.1684
Epoch 892/1000
13/13 [==============================] - 0s 810us/step - loss: 0.1695
Epoch 893/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1680
Epoch 894/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1694
Epoch 895/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1683
Epoch 896/1000
13/13 [==============================] - 0s 938us/step - loss: 0.1694
Epoch 897/1000
13/13 [==============================] - 0s 954us/step - loss: 0.1714
Epoch 898/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1682
Epoch 899/1000
13/13 [==============================] - 0s 821us/step - loss: 0.1704
Epoch 900/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1664
Epoch 901/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1683
Epoch 902/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1682
Epoch 903/1000
13/13 [==============================] - 0s 960us/step - loss: 0.1669
Epoch 904/1000
13/13 [==============================] - 0s 999us/step - loss: 0.1688
Epoch 905/1000
13/13 [==============================] - 0s 882us/step - loss: 0.1686
Epoch 906/1000
13/13 [==============================] - 0s 823us/step - loss: 0.1739
Epoch 907/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1693
Epoch 908/1000
13/13 [==============================] - 0s 980us/step - loss: 0.1689
Epoch 909/1000
13/13 [==============================] - 0s 933us/step - loss: 0.1673
Epoch 910/1000
13/13 [==============================] - 0s 908us/step - loss: 0.1700
Epoch 911/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1672
Epoch 912/1000
13/13 [==============================] - 0s 911us/step - loss: 0.1672
Epoch 913/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1702
Epoch 914/1000
13/13 [==============================] - 0s 995us/step - loss: 0.1662
Epoch 915/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1716
Epoch 916/1000
13/13 [==============================] - 0s 948us/step - loss: 0.1669
Epoch 917/1000
13/13 [==============================] - 0s 939us/step - loss: 0.1704
Epoch 918/1000
13/13 [==============================] - 0s 951us/step - loss: 0.1659
Epoch 919/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1725
Epoch 920/1000
13/13 [==============================] - 0s 851us/step - loss: 0.1718
Epoch 921/1000
13/13 [==============================] - 0s 832us/step - loss: 0.1670
Epoch 922/1000
13/13 [==============================] - 0s 969us/step - loss: 0.1695
Epoch 923/1000
13/13 [==============================] - 0s 908us/step - loss: 0.1670
Epoch 924/1000
13/13 [==============================] - 0s 913us/step - loss: 0.1672
Epoch 925/1000
13/13 [==============================] - 0s 920us/step - loss: 0.1685
Epoch 926/1000
13/13 [==============================] - 0s 979us/step - loss: 0.1681
Epoch 927/1000
13/13 [==============================] - 0s 897us/step - loss: 0.1698
Epoch 928/1000
13/13 [==============================] - 0s 873us/step - loss: 0.1660
Epoch 929/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1704
Epoch 930/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1678
Epoch 931/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1703
Epoch 932/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1700
Epoch 933/1000
13/13 [==============================] - 0s 970us/step - loss: 0.1699
Epoch 934/1000
13/13 [==============================] - 0s 958us/step - loss: 0.1691
Epoch 935/1000
13/13 [==============================] - 0s 883us/step - loss: 0.1689
Epoch 936/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1680
Epoch 937/1000
13/13 [==============================] - 0s 999us/step - loss: 0.1701
Epoch 938/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1681
Epoch 939/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1693
Epoch 940/1000
13/13 [==============================] - 0s 917us/step - loss: 0.1703
Epoch 941/1000
13/13 [==============================] - 0s 840us/step - loss: 0.1674
Epoch 942/1000
13/13 [==============================] - 0s 931us/step - loss: 0.1667
Epoch 943/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1682
Epoch 944/1000
13/13 [==============================] - 0s 917us/step - loss: 0.1706
Epoch 945/1000
13/13 [==============================] - 0s 923us/step - loss: 0.1679
Epoch 946/1000
13/13 [==============================] - 0s 986us/step - loss: 0.1647
Epoch 947/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1759
Epoch 948/1000
13/13 [==============================] - 0s 976us/step - loss: 0.1712
Epoch 949/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1679
Epoch 950/1000
13/13 [==============================] - 0s 922us/step - loss: 0.1669
Epoch 951/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1733
Epoch 952/1000
13/13 [==============================] - 0s 962us/step - loss: 0.1662
Epoch 953/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1751
Epoch 954/1000
13/13 [==============================] - 0s 976us/step - loss: 0.1705
Epoch 955/1000
13/13 [==============================] - 0s 867us/step - loss: 0.1661
Epoch 956/1000
13/13 [==============================] - 0s 966us/step - loss: 0.1658
Epoch 957/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1676
Epoch 958/1000
13/13 [==============================] - 0s 961us/step - loss: 0.1718
Epoch 959/1000
13/13 [==============================] - 0s 937us/step - loss: 0.1644
Epoch 960/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1697
Epoch 961/1000
13/13 [==============================] - 0s 936us/step - loss: 0.1654
Epoch 962/1000
13/13 [==============================] - 0s 840us/step - loss: 0.1667
Epoch 963/1000
13/13 [==============================] - 0s 835us/step - loss: 0.1757
Epoch 964/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1661
Epoch 965/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1713
Epoch 966/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1671
Epoch 967/1000
13/13 [==============================] - 0s 972us/step - loss: 0.1697
Epoch 968/1000
13/13 [==============================] - 0s 918us/step - loss: 0.1716
Epoch 969/1000
13/13 [==============================] - 0s 857us/step - loss: 0.1688
Epoch 970/1000
13/13 [==============================] - 0s 855us/step - loss: 0.1672
Epoch 971/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1664
Epoch 972/1000
13/13 [==============================] - 0s 879us/step - loss: 0.1684
Epoch 973/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1660
Epoch 974/1000
13/13 [==============================] - 0s 943us/step - loss: 0.1678
Epoch 975/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1675
Epoch 976/1000
13/13 [==============================] - 0s 935us/step - loss: 0.1710
Epoch 977/1000
13/13 [==============================] - 0s 899us/step - loss: 0.1722
Epoch 978/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1648
Epoch 979/1000
13/13 [==============================] - 0s 900us/step - loss: 0.1716
Epoch 980/1000
13/13 [==============================] - 0s 999us/step - loss: 0.1666
Epoch 981/1000
13/13 [==============================] - 0s 953us/step - loss: 0.1666
Epoch 982/1000
13/13 [==============================] - 0s 971us/step - loss: 0.1696
Epoch 983/1000
13/13 [==============================] - 0s 894us/step - loss: 0.1703
Epoch 984/1000
13/13 [==============================] - 0s 837us/step - loss: 0.1655
Epoch 985/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1658
Epoch 986/1000
13/13 [==============================] - 0s 987us/step - loss: 0.1691
Epoch 987/1000
13/13 [==============================] - 0s 924us/step - loss: 0.1665
Epoch 988/1000
13/13 [==============================] - 0s 945us/step - loss: 0.1680
Epoch 989/1000
13/13 [==============================] - 0s 930us/step - loss: 0.1682
Epoch 990/1000
13/13 [==============================] - 0s 929us/step - loss: 0.1664
Epoch 991/1000
13/13 [==============================] - 0s 829us/step - loss: 0.1682
Epoch 992/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1685
Epoch 993/1000
13/13 [==============================] - 0s 947us/step - loss: 0.1672
Epoch 994/1000
13/13 [==============================] - 0s 968us/step - loss: 0.1660
Epoch 995/1000
13/13 [==============================] - 0s 937us/step - loss: 0.1705
Epoch 996/1000
13/13 [==============================] - 0s 904us/step - loss: 0.1678
Epoch 997/1000
13/13 [==============================] - 0s 872us/step - loss: 0.1689
Epoch 998/1000
13/13 [==============================] - 0s 806us/step - loss: 0.1701
Epoch 999/1000
13/13 [==============================] - 0s 995us/step - loss: 0.1711
Epoch 1000/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.1628
</code>
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>&lt;keras.callbacks.History at 0x7f89102c8dd0&gt;</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model_s</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">model_s_test</span><span class="p">(</span><span class="n">model_s</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Model: &#34;Simple&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_6 (Dense)             (None, 6)                 18        

 dense_7 (Dense)             (None, 6)                 42        

=================================================================
Total params: 60
Trainable params: 60
Non-trainable params: 0
_________________________________________________________________
<span class="ansi-green-intense-fg">All tests passed!
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<details>
<summary><font size="3" color="darkgreen"><b>Click for hints</b></font></summary>

Summary should match this (layer instance names may increment )
<div class="highlight"><pre><span></span><code>Model: &quot;Simple&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
L1 (Dense)                   (None, 6)                 18        
_________________________________________________________________
L2 (Dense)                   (None, 6)                 42        
=================================================================
Total params: 60
Trainable params: 60
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>
  <details>
<summary><font size="3" color="darkgreen"><b>Click for more hints</b></font></summary>

<div class="highlight"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model_s</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;L1&quot;</span><span class="p">),</span>            <span class="c1"># @REPLACE</span>
        <span class="n">Dense</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;L2&quot;</span><span class="p">)</span>     <span class="c1"># @REPLACE</span>
    <span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Simple&quot;</span>
<span class="p">)</span>
<span class="n">model_s</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>     <span class="c1"># @REPLACE</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>     <span class="c1"># @REPLACE</span>
<span class="p">)</span>

<span class="n">model_s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>                                   
</code></pre></div>
</details>
</details>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1">#make a model for plotting routines to call</span>
<span class="n">model_predict_s</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">Xl</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">model_s</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xl</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt_nn</span><span class="p">(</span><span class="n">model_predict_s</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">suptitle</span><span class="o">=</span><span class="s2">&quot;Simple Model&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This simple models does pretty well. Let's calculate the classification error.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">training_cerr_simple</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">model_predict_s</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">cv_cerr_simple</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_cv</span><span class="p">,</span> <span class="n">model_predict_s</span><span class="p">(</span><span class="n">X_cv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, training, simple model, </span><span class="si">{</span><span class="n">training_cerr_simple</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, complex model: </span><span class="si">{</span><span class="n">training_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, cv,       simple model, </span><span class="si">{</span><span class="n">cv_cerr_simple</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, complex model: </span><span class="si">{</span><span class="n">cv_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>categorization error, training, simple model, 0.062, complex model: 0.003
categorization error, cv,       simple model, 0.087, complex model: 0.122
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our simple model has a little higher classification error on training data but does better on cross-validation data than the more complex model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="6"></a></p>
<h2 id="6-regularization">6 - Regularization</h2>
<p>As in the case of polynomial regression, one can apply regularization to moderate the impact of a more complex model. Let's try this below.</p>
<p><a name="ex05"></a></p>
<h3 id="exercise-5">Exercise 5</h3>
<p>Reconstruct your complex model, but this time include regularization.
Below, compose a three-layer model:
* Dense layer with 120 units, relu activation, <code>kernel_regularizer=tf.keras.regularizers.l2(0.1)</code>
* Dense layer with 40 units, relu activation, <code>kernel_regularizer=tf.keras.regularizers.l2(0.1)</code>
* Dense layer with 6 units and a linear activation. 
Compile using
* loss with <code>SparseCategoricalCrossentropy</code>, remember to use  <code>from_logits=True</code>
* Adam optimizer with learning rate of 0.01.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># UNQ_C5</span>
<span class="c1"># GRADED CELL: model_r</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model_r</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1">### START CODE HERE ### </span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

        <span class="c1">### START CODE HERE ### </span>
    <span class="p">],</span> <span class="n">name</span><span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
<span class="n">model_r</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="c1">### START CODE HERE ### </span>
    <span class="n">loss</span><span class="o">=</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="c1">### START CODE HERE ### </span>
<span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model_r</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Epoch 1/1000
13/13 [==============================] - 0s 2ms/step - loss: 4.4464
Epoch 2/1000
13/13 [==============================] - 0s 1ms/step - loss: 1.7086
Epoch 3/1000
13/13 [==============================] - 0s 1ms/step - loss: 1.3465
Epoch 4/1000
13/13 [==============================] - 0s 1ms/step - loss: 1.0870
Epoch 5/1000
13/13 [==============================] - 0s 4ms/step - loss: 1.0137
Epoch 6/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.9718
Epoch 7/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.9481
Epoch 8/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.8934
Epoch 9/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.8171
Epoch 10/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7715
Epoch 11/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7611
Epoch 12/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7521
Epoch 13/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.7430
Epoch 14/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7474
Epoch 15/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7045
Epoch 16/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7056
Epoch 17/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.7182
Epoch 18/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7126
Epoch 19/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6868
Epoch 20/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6733
Epoch 21/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.6572
Epoch 22/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6630
Epoch 23/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6508
Epoch 24/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6395
Epoch 25/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.6603
Epoch 26/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.7651
Epoch 27/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6369
Epoch 28/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6122
Epoch 29/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6002
Epoch 30/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6216
Epoch 31/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.6096
Epoch 32/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6260
Epoch 33/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6151
Epoch 34/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.6551
Epoch 35/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6538
Epoch 36/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.6324
Epoch 37/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5940
Epoch 38/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5739
Epoch 39/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5686
Epoch 40/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5697
Epoch 41/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5845
Epoch 42/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5564
Epoch 43/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5791
Epoch 44/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5855
Epoch 45/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5822
Epoch 46/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5683
Epoch 47/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.5278
Epoch 48/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5762
Epoch 49/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5532
Epoch 50/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5313
Epoch 51/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5409
Epoch 52/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5302
Epoch 53/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5362
Epoch 54/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5209
Epoch 55/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5680
Epoch 56/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5131
Epoch 57/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5216
Epoch 58/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5181
Epoch 59/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5470
Epoch 60/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.5524
Epoch 61/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5482
Epoch 62/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5393
Epoch 63/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5135
Epoch 64/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.5322
Epoch 65/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5148
Epoch 66/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5021
Epoch 67/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5041
Epoch 68/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5086
Epoch 69/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5108
Epoch 70/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5156
Epoch 71/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5115
Epoch 72/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5003
Epoch 73/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4989
Epoch 74/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5097
Epoch 75/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5001
Epoch 76/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.5060
Epoch 77/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4977
Epoch 78/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5227
Epoch 79/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5380
Epoch 80/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5101
Epoch 81/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5247
Epoch 82/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4910
Epoch 83/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4799
Epoch 84/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4673
Epoch 85/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4877
Epoch 86/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4816
Epoch 87/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4969
Epoch 88/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4812
Epoch 89/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4776
Epoch 90/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4696
Epoch 91/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4759
Epoch 92/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4731
Epoch 93/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4599
Epoch 94/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4623
Epoch 95/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4669
Epoch 96/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4545
Epoch 97/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4709
Epoch 98/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4669
Epoch 99/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4961
Epoch 100/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4954
Epoch 101/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4874
Epoch 102/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4759
Epoch 103/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4739
Epoch 104/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4682
Epoch 105/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.5125
Epoch 106/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4548
Epoch 107/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4610
Epoch 108/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4702
Epoch 109/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4565
Epoch 110/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4568
Epoch 111/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4550
Epoch 112/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4541
Epoch 113/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4450
Epoch 114/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4411
Epoch 115/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4398
Epoch 116/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4482
Epoch 117/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4724
Epoch 118/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4591
Epoch 119/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4686
Epoch 120/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4736
Epoch 121/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.5020
Epoch 122/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4630
Epoch 123/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4543
Epoch 124/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4465
Epoch 125/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4328
Epoch 126/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4386
Epoch 127/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4468
Epoch 128/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4348
Epoch 129/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4419
Epoch 130/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4371
Epoch 131/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4542
Epoch 132/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4331
Epoch 133/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4236
Epoch 134/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4470
Epoch 135/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4431
Epoch 136/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4460
Epoch 137/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4281
Epoch 138/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4470
Epoch 139/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4480
Epoch 140/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4627
Epoch 141/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4332
Epoch 142/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4201
Epoch 143/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4340
Epoch 144/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4382
Epoch 145/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4264
Epoch 146/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4260
Epoch 147/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4603
Epoch 148/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4396
Epoch 149/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4239
Epoch 150/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4208
Epoch 151/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4169
Epoch 152/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4201
Epoch 153/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4391
Epoch 154/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4230
Epoch 155/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4316
Epoch 156/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4312
Epoch 157/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4280
Epoch 158/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4210
Epoch 159/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4066
Epoch 160/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4302
Epoch 161/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4433
Epoch 162/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4284
Epoch 163/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4102
Epoch 164/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4265
Epoch 165/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4454
Epoch 166/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4595
Epoch 167/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4779
Epoch 168/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4529
Epoch 169/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4328
Epoch 170/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4336
Epoch 171/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4206
Epoch 172/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4214
Epoch 173/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4343
Epoch 174/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4415
Epoch 175/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4200
Epoch 176/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4431
Epoch 177/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4323
Epoch 178/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4162
Epoch 179/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4214
Epoch 180/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4130
Epoch 181/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4324
Epoch 182/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4232
Epoch 183/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4093
Epoch 184/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4030
Epoch 185/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4055
Epoch 186/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4087
Epoch 187/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4134
Epoch 188/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4165
Epoch 189/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3974
Epoch 190/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3971
Epoch 191/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4116
Epoch 192/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4153
Epoch 193/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4132
Epoch 194/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4158
Epoch 195/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4026
Epoch 196/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3953
Epoch 197/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4191
Epoch 198/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3963
Epoch 199/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4080
Epoch 200/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4032
Epoch 201/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4268
Epoch 202/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3954
Epoch 203/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3980
Epoch 204/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4088
Epoch 205/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4571
Epoch 206/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4315
Epoch 207/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4097
Epoch 208/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4166
Epoch 209/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4393
Epoch 210/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4124
Epoch 211/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4216
Epoch 212/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4118
Epoch 213/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4038
Epoch 214/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4036
Epoch 215/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3945
Epoch 216/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4068
Epoch 217/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3940
Epoch 218/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4194
Epoch 219/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3976
Epoch 220/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3994
Epoch 221/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3873
Epoch 222/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4067
Epoch 223/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4034
Epoch 224/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4393
Epoch 225/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4334
Epoch 226/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4213
Epoch 227/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4377
Epoch 228/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3912
Epoch 229/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4028
Epoch 230/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4112
Epoch 231/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4021
Epoch 232/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4107
Epoch 233/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3893
Epoch 234/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3889
Epoch 235/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3881
Epoch 236/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3966
Epoch 237/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3954
Epoch 238/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4168
Epoch 239/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4049
Epoch 240/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3863
Epoch 241/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3890
Epoch 242/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3908
Epoch 243/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3888
Epoch 244/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3984
Epoch 245/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3993
Epoch 246/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4078
Epoch 247/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3814
Epoch 248/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3897
Epoch 249/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3995
Epoch 250/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3910
Epoch 251/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4142
Epoch 252/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4036
Epoch 253/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3950
Epoch 254/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4073
Epoch 255/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4041
Epoch 256/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3808
Epoch 257/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4020
Epoch 258/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3885
Epoch 259/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3947
Epoch 260/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3841
Epoch 261/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4000
Epoch 262/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4665
Epoch 263/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4367
Epoch 264/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3957
Epoch 265/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3989
Epoch 266/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4251
Epoch 267/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4346
Epoch 268/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4114
Epoch 269/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3832
Epoch 270/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3787
Epoch 271/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3874
Epoch 272/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3891
Epoch 273/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4039
Epoch 274/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3776
Epoch 275/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3903
Epoch 276/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3870
Epoch 277/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3825
Epoch 278/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3812
Epoch 279/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4026
Epoch 280/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3938
Epoch 281/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3764
Epoch 282/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3800
Epoch 283/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3876
Epoch 284/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3853
Epoch 285/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4070
Epoch 286/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3956
Epoch 287/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3915
Epoch 288/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3877
Epoch 289/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3760
Epoch 290/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3892
Epoch 291/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3911
Epoch 292/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3697
Epoch 293/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3800
Epoch 294/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4007
Epoch 295/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4066
Epoch 296/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3768
Epoch 297/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3841
Epoch 298/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3884
Epoch 299/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3926
Epoch 300/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4250
Epoch 301/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3915
Epoch 302/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3894
Epoch 303/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3858
Epoch 304/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3804
Epoch 305/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3810
Epoch 306/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3883
Epoch 307/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3922
Epoch 308/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3879
Epoch 309/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3801
Epoch 310/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3715
Epoch 311/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3690
Epoch 312/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3733
Epoch 313/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3863
Epoch 314/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3843
Epoch 315/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3822
Epoch 316/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3789
Epoch 317/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3808
Epoch 318/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3742
Epoch 319/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3791
Epoch 320/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3836
Epoch 321/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3935
Epoch 322/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3927
Epoch 323/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4023
Epoch 324/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4109
Epoch 325/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3989
Epoch 326/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3860
Epoch 327/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3807
Epoch 328/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3919
Epoch 329/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3763
Epoch 330/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3669
Epoch 331/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3715
Epoch 332/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3724
Epoch 333/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4101
Epoch 334/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3930
Epoch 335/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3933
Epoch 336/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3975
Epoch 337/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4038
Epoch 338/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3737
Epoch 339/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3719
Epoch 340/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3868
Epoch 341/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3792
Epoch 342/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3749
Epoch 343/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3693
Epoch 344/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3644
Epoch 345/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3633
Epoch 346/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3662
Epoch 347/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3888
Epoch 348/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4182
Epoch 349/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3776
Epoch 350/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4027
Epoch 351/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3697
Epoch 352/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3903
Epoch 353/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3757
Epoch 354/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3691
Epoch 355/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3733
Epoch 356/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3651
Epoch 357/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3814
Epoch 358/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3961
Epoch 359/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3892
Epoch 360/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3938
Epoch 361/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4104
Epoch 362/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4556
Epoch 363/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4061
Epoch 364/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3714
Epoch 365/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3674
Epoch 366/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3638
Epoch 367/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3693
Epoch 368/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3912
Epoch 369/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3991
Epoch 370/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3732
Epoch 371/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3608
Epoch 372/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3611
Epoch 373/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3791
Epoch 374/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3565
Epoch 375/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3797
Epoch 376/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3772
Epoch 377/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3616
Epoch 378/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3748
Epoch 379/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3832
Epoch 380/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3814
Epoch 381/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4119
Epoch 382/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3712
Epoch 383/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3780
Epoch 384/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3642
Epoch 385/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3681
Epoch 386/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3574
Epoch 387/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3764
Epoch 388/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3717
Epoch 389/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3674
Epoch 390/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3531
Epoch 391/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3664
Epoch 392/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3819
Epoch 393/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3605
Epoch 394/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3635
Epoch 395/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3932
Epoch 396/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3799
Epoch 397/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3915
Epoch 398/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3771
Epoch 399/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3753
Epoch 400/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3727
Epoch 401/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3584
Epoch 402/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3613
Epoch 403/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3600
Epoch 404/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3617
Epoch 405/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3545
Epoch 406/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3600
Epoch 407/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3698
Epoch 408/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3630
Epoch 409/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3818
Epoch 410/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3842
Epoch 411/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3936
Epoch 412/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3794
Epoch 413/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3626
Epoch 414/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3576
Epoch 415/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3730
Epoch 416/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3806
Epoch 417/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3915
Epoch 418/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3629
Epoch 419/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3673
Epoch 420/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3534
Epoch 421/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3874
Epoch 422/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3942
Epoch 423/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3729
Epoch 424/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3723
Epoch 425/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3682
Epoch 426/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3655
Epoch 427/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3641
Epoch 428/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3707
Epoch 429/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3673
Epoch 430/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3631
Epoch 431/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3523
Epoch 432/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3592
Epoch 433/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3893
Epoch 434/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3961
Epoch 435/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4097
Epoch 436/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3961
Epoch 437/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3837
Epoch 438/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3836
Epoch 439/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3501
Epoch 440/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3474
Epoch 441/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3626
Epoch 442/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3807
Epoch 443/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3725
Epoch 444/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3662
Epoch 445/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3735
Epoch 446/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3537
Epoch 447/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3685
Epoch 448/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3609
Epoch 449/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3533
Epoch 450/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3551
Epoch 451/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3492
Epoch 452/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3630
Epoch 453/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3763
Epoch 454/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3718
Epoch 455/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3727
Epoch 456/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3628
Epoch 457/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3558
Epoch 458/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3812
Epoch 459/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3643
Epoch 460/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3624
Epoch 461/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3632
Epoch 462/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3509
Epoch 463/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3559
Epoch 464/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3718
Epoch 465/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3495
Epoch 466/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3765
Epoch 467/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3667
Epoch 468/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4002
Epoch 469/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4147
Epoch 470/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3473
Epoch 471/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3688
Epoch 472/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4113
Epoch 473/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4088
Epoch 474/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3998
Epoch 475/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3723
Epoch 476/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3604
Epoch 477/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3805
Epoch 478/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3670
Epoch 479/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3594
Epoch 480/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3609
Epoch 481/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3550
Epoch 482/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3755
Epoch 483/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3802
Epoch 484/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3782
Epoch 485/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3808
Epoch 486/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3564
Epoch 487/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3470
Epoch 488/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3539
Epoch 489/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3401
Epoch 490/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3561
Epoch 491/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3693
Epoch 492/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3690
Epoch 493/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3510
Epoch 494/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3548
Epoch 495/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3525
Epoch 496/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3736
Epoch 497/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4008
Epoch 498/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3497
Epoch 499/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3444
Epoch 500/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3610
Epoch 501/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3546
Epoch 502/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3586
Epoch 503/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3814
Epoch 504/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3645
Epoch 505/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3684
Epoch 506/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3834
Epoch 507/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3581
Epoch 508/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3402
Epoch 509/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3503
Epoch 510/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3488
Epoch 511/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3514
Epoch 512/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3611
Epoch 513/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3482
Epoch 514/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3461
Epoch 515/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3535
Epoch 516/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3595
Epoch 517/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3676
Epoch 518/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3638
Epoch 519/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3670
Epoch 520/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3616
Epoch 521/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3475
Epoch 522/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3659
Epoch 523/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3748
Epoch 524/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3416
Epoch 525/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3484
Epoch 526/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3559
Epoch 527/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3420
Epoch 528/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3476
Epoch 529/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3793
Epoch 530/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3642
Epoch 531/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3761
Epoch 532/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3456
Epoch 533/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3398
Epoch 534/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3614
Epoch 535/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3618
Epoch 536/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3422
Epoch 537/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4039
Epoch 538/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3591
Epoch 539/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3597
Epoch 540/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3934
Epoch 541/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.4010
Epoch 542/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3746
Epoch 543/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3709
Epoch 544/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3576
Epoch 545/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3510
Epoch 546/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3669
Epoch 547/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3648
Epoch 548/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3654
Epoch 549/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3436
Epoch 550/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3411
Epoch 551/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3460
Epoch 552/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3460
Epoch 553/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3396
Epoch 554/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3513
Epoch 555/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3890
Epoch 556/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3884
Epoch 557/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3706
Epoch 558/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3578
Epoch 559/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3826
Epoch 560/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3486
Epoch 561/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3443
Epoch 562/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3528
Epoch 563/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3515
Epoch 564/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3615
Epoch 565/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3448
Epoch 566/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3620
Epoch 567/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3439
Epoch 568/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3493
Epoch 569/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3499
Epoch 570/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3386
Epoch 571/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3667
Epoch 572/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3514
Epoch 573/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3500
Epoch 574/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3619
Epoch 575/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3435
Epoch 576/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3396
Epoch 577/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3557
Epoch 578/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4221
Epoch 579/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3583
Epoch 580/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3376
Epoch 581/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3628
Epoch 582/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3540
Epoch 583/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3571
Epoch 584/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3818
Epoch 585/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3954
Epoch 586/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3669
Epoch 587/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3536
Epoch 588/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3407
Epoch 589/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3348
Epoch 590/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3374
Epoch 591/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3489
Epoch 592/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3452
Epoch 593/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3429
Epoch 594/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3425
Epoch 595/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.4209
Epoch 596/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3978
Epoch 597/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3565
Epoch 598/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3443
Epoch 599/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3419
Epoch 600/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3529
Epoch 601/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3345
Epoch 602/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3436
Epoch 603/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3594
Epoch 604/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3504
Epoch 605/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3590
Epoch 606/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3738
Epoch 607/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3654
Epoch 608/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3516
Epoch 609/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3480
Epoch 610/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3599
Epoch 611/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3539
Epoch 612/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3668
Epoch 613/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3593
Epoch 614/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3483
Epoch 615/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3536
Epoch 616/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3456
Epoch 617/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3287
Epoch 618/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3673
Epoch 619/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4033
Epoch 620/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3884
Epoch 621/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3619
Epoch 622/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3834
Epoch 623/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3413
Epoch 624/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3359
Epoch 625/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3319
Epoch 626/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3425
Epoch 627/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3567
Epoch 628/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3715
Epoch 629/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3719
Epoch 630/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3774
Epoch 631/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3697
Epoch 632/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3777
Epoch 633/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3753
Epoch 634/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3749
Epoch 635/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3667
Epoch 636/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3486
Epoch 637/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3488
Epoch 638/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3443
Epoch 639/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3455
Epoch 640/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3583
Epoch 641/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3428
Epoch 642/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3522
Epoch 643/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3642
Epoch 644/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3473
Epoch 645/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3546
Epoch 646/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3543
Epoch 647/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3561
Epoch 648/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3643
Epoch 649/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3590
Epoch 650/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3484
Epoch 651/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3427
Epoch 652/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3329
Epoch 653/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3478
Epoch 654/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3550
Epoch 655/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3478
Epoch 656/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3361
Epoch 657/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3457
Epoch 658/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3430
Epoch 659/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3480
Epoch 660/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3667
Epoch 661/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3403
Epoch 662/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3545
Epoch 663/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3889
Epoch 664/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3568
Epoch 665/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3541
Epoch 666/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3520
Epoch 667/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3340
Epoch 668/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3299
Epoch 669/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3509
Epoch 670/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3352
Epoch 671/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3466
Epoch 672/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3784
Epoch 673/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4029
Epoch 674/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4009
Epoch 675/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3426
Epoch 676/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3406
Epoch 677/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3369
Epoch 678/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3356
Epoch 679/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3463
Epoch 680/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3406
Epoch 681/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3549
Epoch 682/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3399
Epoch 683/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3363
Epoch 684/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3415
Epoch 685/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3470
Epoch 686/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3487
Epoch 687/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3424
Epoch 688/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3321
Epoch 689/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3976
Epoch 690/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3724
Epoch 691/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3471
Epoch 692/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3554
Epoch 693/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3445
Epoch 694/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3483
Epoch 695/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3390
Epoch 696/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3378
Epoch 697/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3355
Epoch 698/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3517
Epoch 699/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3456
Epoch 700/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3493
Epoch 701/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3460
Epoch 702/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3256
Epoch 703/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3269
Epoch 704/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3510
Epoch 705/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3470
Epoch 706/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3533
Epoch 707/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3518
Epoch 708/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3458
Epoch 709/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3581
Epoch 710/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3513
Epoch 711/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3361
Epoch 712/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3854
Epoch 713/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3573
Epoch 714/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3398
Epoch 715/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3291
Epoch 716/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3360
Epoch 717/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3615
Epoch 718/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3587
Epoch 719/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4233
Epoch 720/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4165
Epoch 721/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3999
Epoch 722/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3667
Epoch 723/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3688
Epoch 724/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3474
Epoch 725/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3534
Epoch 726/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3492
Epoch 727/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3512
Epoch 728/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3524
Epoch 729/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3441
Epoch 730/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3547
Epoch 731/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3466
Epoch 732/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3483
Epoch 733/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3376
Epoch 734/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3519
Epoch 735/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3520
Epoch 736/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3650
Epoch 737/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3722
Epoch 738/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3423
Epoch 739/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3472
Epoch 740/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3422
Epoch 741/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3447
Epoch 742/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3786
Epoch 743/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3409
Epoch 744/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3318
Epoch 745/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3281
Epoch 746/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3304
Epoch 747/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3277
Epoch 748/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3441
Epoch 749/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3797
Epoch 750/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3511
Epoch 751/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3599
Epoch 752/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4169
Epoch 753/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4063
Epoch 754/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3516
Epoch 755/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3407
Epoch 756/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3493
Epoch 757/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3608
Epoch 758/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3780
Epoch 759/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3424
Epoch 760/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3436
Epoch 761/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3541
Epoch 762/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3457
Epoch 763/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3317
Epoch 764/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3496
Epoch 765/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3551
Epoch 766/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3396
Epoch 767/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3339
Epoch 768/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3589
Epoch 769/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3521
Epoch 770/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3301
Epoch 771/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3454
Epoch 772/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3471
Epoch 773/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3825
Epoch 774/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3659
Epoch 775/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3377
Epoch 776/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3882
Epoch 777/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3705
Epoch 778/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3279
Epoch 779/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3339
Epoch 780/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3435
Epoch 781/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3393
Epoch 782/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3259
Epoch 783/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3296
Epoch 784/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3298
Epoch 785/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3286
Epoch 786/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3392
Epoch 787/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3368
Epoch 788/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3307
Epoch 789/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3382
Epoch 790/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3355
Epoch 791/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3734
Epoch 792/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3761
Epoch 793/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3444
Epoch 794/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3632
Epoch 795/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3406
Epoch 796/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3788
Epoch 797/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3315
Epoch 798/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3506
Epoch 799/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3608
Epoch 800/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3491
Epoch 801/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3315
Epoch 802/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3287
Epoch 803/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3276
Epoch 804/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3280
Epoch 805/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3504
Epoch 806/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3500
Epoch 807/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3403
Epoch 808/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3552
Epoch 809/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3773
Epoch 810/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3458
Epoch 811/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3324
Epoch 812/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3241
Epoch 813/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3331
Epoch 814/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3376
Epoch 815/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3443
Epoch 816/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3452
Epoch 817/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3625
Epoch 818/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3543
Epoch 819/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3300
Epoch 820/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3694
Epoch 821/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3836
Epoch 822/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3472
Epoch 823/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3578
Epoch 824/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3510
Epoch 825/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3420
Epoch 826/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3308
Epoch 827/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3247
Epoch 828/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3456
Epoch 829/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3698
Epoch 830/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4228
Epoch 831/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3441
Epoch 832/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3515
Epoch 833/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3434
Epoch 834/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3518
Epoch 835/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3238
Epoch 836/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3339
Epoch 837/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3339
Epoch 838/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3434
Epoch 839/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3268
Epoch 840/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3740
Epoch 841/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3566
Epoch 842/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3545
Epoch 843/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3543
Epoch 844/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3347
Epoch 845/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3272
Epoch 846/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3351
Epoch 847/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3570
Epoch 848/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3441
Epoch 849/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3220
Epoch 850/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3376
Epoch 851/1000
13/13 [==============================] - 0s 5ms/step - loss: 0.3364
Epoch 852/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3501
Epoch 853/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3658
Epoch 854/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3400
Epoch 855/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3381
Epoch 856/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3374
Epoch 857/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3421
Epoch 858/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3686
Epoch 859/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3783
Epoch 860/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3459
Epoch 861/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3653
Epoch 862/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3272
Epoch 863/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3222
Epoch 864/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3736
Epoch 865/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3834
Epoch 866/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3725
Epoch 867/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3334
Epoch 868/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3360
Epoch 869/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3430
Epoch 870/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3601
Epoch 871/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3625
Epoch 872/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3410
Epoch 873/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3373
Epoch 874/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3479
Epoch 875/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3524
Epoch 876/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3360
Epoch 877/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3316
Epoch 878/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3564
Epoch 879/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3425
Epoch 880/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3270
Epoch 881/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3594
Epoch 882/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3598
Epoch 883/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4354
Epoch 884/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3778
Epoch 885/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3704
Epoch 886/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3419
Epoch 887/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3491
Epoch 888/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3509
Epoch 889/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3373
Epoch 890/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3713
Epoch 891/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3285
Epoch 892/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3294
Epoch 893/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3340
Epoch 894/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3266
Epoch 895/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3464
Epoch 896/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3392
Epoch 897/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3304
Epoch 898/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3448
Epoch 899/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3721
Epoch 900/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3583
Epoch 901/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3743
Epoch 902/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3616
Epoch 903/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3491
Epoch 904/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3283
Epoch 905/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3386
Epoch 906/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3571
Epoch 907/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3552
Epoch 908/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3694
Epoch 909/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.4247
Epoch 910/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3797
Epoch 911/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3910
Epoch 912/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3706
Epoch 913/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3323
Epoch 914/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3561
Epoch 915/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3473
Epoch 916/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3535
Epoch 917/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3453
Epoch 918/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3378
Epoch 919/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3582
Epoch 920/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3751
Epoch 921/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3452
Epoch 922/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3507
Epoch 923/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3225
Epoch 924/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3479
Epoch 925/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3356
Epoch 926/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3285
Epoch 927/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3434
Epoch 928/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3272
Epoch 929/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3504
Epoch 930/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3919
Epoch 931/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.4201
Epoch 932/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3934
Epoch 933/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3428
Epoch 934/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3645
Epoch 935/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3348
Epoch 936/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3342
Epoch 937/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3461
Epoch 938/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3503
Epoch 939/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3471
Epoch 940/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3407
Epoch 941/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3188
Epoch 942/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3240
Epoch 943/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3440
Epoch 944/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3599
Epoch 945/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3812
Epoch 946/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3393
Epoch 947/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3357
Epoch 948/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3297
Epoch 949/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3231
Epoch 950/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3178
Epoch 951/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3111
Epoch 952/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3343
Epoch 953/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3389
Epoch 954/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3572
Epoch 955/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3215
Epoch 956/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3439
Epoch 957/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3319
Epoch 958/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3322
Epoch 959/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3159
Epoch 960/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3218
Epoch 961/1000
13/13 [==============================] - 0s 4ms/step - loss: 0.3287
Epoch 962/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3196
Epoch 963/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3408
Epoch 964/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3208
Epoch 965/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3241
Epoch 966/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3396
Epoch 967/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3292
Epoch 968/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3362
Epoch 969/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3865
Epoch 970/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3795
Epoch 971/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3494
Epoch 972/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3260
Epoch 973/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3279
Epoch 974/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3238
Epoch 975/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3419
Epoch 976/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3488
Epoch 977/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3278
Epoch 978/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3219
Epoch 979/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3267
Epoch 980/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3458
Epoch 981/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3263
Epoch 982/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3288
Epoch 983/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3174
Epoch 984/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3339
Epoch 985/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3361
Epoch 986/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3253
Epoch 987/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3248
Epoch 988/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3199
Epoch 989/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3323
Epoch 990/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3463
Epoch 991/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3422
Epoch 992/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3354
Epoch 993/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3225
Epoch 994/1000
13/13 [==============================] - 0s 3ms/step - loss: 0.3282
Epoch 995/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3532
Epoch 996/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3445
Epoch 997/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3738
Epoch 998/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3308
Epoch 999/1000
13/13 [==============================] - 0s 2ms/step - loss: 0.3505
Epoch 1000/1000
13/13 [==============================] - 0s 1ms/step - loss: 0.3514
</code>
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>&lt;keras.callbacks.History at 0x7f890fb0ab90&gt;</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1"># BEGIN UNIT TEST</span>
<span class="n">model_r</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">model_r_test</span><span class="p">(</span><span class="n">model_r</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> 
<span class="c1"># END UNIT TEST</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Model: &#34;sequential&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_8 (Dense)             (None, 120)               360       

 dense_9 (Dense)             (None, 40)                4840      

 dense_10 (Dense)            (None, 6)                 246       

=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
ddd
<span class="ansi-green-intense-fg">All tests passed!
</span></code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<details>
<summary><font size="3" color="darkgreen"><b>Click for hints</b></font></summary>

Summary should match this (layer instance names may increment )
<div class="highlight"><pre><span></span><code>Model: &quot;ComplexRegularized&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
L1 (Dense)                   (None, 120)               360       
_________________________________________________________________
L2 (Dense)                   (None, 40)                4840      
_________________________________________________________________
L3 (Dense)                   (None, 6)                 246       
=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>
  <details>
<summary><font size="3" color="darkgreen"><b>Click for more hints</b></font></summary>

<div class="highlight"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model_r</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;L1&quot;</span><span class="p">),</span> 
        <span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;L2&quot;</span><span class="p">),</span>  
        <span class="n">Dense</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;L3&quot;</span><span class="p">)</span>  
    <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ComplexRegularized&quot;</span>
<span class="p">)</span>
<span class="n">model_r</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> 
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>                             
<span class="p">)</span>

<span class="n">model_r</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>                                   
</code></pre></div>
</details>
</details>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="c1">#make a model for plotting routines to call</span>
<span class="n">model_predict_r</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">Xl</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">model_r</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xl</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt_nn</span><span class="p">(</span><span class="n">model_predict_r</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">suptitle</span><span class="o">=</span><span class="s2">&quot;Regularized&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The results look very similar to the 'ideal' model. Let's check classification error.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">training_cerr_reg</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">model_predict_r</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">cv_cerr_reg</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_cv</span><span class="p">,</span> <span class="n">model_predict_r</span><span class="p">(</span><span class="n">X_cv</span><span class="p">))</span>
<span class="n">test_cerr_reg</span> <span class="o">=</span> <span class="n">eval_cat_err</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model_predict_r</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, training, regularized: </span><span class="si">{</span><span class="n">training_cerr_reg</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, simple model, </span><span class="si">{</span><span class="n">training_cerr_simple</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, complex model: </span><span class="si">{</span><span class="n">training_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;categorization error, cv,       regularized: </span><span class="si">{</span><span class="n">cv_cerr_reg</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, simple model, </span><span class="si">{</span><span class="n">cv_cerr_simple</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">, complex model: </span><span class="si">{</span><span class="n">cv_cerr_complex</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>categorization error, training, regularized: 0.072, simple model, 0.062, complex model: 0.003
categorization error, cv,       regularized: 0.066, simple model, 0.087, complex model: 0.122
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The simple model is a bit better in the training set than the regularized model but it worse in the cross validation set.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="7"></a></p>
<h2 id="7-iterate-to-find-optimal-regularization-value">7 - Iterate to find optimal regularization value</h2>
<p>As you did in linear regression, you can try many regularization values. This code takes several minutes to run. If you have time, you can run it and check the results. If not, you have completed the graded parts of the assignment!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">lambdas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)):</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">lambdas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>  <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)),</span>
            <span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)),</span>
            <span class="n">Dense</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished lambda = </span><span class="si">{</span><span class="n">lambda_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Finished lambda = 0.0
Finished lambda = 0.001
Finished lambda = 0.01
Finished lambda = 0.05
Finished lambda = 0.1
Finished lambda = 0.2
Finished lambda = 0.3
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">plot_iterate</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As regularization is increased, the performance of the model on the training and cross-validation data sets converge. For this data set and model, lambda &gt; 0.01 seems to be a reasonable choice.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a name="7.1"></a></p>
<h3 id="71-test">7.1 Test</h3>
<p>Let's try our optimized models on the test set and compare them to 'ideal' performance. </p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">plt_compare</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">model_predict_s</span><span class="p">,</span> <span class="n">model_predict_r</span><span class="p">,</span> <span class="n">centers</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea ">
<pre>
<code>Canvas(toolbar=Toolbar(toolitems=[(&#39;Home&#39;, &#39;Reset original view&#39;, &#39;home&#39;, &#39;home&#39;), (&#39;Back&#39;, &#39;Back to previous …</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our test set is small and seems to have a number of outliers so classification error is high. However, the performance of our optimized models is comparable to ideal performance.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="congratulations">Congratulations!</h2>
<p>You have become familiar with important tools to apply when evaluating your machine learning models. Namely:<br />
* splitting data into trained and untrained sets allows you to differentiate between underfitting and overfitting
* creating three data sets, Training, Cross-Validation and Test allows you to
    * train your parameters <span class="arithmatex">\(W,B\)</span> with the training set
    * tune model parameters such as complexity, regularization and number of examples with the cross-validation set
    * evaluate your 'real world' performance using the test set.
* comparing training vs cross-validation performance provides insight into a model's propensity towards overfitting (high variance) or underfitting (high bias)</p>
</div>
</div>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/hari31416" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/hari31416" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/hari31416" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.kaggle.com/hari31416" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.instagram.com/hari31416" target="_blank" rel="noopener" title="www.instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.top", "toc.integrate"], "search": "../../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>