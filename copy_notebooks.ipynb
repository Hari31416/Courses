{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "all_ipynb_files = glob.glob('./**/*.ipynb', recursive=True)\n",
    "all_ipynb_files = [f for f in all_ipynb_files if not f.startswith('./all_notebooks')]\n",
    "all_ipynb_files = [f for f in all_ipynb_files if not f.startswith('./site')]\n",
    "print(len(all_ipynb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ipynb_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".exclude\", \"r\") as f:\n",
    "    exclude_files = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "final_notebooks_to_copy = [f for f in all_ipynb_files if f not in exclude_files]\n",
    "print(len(final_notebooks_to_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks_in_all_notebooks = glob.glob('./all_notebooks/**/*.ipynb', recursive=True)\n",
    "# print(len(notebooks_in_all_notebooks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_base_dir = './all_notebooks'\n",
    "if not os.path.exists(new_base_dir):\n",
    "    os.makedirs(new_base_dir)\n",
    "\n",
    "for file in final_notebooks_to_copy:\n",
    "    new_file = os.path.join(new_base_dir, file)\n",
    "    new_dir = os.path.dirname(new_file)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    shutil.copyfile(file, new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node = \"DLS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "all_notebooks_second = glob.glob(f'./all_notebooks/{root_node}/**/*.ipynb', recursive=True)\n",
    "print(len(all_notebooks_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./all_notebooks/DLS/C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step.ipynb'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_notebooks_second[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C1']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = all_notebooks_second[0].split('/')\n",
    "max_nodes = 1\n",
    "skip_nodes = 3\n",
    "final_notebooks_path = '/'.join(nodes[skip_nodes:])\n",
    "print(final_notebooks_path)\n",
    "nodes = nodes[skip_nodes:skip_nodes+max_nodes]\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_text = \"\"\n",
    "sep = \"  \"\n",
    "for i, node in enumerate(nodes):\n",
    "    yaml_text += sep*i + \"- \" + node + \":\\n\"\n",
    "yaml_text += sep*(i+1) + \"- \" +final_notebooks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- C1:\n",
      "  - C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(yaml_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_and_final_path(notebook_dir, max_nodes = 1, skip_nodes= 3):\n",
    "    all_nodes = notebook_dir.split(os.path.sep)\n",
    "    final_notebooks_path = os.path.sep.join(all_nodes[skip_nodes:])\n",
    "    final_notebooks_path = os.path.join(root_node, final_notebooks_path)\n",
    "    nodes = all_nodes[skip_nodes:skip_nodes+max_nodes]\n",
    "    nodes = \"/\".join(nodes)\n",
    "    return nodes, final_notebooks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \" \"*4\n",
    "def create_node_yaml(nodes):\n",
    "    nodes = nodes.split(\"/\")\n",
    "    yaml_text = \"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        yaml_text += sep*(i+1) + \"- \" + node + \":\\n\"\n",
    "    return yaml_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C5',\n",
       " 'DLS/C5/Assignments/Operations_on_Word_Vectors_Debiasing/Operations_on_word_vectors_v2a.ipynb')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_node_and_final_path(all_notebooks_second[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "all_file_paths = []\n",
    "for file in all_notebooks_second:\n",
    "    res = get_node_and_final_path(file)\n",
    "    all_nodes.append(res[0])\n",
    "    all_file_paths.append(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = list(set(all_nodes))\n",
    "unique_nodes = sorted(unique_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t- C1:\\n', '\\t- C2:\\n', '\\t- C4:\\n', '\\t- C5:\\n']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_yamls = [create_node_yaml(n) for n in unique_nodes]\n",
    "unique_yamls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_yaml_mapping = {k:v for k, v in zip(unique_nodes, unique_yamls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_yaml = pd.DataFrame({\n",
    "#     \"node\": unique_nodes,\n",
    "#     \"yaml\": unique_yamls\n",
    "# })\n",
    "# node_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (file, node) in enumerate(zip(all_file_paths, all_nodes)):\n",
    "    depth = len(node.split(\"/\")) + 1\n",
    "    node_yaml_mapping[node] += sep*depth + \"- \" +  file + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = f\"- {root_node}:\\n\"\n",
    "for val in node_yaml_mapping.values():\n",
    "    final_text += val + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- DLS:\n",
      "\t- C1:\n",
      "\t\t- DLS/C1/Assignments/Building_your_Deep_Neural_Network_Step_by_Step/Building_your_Deep_Neural_Network_Step_by_Step.ipynb\n",
      "\t\t- DLS/C1/Assignments/Deep_Neural_Network_-_Application/Deep_Neural_Network_-_Application.ipynb\n",
      "\t\t- DLS/C1/Assignments/Logistic_Regression_with_a_Neural_Network_mindset/Logistic_Regression_with_a_Neural_Network_mindset.ipynb\n",
      "\t\t- DLS/C1/Assignments/Planar_data_classification_with_one_hidden_layer/Planar_data_classification_with_one_hidden_layer.ipynb\n",
      "\n",
      "\t- C2:\n",
      "\t\t- DLS/C2/Assignments/Gradient_Checking/Gradient_Checking.ipynb\n",
      "\t\t- DLS/C2/Assignments/Initialization/Initialization.ipynb\n",
      "\t\t- DLS/C2/Assignments/Optimization_Methods/Optimization_methods.ipynb\n",
      "\t\t- DLS/C2/Assignments/Regularization/Regularization.ipynb\n",
      "\t\t- DLS/C2/Assignments/Tensorflow_introduction/Tensorflow_introduction.ipynb\n",
      "\n",
      "\t- C4:\n",
      "\t\t- DLS/C4/Assignments/Car_detection_with_YOLO/Autonomous_driving_application_Car_detection.ipynb\n",
      "\t\t- DLS/C4/Assignments/Convolution_model_Application/Convolution_model_Application.ipynb\n",
      "\t\t- DLS/C4/Assignments/Image_segmentation_Unet/Image_segmentation_Unet_v2.ipynb\n",
      "\t\t- DLS/C4/Assignments/Residual_Networks/Residual_Networks.ipynb\n",
      "\t\t- DLS/C4/Assignments/Transfer_learning_with_MobileNet/Transfer_learning_with_MobileNet_v1.ipynb\n",
      "\n",
      "\t- C5:\n",
      "\t\t- DLS/C5/Assignments/Building_a_Recurrent_Neural_Network_Step_by_Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb\n",
      "\t\t- DLS/C5/Assignments/Dinosaurus_Island_Character_level_language_model/Dinosaurus_Island_Character_level_language_model.ipynb\n",
      "\t\t- DLS/C5/Assignments/Emojify/Emoji_v3a.ipynb\n",
      "\t\t- DLS/C5/Assignments/Improvise_a_Jazz_Solo_with_an_LSTM_Network/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4.ipynb\n",
      "\t\t- DLS/C5/Assignments/Neural_machine_translation_with_attention/Neural_machine_translation_with_attention_v4a.ipynb\n",
      "\t\t- DLS/C5/Assignments/Operations_on_Word_Vectors_Debiasing/Operations_on_word_vectors_v2a.ipynb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \" \"*4\n",
    "root_node = \"DLS\"\n",
    "root_to_name_map = {\n",
    "    \"DLS\": \"Deep Learning Specialization\",\n",
    "    \"MLS\": \"Machine Learning Specialization\",\n",
    "    \"TF_Specialization\": \"TensorFlow Advanced Techniques Specialization\",\n",
    "    \"GAN\": \"Generative Adversarial Networks Specialization\",\n",
    "    \"DLAI\": \"Deep Learning AI\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_and_final_path(root_node, notebook_dir, max_nodes = 1, skip_nodes= 3):\n",
    "    all_nodes = notebook_dir.split(os.path.sep)\n",
    "    final_notebooks_path = os.path.sep.join(all_nodes[skip_nodes:])\n",
    "    final_notebooks_path = os.path.join(root_node, final_notebooks_path)\n",
    "    nodes = all_nodes[skip_nodes:skip_nodes+max_nodes]\n",
    "    nodes = \"/\".join(nodes)\n",
    "    return nodes, final_notebooks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_yaml(nodes):\n",
    "    nodes = nodes.split(\"/\")\n",
    "    yaml_text = \"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        yaml_text += sep*(i+1) + \"- \" + node + \":\\n\"\n",
    "    return yaml_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_to_name_map = {\n",
    "    \"DLS\": \"Deep Learning Specialization\",\n",
    "    \"MLS\": \"Machine Learning Specialization\",\n",
    "    \"TF_Specialization\": \"TensorFlow Advanced Techniques Specialization\",\n",
    "    \"GAN\": \"Generative Adversarial Networks Specialization\",\n",
    "    \"DLAI\": \"Deep Learning AI\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_root(root_node, max_nodes = 1, skip_nodes = 3):\n",
    "    all_notebooks_second = glob.glob(f'./all_notebooks/{root_node}/**/*.ipynb', recursive=True)\n",
    "    all_nodes = []\n",
    "    all_file_paths = []\n",
    "\n",
    "    for file in all_notebooks_second:\n",
    "        res = get_node_and_final_path(root_node, file, max_nodes, skip_nodes)\n",
    "        all_nodes.append(res[0])\n",
    "        all_file_paths.append(res[1])\n",
    "    unique_nodes = list(set(all_nodes))\n",
    "    unique_nodes = sorted(unique_nodes)\n",
    "    unique_yamls = [create_node_yaml(n) for n in unique_nodes]\n",
    "    node_yaml_mapping = {k:v for k, v in zip(unique_nodes, unique_yamls)}\n",
    "\n",
    "    for i, (file, node) in enumerate(zip(all_file_paths, all_nodes)):\n",
    "        depth = len(node.split(\"/\")) + 1\n",
    "        node_yaml_mapping[node] += sep*depth + \"- \" +  file + \"\\n\"\n",
    "    \n",
    "    final_text = f\"- {root_to_name_map[root_node]}:\\n\"\n",
    "    #sort node_yaml_mapping via value\n",
    "    for val in node_yaml_mapping.values():\n",
    "        final_text += val + \"\\n\"\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = one_root(root_node=root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DLAI', 'DLS', 'GAN', 'MLS', 'TF_Specialization']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_root_nodes = os.listdir(\"all_notebooks\")\n",
    "all_root_nodes = [folder for folder in all_root_nodes if os.path.isdir(folder)]\n",
    "all_root_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = \"\"\n",
    "for node in all_root_nodes:\n",
    "    final_text = final_text + one_root(node) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".yml_out\", \"w\") as f:\n",
    "    for line in final_text.split(\"\\n\"):\n",
    "        f.write(sep+line+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
